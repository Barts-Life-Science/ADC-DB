{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8750f02e-7198-4b28-a276-2bc6b92a6f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_table_to_pipeline(\n",
    "    table_name,\n",
    "    src_server_name=None,\n",
    "    src_server_id=None,\n",
    "    src_database=None,\n",
    "    src_schema=None,\n",
    "    src_table=None,\n",
    "    dst_catalog=\"4_prod\",\n",
    "    dst_schema=\"raw\",\n",
    "    dst_table=None,\n",
    "    key_columns=None,\n",
    "    watermark_column=None,\n",
    "    watermark_value=None,\n",
    "    item_tag=None,\n",
    "    comment=None,\n",
    "    active_ind=1,\n",
    "    query_timeout=\"00:05:00\",\n",
    "    dry_run=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Add a new table to the ETL pipeline by registering it in the watermark management system.\n",
    "    Works even if the destination table doesn't exist yet.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from pyspark.sql import SparkSession\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Standardize destination table name\n",
    "    if dst_table is None:\n",
    "        dst_table = table_name.lower()\n",
    "    else:\n",
    "        dst_table = dst_table.lower()\n",
    "    \n",
    "    # Detect source information based on destination table prefix patterns\n",
    "    if src_server_name is None or src_server_id is None or item_tag is None:\n",
    "        if dst_table.startswith('mill_'):\n",
    "            src_server_name = src_server_name or 'oracle_mill'\n",
    "            src_server_id = src_server_id or 107\n",
    "            src_schema = src_schema or 'V500'\n",
    "            item_tag = item_tag or 'mill'\n",
    "        elif dst_table.startswith(('cds_', 'msds', 'mat_', 'slam_', 'pi_')):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'BH_DATAWAREHOUSE'\n",
    "            src_schema = src_schema or 'dbo'\n",
    "            item_tag = item_tag or 'dwh_cds' if dst_table.startswith('cds_') else 'dwh'\n",
    "        elif dst_table.startswith('iqemo_'):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'iqemo.iqemo'\n",
    "            src_schema = src_schema or 'dbo'\n",
    "            item_tag = item_tag or 'iqemo'\n",
    "        elif dst_table.startswith('pacs_'):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'IMAGING_Sectra.SDWH_PACS_Datamodel'\n",
    "            src_schema = src_schema or '[Interface 1.25]'\n",
    "            item_tag = item_tag or 'pacs_incr_updt'\n",
    "        elif dst_table.startswith('nnu_'):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'BadgerNetReporting'\n",
    "            src_schema = src_schema or 'bnf_dbsync'\n",
    "            item_tag = item_tag or 'dwh'\n",
    "        elif dst_table.startswith('path_'):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'PATHOLOGY.BH_PathDataWarehouse'\n",
    "            src_schema = src_schema or 'dbo'\n",
    "            item_tag = item_tag or 'path_incr_updt'\n",
    "    \n",
    "    # Derive source table name if not specified\n",
    "    if src_table is None:\n",
    "        if dst_table.startswith('mill_'):\n",
    "            src_table = re.sub(r'^mill_', '', dst_table, flags=re.IGNORECASE).upper()\n",
    "        elif dst_table.startswith('iqemo_'):\n",
    "            src_table = re.sub(r'^iqemo_', '', dst_table, flags=re.IGNORECASE)\n",
    "        elif dst_table.startswith('pacs_'):\n",
    "            src_table = re.sub(r'^pacs_', '', dst_table, flags=re.IGNORECASE)\n",
    "        elif dst_table.startswith('nnu_'):\n",
    "            src_table = re.sub(r'^nnu_', '', dst_table, flags=re.IGNORECASE)\n",
    "        elif dst_table.startswith('path_'):\n",
    "            src_table = re.sub(r'^path_', '', dst_table, flags=re.IGNORECASE)\n",
    "        else:\n",
    "            src_table = dst_table.upper()\n",
    "    \n",
    "    # Fully qualified destination table\n",
    "    full_dst_table = f\"{dst_catalog}.{dst_schema}.{dst_table}\"\n",
    "    \n",
    "    # Check if table exists in destination\n",
    "    table_exists = spark.sql(f\"SHOW TABLES FROM {dst_catalog}.{dst_schema} LIKE '{dst_table}'\").count() > 0\n",
    "    \n",
    "    # If table doesn't exist, ensure we have required parameters\n",
    "    if not table_exists:\n",
    "        if watermark_value is None:\n",
    "            watermark_value = \"2008-01-01\"\n",
    "        if key_columns is None:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Table {full_dst_table} does not exist yet. Must specify key_columns explicitly.\"\n",
    "            }\n",
    "    \n",
    "    # Check if table is already in the watermark table\n",
    "    existing_wm = spark.sql(f\"\"\"\n",
    "        SELECT watermark_id \n",
    "        FROM 6_mgmt.incr_updt.watermark\n",
    "        WHERE dst_catalog = '{dst_catalog}'\n",
    "          AND dst_schema = '{dst_schema}'\n",
    "          AND dst_table = '{dst_table}'\n",
    "          AND active_ind = 1\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    if existing_wm:\n",
    "        existing_id = existing_wm[0][\"watermark_id\"]\n",
    "        return {\n",
    "            \"status\": \"warning\",\n",
    "            \"message\": f\"Table {dst_table} already exists in the watermark table with ID {existing_id}.\",\n",
    "            \"watermark_id\": existing_id\n",
    "        }\n",
    "    \n",
    "    # For tables that exist, perform column validation and auto-detection\n",
    "    if table_exists:\n",
    "        # Get all columns\n",
    "        columns_df = spark.sql(f\"DESCRIBE TABLE {full_dst_table}\")\n",
    "        all_columns = {row[\"col_name\"].lower(): row[\"data_type\"] for row in columns_df.collect()}\n",
    "        \n",
    "        # Auto-detect watermark column if not specified\n",
    "        if watermark_column is None:\n",
    "            # Different patterns based on source\n",
    "            if src_server_name == 'oracle_mill':\n",
    "                watermark_candidates = [\"updt_dt_tm\", \"update_dt_tm\"]\n",
    "            elif src_server_name == 'BH2VMDWRL1' and 'datawarehouse' in str(src_database).lower():\n",
    "                watermark_candidates = [\"record_updated_dt\", \"record_update_dt\"]\n",
    "            elif src_database and 'iqemo' in str(src_database).lower():\n",
    "                watermark_candidates = [\"dateupdated\", \"lastupdate\"]\n",
    "            elif src_database and 'pacs' in str(src_database).lower():\n",
    "                watermark_candidates = [\"reportmodifieddateutc\", \"examinationfoldermodifydate\", \"reportdate\"]\n",
    "            elif src_database and 'badger' in str(src_database).lower():\n",
    "                watermark_candidates = [\"lastupdate\", \"recordtimestamp\"]\n",
    "            else:\n",
    "                watermark_candidates = [\n",
    "                    \"updt_dt_tm\", \"update_dt_tm\", \"record_updated_dt\", \"record_update_dt\", \n",
    "                    \"dateupdated\", \"lastupdate\", \"reportdate\", \"reportmodifieddateutc\", \n",
    "                    \"examinationfoldermodifydate\", \"recordtimestamp\"\n",
    "                ]\n",
    "            \n",
    "            for col in watermark_candidates:\n",
    "                if col in all_columns:\n",
    "                    watermark_column = col.upper()\n",
    "                    break\n",
    "                    \n",
    "            if watermark_column is None:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"Could not auto-detect a watermark column for {dst_table}. Please specify one explicitly.\"\n",
    "                }\n",
    "        elif watermark_column.lower() not in all_columns:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Specified watermark column '{watermark_column}' does not exist in table {dst_table}.\"\n",
    "            }\n",
    "        \n",
    "        # Auto-detect key columns if not provided\n",
    "        if key_columns is None:\n",
    "            key_columns = detect_key_columns(spark, full_dst_table)\n",
    "            \n",
    "            if not key_columns:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"Could not auto-detect key columns for {dst_table}. Please specify them explicitly.\"\n",
    "                }\n",
    "        \n",
    "        # Auto-determine watermark value if not specified\n",
    "        if watermark_value is None:\n",
    "            try:\n",
    "                max_ts = spark.sql(f\"SELECT MAX(`{watermark_column}`) as max_ts FROM {full_dst_table}\").collect()[0][\"max_ts\"]\n",
    "                watermark_value = (max_ts - timedelta(days=7)).strftime(\"%Y-%m-%d\") if max_ts else \"2008-01-01\"\n",
    "            except:\n",
    "                watermark_value = \"2008-01-01\"\n",
    "    \n",
    "    # Ensure key_columns is a list\n",
    "    if isinstance(key_columns, str):\n",
    "        key_columns = [key_columns]\n",
    "            \n",
    "    # Format comment for SQL\n",
    "    comment_sql = f\"'{comment}'\" if comment else \"NULL\"\n",
    "    \n",
    "    # Format source database and schema for SQL\n",
    "    src_database_sql = f\"'{src_database}'\" if src_database else \"NULL\"\n",
    "    src_schema_sql = f\"'{src_schema}'\" if src_schema else \"NULL\"\n",
    "            \n",
    "    # Create SQL queries\n",
    "    watermark_query = f\"\"\"\n",
    "    INSERT INTO 6_mgmt.incr_updt.watermark(\n",
    "        src_server_name, src_server_id, src_database, src_schema, src_table, \n",
    "        dst_catalog, dst_schema, dst_table, \n",
    "        watermark_column, watermark_value, item_tag, comment, active_ind, query_timeout\n",
    "    ) VALUES (\n",
    "        '{src_server_name}', {src_server_id}, {src_database_sql}, {src_schema_sql}, '{src_table}',\n",
    "        '{dst_catalog}', '{dst_schema}', '{dst_table}',\n",
    "        '{watermark_column}', '{watermark_value}',\n",
    "        '{item_tag}', {comment_sql}, {active_ind}, '{query_timeout}'\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare key column queries\n",
    "    key_col_queries = [f\"\"\"\n",
    "        INSERT INTO 6_mgmt.incr_updt.table_key_columns(\n",
    "            watermark_id, dst_table_name, key_column_name, active_ind\n",
    "        ) VALUES (\n",
    "            #watermark_id#, '{dst_table}', '{key_column}', 1\n",
    "        )\n",
    "    \"\"\" for key_column in key_columns]\n",
    "    \n",
    "    # If dry run, just return the queries\n",
    "    if dry_run:\n",
    "        return {\n",
    "            \"status\": \"dry_run\",\n",
    "            \"watermark_query\": watermark_query,\n",
    "            \"key_column_queries\": key_col_queries,\n",
    "            \"configuration\": {\n",
    "                \"src_server_name\": src_server_name,\n",
    "                \"src_server_id\": src_server_id,\n",
    "                \"src_database\": src_database,\n",
    "                \"src_schema\": src_schema,\n",
    "                \"src_table\": src_table,\n",
    "                \"dst_catalog\": dst_catalog,\n",
    "                \"dst_schema\": dst_schema,\n",
    "                \"dst_table\": dst_table,\n",
    "                \"key_columns\": key_columns,\n",
    "                \"watermark_column\": watermark_column,\n",
    "                \"watermark_value\": watermark_value,\n",
    "                \"item_tag\": item_tag,\n",
    "                \"table_exists\": table_exists\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Execute queries\n",
    "    try:\n",
    "        spark.sql(watermark_query)\n",
    "        \n",
    "        # Get the newly created watermark_id\n",
    "        watermark_id = spark.sql(f\"\"\"\n",
    "            SELECT MAX(watermark_id) as watermark_id\n",
    "            FROM 6_mgmt.incr_updt.watermark\n",
    "            WHERE dst_catalog = '{dst_catalog}'\n",
    "              AND dst_schema = '{dst_schema}'\n",
    "              AND dst_table = '{dst_table}'\n",
    "        \"\"\").collect()[0][\"watermark_id\"]\n",
    "        \n",
    "        # Insert key columns\n",
    "        for key_column in key_columns:\n",
    "            spark.sql(f\"\"\"\n",
    "                INSERT INTO 6_mgmt.incr_updt.table_key_columns(\n",
    "                    watermark_id, dst_table_name, key_column_name, active_ind\n",
    "                ) VALUES (\n",
    "                    {watermark_id}, '{dst_table}', '{key_column}', 1\n",
    "                )\n",
    "            \"\"\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"watermark_id\": watermark_id,\n",
    "            \"message\": f\"Successfully added {dst_table} to the ETL pipeline with watermark_id {watermark_id}.\",\n",
    "            \"configuration\": {\n",
    "                \"src_server_name\": src_server_name,\n",
    "                \"src_server_id\": src_server_id,\n",
    "                \"src_database\": src_database,\n",
    "                \"src_schema\": src_schema,\n",
    "                \"src_table\": src_table,\n",
    "                \"dst_catalog\": dst_catalog,\n",
    "                \"dst_schema\": dst_schema,\n",
    "                \"dst_table\": dst_table,\n",
    "                \"key_columns\": key_columns,\n",
    "                \"watermark_column\": watermark_column,\n",
    "                \"watermark_value\": watermark_value,\n",
    "                \"item_tag\": item_tag\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Error adding table to pipeline: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3441cbe8-8467-437a-896b-e087aa523f01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dry run to preview the SQL statements\n",
    "# First, let's see what the error is\n",
    "dry_run_result = add_table_to_pipeline(\n",
    "    table_name=\"cds_aea_invest\",\n",
    "    src_server_name=\"BH2VMDWRL1\",\n",
    "    src_server_id=104, \n",
    "    src_database=\"BH_DATAWAREHOUSE\",\n",
    "    src_schema=\"dbo\",\n",
    "    src_table=\"CDS_AEA_INVEST\",\n",
    "    key_columns=[\"CDS_AEA_Id\", \"invest_Num\"],\n",
    "    watermark_column=\"Record_Updated_Dt\",\n",
    "    item_tag=\"dwh_cds\",\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "print(\"Complete result:\")\n",
    "print(dry_run_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Add Table to ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
