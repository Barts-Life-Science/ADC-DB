{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8750f02e-7198-4b28-a276-2bc6b92a6f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Assuming detect_key_columns is defined elsewhere or imported\n",
    "# Placeholder function if not available:\n",
    "def detect_key_columns(spark, full_table_name):\n",
    "    # In a real scenario, this function would inspect table metadata (like primary keys)\n",
    "    # For this example, let's return a placeholder or None\n",
    "    print(f\"Warning: Using placeholder detect_key_columns for {full_table_name}. Implement actual logic.\")\n",
    "    # Example placeholder logic (replace with real implementation)\n",
    "    if \"mill_\" in full_table_name:\n",
    "         return [\"some_mill_id_col\"] # Example\n",
    "    # return None # Or return None if detection fails\n",
    "    return [] # Return empty list if no keys detected/defined\n",
    "\n",
    "\n",
    "def add_table_to_pipeline(\n",
    "table_name,\n",
    "src_server_name=None,\n",
    "src_server_id=None,\n",
    "src_database=None,\n",
    "src_schema=None,\n",
    "src_table=None,\n",
    "dst_catalog=\"4_prod\",\n",
    "dst_schema=\"raw\",\n",
    "dst_table=None,\n",
    "key_columns=None,\n",
    "watermark_column=None,\n",
    "watermark_value=None,\n",
    "item_tag=None,\n",
    "comment=None,\n",
    "active_ind=1,\n",
    "query_timeout=\"00:05:00\",\n",
    "dry_run=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Add a new table to the ETL pipeline by registering it in the watermark management system.\n",
    "    Works even if the destination table doesn't exist yet.\n",
    "\n",
    "    If item_tag includes 'wt_updt', validation/auto-detection for watermark_column\n",
    "    and key_columns is skipped, allowing them to be NULL (for whole table updates).\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from pyspark.sql import SparkSession\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Standardize destination table name\n",
    "    if dst_table is None:\n",
    "        dst_table = table_name.lower()\n",
    "    else:\n",
    "        dst_table = dst_table.lower()\n",
    "\n",
    "    # --- Start of Inference Logic ---\n",
    "    # Detect source information based on destination table prefix patterns\n",
    "    # This block attempts to fill in missing source details and item_tag if possible\n",
    "    if src_server_name is None or src_server_id is None or item_tag is None:\n",
    "        if dst_table.startswith('mill_'):\n",
    "            src_server_name = src_server_name or 'oracle_mill'\n",
    "            src_server_id = src_server_id or 107\n",
    "            src_schema = src_schema or 'V500'\n",
    "            item_tag = item_tag or 'mill'\n",
    "        elif dst_table.startswith(('cds_', 'msds', 'mat_', 'slam_', 'pi_')):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'BH_DATAWAREHOUSE'\n",
    "            src_schema = src_schema or 'dbo'\n",
    "            item_tag = item_tag or 'dwh_cds' if dst_table.startswith('cds_') else 'dwh'\n",
    "        elif dst_table.startswith('iqemo_'):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'iqemo.iqemo'\n",
    "            src_schema = src_schema or 'dbo'\n",
    "            item_tag = item_tag or 'iqemo'\n",
    "        elif dst_table.startswith('pacs_'):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'IMAGING_Sectra.SDWH_PACS_Datamodel'\n",
    "            src_schema = src_schema or '[Interface 1.25]'\n",
    "            item_tag = item_tag or 'pacs_incr_updt'\n",
    "        elif dst_table.startswith('nnu_'):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'BadgerNetReporting'\n",
    "            src_schema = src_schema or 'bnf_dbsync'\n",
    "            item_tag = item_tag or 'dwh'\n",
    "        elif dst_table.startswith('path_'):\n",
    "            src_server_name = src_server_name or 'BH2VMDWRL1'\n",
    "            src_server_id = src_server_id or 104\n",
    "            src_database = src_database or 'PATHOLOGY.BH_PathDataWarehouse'\n",
    "            src_schema = src_schema or 'dbo'\n",
    "            item_tag = item_tag or 'path_incr_updt'\n",
    "        # Fallback if item_tag is still None after pattern matching\n",
    "        item_tag = item_tag or 'generic' # Assign a default if none matched\n",
    "\n",
    "    # Derive source table name if not specified\n",
    "    if src_table is None:\n",
    "        if dst_table.startswith('mill_'):\n",
    "            src_table = re.sub(r'^mill_', '', dst_table, flags=re.IGNORECASE).upper()\n",
    "        elif dst_table.startswith('iqemo_'):\n",
    "            src_table = re.sub(r'^iqemo_', '', dst_table, flags=re.IGNORECASE)\n",
    "        elif dst_table.startswith('pacs_'):\n",
    "            src_table = re.sub(r'^pacs_', '', dst_table, flags=re.IGNORECASE)\n",
    "        elif dst_table.startswith('nnu_'):\n",
    "            src_table = re.sub(r'^nnu_', '', dst_table, flags=re.IGNORECASE)\n",
    "        elif dst_table.startswith('path_'):\n",
    "            src_table = re.sub(r'^path_', '', dst_table, flags=re.IGNORECASE)\n",
    "        else:\n",
    "            src_table = dst_table.upper()\n",
    "    # --- End of Inference Logic ---\n",
    "\n",
    "    # Fully qualified destination table\n",
    "    full_dst_table = f\"{dst_catalog}.{dst_schema}.{dst_table}\"\n",
    "\n",
    "    # Check if table exists in destination\n",
    "    table_exists = spark.sql(f\"SHOW TABLES FROM {dst_catalog}.{dst_schema} LIKE '{dst_table}'\").count() > 0\n",
    "\n",
    "    # Check if the item_tag indicates a whole table update\n",
    "    is_wt_updt = item_tag is not None and \"wt_updt\" in item_tag.lower() # Check case-insensitively\n",
    "\n",
    "    # If table doesn't exist, ensure we have required parameters (unless wt_updt)\n",
    "    if not table_exists and not is_wt_updt:\n",
    "        if watermark_value is None:\n",
    "            watermark_value = \"2008-01-01\" # Default start for new incremental tables\n",
    "        if key_columns is None:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Table {full_dst_table} does not exist yet. Must specify key_columns explicitly (or use 'wt_updt' in item_tag).\"\n",
    "            }\n",
    "    elif not table_exists and is_wt_updt:\n",
    "         # For wt_updt, watermark and keys are not strictly needed even if table doesn't exist\n",
    "         # Set default watermark_value to None if not provided, as it's irrelevant\n",
    "         watermark_value = watermark_value # Keep provided value or None\n",
    "         watermark_column = watermark_column # Keep provided value or None\n",
    "         key_columns = key_columns # Keep provided value or None\n",
    "\n",
    "    # Check if table is already in the watermark table\n",
    "    existing_wm = spark.sql(f\"\"\"\n",
    "        SELECT watermark_id\n",
    "        FROM 6_mgmt.incr_updt.watermark\n",
    "        WHERE dst_catalog = '{dst_catalog}'\n",
    "          AND dst_schema = '{dst_schema}'\n",
    "          AND dst_table = '{dst_table}'\n",
    "          AND active_ind = 1\n",
    "    \"\"\").collect()\n",
    "\n",
    "    if existing_wm:\n",
    "        existing_id = existing_wm[0][\"watermark_id\"]\n",
    "        return {\n",
    "            \"status\": \"warning\",\n",
    "            \"message\": f\"Table {dst_table} already exists in the watermark table with ID {existing_id}.\",\n",
    "            \"watermark_id\": existing_id\n",
    "        }\n",
    "\n",
    "    # For tables that exist, perform column validation and auto-detection (unless wt_updt)\n",
    "    if table_exists and not is_wt_updt:\n",
    "        # Get all columns\n",
    "        columns_df = spark.sql(f\"DESCRIBE TABLE {full_dst_table}\")\n",
    "        all_columns = {row[\"col_name\"].lower(): row[\"data_type\"] for row in columns_df.collect()}\n",
    "\n",
    "        # Auto-detect watermark column if not specified\n",
    "        if watermark_column is None:\n",
    "            # Different patterns based on source\n",
    "            if src_server_name == 'oracle_mill':\n",
    "                watermark_candidates = [\"updt_dt_tm\", \"update_dt_tm\"]\n",
    "            elif src_server_name == 'BH2VMDWRL1' and src_database and 'datawarehouse' in str(src_database).lower():\n",
    "                watermark_candidates = [\"record_updated_dt\", \"record_update_dt\"]\n",
    "            elif src_database and 'iqemo' in str(src_database).lower():\n",
    "                watermark_candidates = [\"dateupdated\", \"lastupdate\"]\n",
    "            elif src_database and 'pacs' in str(src_database).lower():\n",
    "                watermark_candidates = [\"reportmodifieddateutc\", \"examinationfoldermodifydate\", \"reportdate\"]\n",
    "            elif src_database and 'badger' in str(src_database).lower():\n",
    "                watermark_candidates = [\"lastupdate\", \"recordtimestamp\"]\n",
    "            else:\n",
    "                watermark_candidates = [\n",
    "                    \"updt_dt_tm\", \"update_dt_tm\", \"record_updated_dt\", \"record_update_dt\",\n",
    "                    \"dateupdated\", \"lastupdate\", \"reportdate\", \"reportmodifieddateutc\",\n",
    "                    \"examinationfoldermodifydate\", \"recordtimestamp\"\n",
    "                ]\n",
    "\n",
    "            detected_wm_col = None\n",
    "            for col in watermark_candidates:\n",
    "                if col in all_columns:\n",
    "                    detected_wm_col = col.upper() # Use upper case as convention? Or keep original case? Let's stick to upper for consistency with original code.\n",
    "                    break\n",
    "\n",
    "            if detected_wm_col is None:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"Could not auto-detect a watermark column for {dst_table}. Please specify one explicitly or use 'wt_updt' in item_tag.\"\n",
    "                }\n",
    "            watermark_column = detected_wm_col # Assign detected column\n",
    "\n",
    "        elif watermark_column.lower() not in all_columns:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Specified watermark column '{watermark_column}' does not exist in table {dst_table}.\"\n",
    "            }\n",
    "\n",
    "        # Auto-detect key columns if not provided\n",
    "        if key_columns is None:\n",
    "            # Assuming detect_key_columns function exists and works\n",
    "            try:\n",
    "                 detected_keys = detect_key_columns(spark, full_dst_table)\n",
    "                 if not detected_keys: # If detection returns None or empty list\n",
    "                     return {\n",
    "                        \"status\": \"error\",\n",
    "                        \"message\": f\"Could not auto-detect key columns for {dst_table}. Please specify them explicitly or use 'wt_updt' in item_tag.\"\n",
    "                    }\n",
    "                 key_columns = detected_keys\n",
    "            except Exception as e:\n",
    "                 return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"Error during key column detection for {dst_table}: {e}. Please specify them explicitly or use 'wt_updt' in item_tag.\"\n",
    "                }\n",
    "\n",
    "\n",
    "        # Auto-determine watermark value if not specified (only if watermark column is valid)\n",
    "        if watermark_value is None and watermark_column:\n",
    "            try:\n",
    "                # Ensure watermark column name is quoted in case it contains special characters\n",
    "                max_ts_result = spark.sql(f\"SELECT MAX(`{watermark_column}`) as max_ts FROM {full_dst_table}\").collect()\n",
    "                if max_ts_result and max_ts_result[0][\"max_ts\"] is not None:\n",
    "                     max_ts = max_ts_result[0][\"max_ts\"]\n",
    "                     # Check if max_ts is a datetime or date object before subtracting timedelta\n",
    "                     if isinstance(max_ts, (datetime, date)):\n",
    "                         watermark_value = (max_ts - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "                     else:\n",
    "                         # Handle cases where it might be a string or number that looks like a date\n",
    "                         # This part might need refinement based on actual data types\n",
    "                         print(f\"Warning: Max value for {watermark_column} is not a date/datetime type ({type(max_ts)}). Setting default watermark.\")\n",
    "                         watermark_value = \"2008-01-01\"\n",
    "                else:\n",
    "                    watermark_value = \"2008-01-01\" # Default if table is empty or max is NULL\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not determine max watermark value for {watermark_column} in {full_dst_table}: {e}. Using default.\")\n",
    "                watermark_value = \"2008-01-01\"\n",
    "\n",
    "    # Ensure key_columns is a list if it's not None (and not wt_updt case where it might stay None)\n",
    "    if key_columns is not None and not isinstance(key_columns, list):\n",
    "        key_columns = [key_columns]\n",
    "\n",
    "    # Format values for SQL insertion, handling NULLs properly\n",
    "    comment_sql = f\"'{comment}'\" if comment is not None else \"NULL\"\n",
    "    src_database_sql = f\"'{src_database}'\" if src_database is not None else \"NULL\"\n",
    "    src_schema_sql = f\"'{src_schema}'\" if src_schema is not None else \"NULL\"\n",
    "    # IMPORTANT: Handle NULL for watermark column and value\n",
    "    watermark_column_sql = f\"'{watermark_column}'\" if watermark_column is not None else \"NULL\"\n",
    "    watermark_value_sql = f\"'{watermark_value}'\" if watermark_value is not None else \"NULL\"\n",
    "    # Ensure item_tag is quoted\n",
    "    item_tag_sql = f\"'{item_tag}'\" if item_tag is not None else \"NULL\"\n",
    "\n",
    "\n",
    "    # Create SQL queries\n",
    "    watermark_query = f\"\"\"\n",
    "    INSERT INTO 6_mgmt.incr_updt.watermark(\n",
    "        src_server_name, src_server_id, src_database, src_schema, src_table,\n",
    "        dst_catalog, dst_schema, dst_table,\n",
    "        watermark_column, watermark_value, item_tag, comment, active_ind, query_timeout\n",
    "    ) VALUES (\n",
    "        '{src_server_name}', {src_server_id}, {src_database_sql}, {src_schema_sql}, '{src_table}',\n",
    "        '{dst_catalog}', '{dst_schema}', '{dst_table}',\n",
    "        {watermark_column_sql}, {watermark_value_sql},\n",
    "        {item_tag_sql}, {comment_sql}, {active_ind}, '{query_timeout}'\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare key column queries (only if key_columns is not None and not empty)\n",
    "    key_col_queries = []\n",
    "    if key_columns: # Check handles None and empty list\n",
    "        key_col_queries = [f\"\"\"\n",
    "            INSERT INTO 6_mgmt.incr_updt.table_key_columns(\n",
    "                watermark_id, dst_table_name, key_column_name, active_ind\n",
    "            ) VALUES (\n",
    "                #watermark_id#, '{dst_table}', '{key_column}', 1\n",
    "            )\n",
    "        \"\"\" for key_column in key_columns] # Assumes key_columns is now a list\n",
    "\n",
    "\n",
    "    # --- Execution or Dry Run ---\n",
    "    if dry_run:\n",
    "        return {\n",
    "            \"status\": \"dry_run\",\n",
    "            \"watermark_query\": watermark_query,\n",
    "            \"key_column_queries\": key_col_queries if key_columns else [\"No key columns specified or needed (wt_updt).\"],\n",
    "            \"configuration\": {\n",
    "                \"src_server_name\": src_server_name,\n",
    "                \"src_server_id\": src_server_id,\n",
    "                \"src_database\": src_database,\n",
    "                \"src_schema\": src_schema,\n",
    "                \"src_table\": src_table,\n",
    "                \"dst_catalog\": dst_catalog,\n",
    "                \"dst_schema\": dst_schema,\n",
    "                \"dst_table\": dst_table,\n",
    "                \"key_columns\": key_columns, # Will be None or list\n",
    "                \"watermark_column\": watermark_column, # Will be None or string\n",
    "                \"watermark_value\": watermark_value, # Will be None or string\n",
    "                \"item_tag\": item_tag,\n",
    "                \"table_exists\": table_exists,\n",
    "                \"is_wt_updt\": is_wt_updt\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Execute queries\n",
    "    try:\n",
    "        spark.sql(watermark_query)\n",
    "\n",
    "        # Get the newly created watermark_id\n",
    "        watermark_id_result = spark.sql(f\"\"\"\n",
    "            SELECT MAX(watermark_id) as watermark_id\n",
    "            FROM 6_mgmt.incr_updt.watermark\n",
    "            WHERE dst_catalog = '{dst_catalog}'\n",
    "              AND dst_schema = '{dst_schema}'\n",
    "              AND dst_table = '{dst_table}'\n",
    "              AND active_ind = {active_ind} -- Match active_ind used in insert\n",
    "              AND src_server_name = '{src_server_name}' -- Add more conditions for robustness\n",
    "              AND src_table = '{src_table}'\n",
    "        \"\"\").collect()\n",
    "\n",
    "        if not watermark_id_result or watermark_id_result[0][\"watermark_id\"] is None:\n",
    "             raise ValueError(\"Could not retrieve watermark_id after insertion.\")\n",
    "        watermark_id = watermark_id_result[0][\"watermark_id\"]\n",
    "\n",
    "        # Insert key columns (only if key_columns were provided/detected)\n",
    "        if key_columns: # Check again before executing inserts\n",
    "            for key_column in key_columns:\n",
    "                spark.sql(f\"\"\"\n",
    "                    INSERT INTO 6_mgmt.incr_updt.table_key_columns(\n",
    "                        watermark_id, dst_table_name, key_column_name, active_ind\n",
    "                    ) VALUES (\n",
    "                        {watermark_id}, '{dst_table}', '{key_column}', 1\n",
    "                    )\n",
    "                \"\"\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"watermark_id\": watermark_id,\n",
    "            \"message\": f\"Successfully added {dst_table} to the ETL pipeline with watermark_id {watermark_id}.\"\n",
    "                       f\"{' (Whole Table Update)' if is_wt_updt else ''}\",\n",
    "            \"configuration\": {\n",
    "                \"src_server_name\": src_server_name,\n",
    "                \"src_server_id\": src_server_id,\n",
    "                \"src_database\": src_database,\n",
    "                \"src_schema\": src_schema,\n",
    "                \"src_table\": src_table,\n",
    "                \"dst_catalog\": dst_catalog,\n",
    "                \"dst_schema\": dst_schema,\n",
    "                \"dst_table\": dst_table,\n",
    "                \"key_columns\": key_columns,\n",
    "                \"watermark_column\": watermark_column,\n",
    "                \"watermark_value\": watermark_value,\n",
    "                \"item_tag\": item_tag,\n",
    "                \"is_wt_updt\": is_wt_updt\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Attempt to clean up if watermark record was inserted but keys failed (optional)\n",
    "        # spark.sql(f\"DELETE FROM 6_mgmt.incr_updt.watermark WHERE watermark_id = {watermark_id}\") # Be cautious with cleanup\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Error adding table to pipeline: {str(e)}\",\n",
    "            \"watermark_query\": watermark_query, # Include query for debugging\n",
    "            \"key_column_queries\": key_col_queries if key_columns else [] # Include queries for debugging\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf93c28-07d1-4355-ad34-00702ebd30bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_result = add_table_to_pipeline(\n",
    "    table_name=\"CDS_ECD_REF_CHIEF_COMPLAINT\",\n",
    "    dst_catalog=\"3_lookup\",\n",
    "    dst_schema=\"dwh\",\n",
    "    src_server_name=\"BH2VMDWRL1\",\n",
    "    src_server_id=104, \n",
    "    src_database=\"BH_DATAWAREHOUSE\",\n",
    "    src_schema=\"dbo\",\n",
    "    src_table=\"LKP_CDS_ECD_REF_CHIEF_COMPLAINT\",\n",
    "    key_columns=None,\n",
    "    watermark_column=None,\n",
    "    item_tag=\"dwh_wt_updt\",\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "print(\"Complete result:\")\n",
    "print(run_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb41230-9629-4397-9491-2549ea9dc7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_result = add_table_to_pipeline(\n",
    "    table_name=\"CDS_ECD_REF_INVESTIGATION\",\n",
    "    dst_catalog=\"3_lookup\",\n",
    "    dst_schema=\"dwh\",\n",
    "    src_server_name=\"BH2VMDWRL1\",\n",
    "    src_server_id=104, \n",
    "    src_database=\"BH_DATAWAREHOUSE\",\n",
    "    src_schema=\"dbo\",\n",
    "    src_table=\"LKP_CDS_ECD_REF_INVESTIGATION\",\n",
    "    key_columns=None,\n",
    "    watermark_column=None,\n",
    "    item_tag=\"dwh_wt_updt\",\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "print(\"Complete result:\")\n",
    "print(run_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1338914399116284,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Add Table to ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
