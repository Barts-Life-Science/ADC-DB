{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396cc064-7f34-4150-a93d-a4880f108825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook parameters\n",
    "\n",
    "params = {\n",
    "    \"proj_dir\": \"\"\n",
    "}\n",
    "\n",
    "# create text widgets\n",
    "for k in params.keys():\n",
    "    dbutils.widgets.text(k, \"\", \"\")\n",
    "\n",
    "# fetch values\n",
    "for k in params.keys():\n",
    "    params[k] = dbutils.widgets.get(k)\n",
    "    print(k, \":\", params[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303ffa7e-5984-4c88-92ee-14d31f60fa43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import pydicom\n",
    "import asyncio\n",
    "from functools import lru_cache\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d32184-ac3d-4f49-b1ae-de7dbf4cabcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/Volumes/1_inland/sectra/vone\"\n",
    "full_proj_path = os.path.join(ROOT_DIR, params[\"proj_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242bd47b-f7f3-4ae3-950d-f83dae5da8c7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768907926212}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scan_dirs = glob(f\"{full_proj_path}/*/*/\")\n",
    "from pyspark.sql import Row\n",
    "\n",
    "scan_df = spark.createDataFrame([Row(scan_dir=sd) for sd in scan_dirs])\n",
    "\n",
    "def file_01_exists(scan_dir):\n",
    "    if os.path.exists(os.path.join(scan_dir, \"000001.dcm\")):\n",
    "        return \"dbfs:\"+os.path.join(scan_dir, \"000001.dcm\")\n",
    "    else:\n",
    "        dcm_file = glob(os.path.join(scan_dir, \"*.dcm\"))[0]\n",
    "        return \"dbfs:\"+os.path.join(scan_dir, dcm_file)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType, StringType\n",
    "\n",
    "file_exists_udf = udf(file_01_exists, StringType())\n",
    "\n",
    "scan_df = scan_df.withColumn(\"dcm_file\", file_exists_udf(\"scan_dir\"))\n",
    "scan_df = scan_df.withColumnRenamed(\"scan_dir\", \"full_dir_path\")\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "scan_df = scan_df.withColumn(\"scan_dir\", expr(f\"replace(full_dir_path, '{full_proj_path}/', '')\"))\n",
    "display(scan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083861e9-46bd-4d7d-b2c9-56d791632577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collect file paths as a Python list using DataFrame API\n",
    "file_paths = scan_df.select(\"dcm_file\").toPandas()[\"dcm_file\"].tolist()\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .load(file_paths)\n",
    "    .withColumnRenamed(\"path\", \"dcm_file\")\n",
    ")\n",
    "\n",
    "scan_df = scan_df.join(df, on=\"dcm_file\", how=\"left\")\n",
    "#display(scan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a40e59-6df1-4811-a66d-c84fd4b6ed19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c328803-72e8-4611-bd35-add37ea29628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from pydicom.filebase import DicomFileLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f3c660d-0893-4d51-a2b5-5fd6925a876f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def process_dicom_batch(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        output_rows = []\n",
    "        for scan_dir, path, content in zip(pdf['scan_dir'], pdf['dcm_file'], pdf['content']):\n",
    "            try:\n",
    "                ds = pydicom.dcmread(BytesIO(content), force=True)\n",
    "                #ds.decompress()\n",
    "\n",
    "                try:\n",
    "                    accession_nbr = str(ds[\"AccessionNumber\"].value)\n",
    "                except:\n",
    "                    accession_nbr = None\n",
    "\n",
    "                try:\n",
    "                    study_id = str(ds[\"StudyID\"].value)\n",
    "                except:\n",
    "                    study_id = None\n",
    "\n",
    "                try:\n",
    "                    if \"ProcedureCodeSequence\" in ds:\n",
    "                        code_value = ds.ProcedureCodeSequence[0].CodeValue\n",
    "                    elif \"RequestedProcedureCodeSequence\" in ds:\n",
    "                        seq = ds.RequestedProcedureCodeSequence\n",
    "                        if len(seq) > 0 and \"CodeValue\" in seq[0]:\n",
    "                            code_value = seq[0].CodeValue\n",
    "                        else:\n",
    "                            code_value = None\n",
    "                    else:\n",
    "                        code_value = ds.RequestedProcedureID\n",
    "                except:\n",
    "                    code_value = None\n",
    "\n",
    "                try:\n",
    "                    description = str(ds[\"StudyDescription\"].value)\n",
    "                except:\n",
    "                    description = None\n",
    "                \n",
    "                output_rows.append((path, scan_dir, accession_nbr, study_id, code_value, description))\n",
    "            except:\n",
    "                output_rows.append((path, None, None, None, None, None))\n",
    "\n",
    "\n",
    "        yield pd.DataFrame(output_rows, columns=[\"dcm_file\", \"scan_dir\", \"accession_number\", \"study_id\", \"code_value\", \"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3b3dbe-ab8a-48f5-9738-558796bf8c82",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768917003519}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, BinaryType\n",
    "\n",
    "# Define output schema: path + redacted DICOM binary\n",
    "output_schema = StructType([\n",
    "    StructField(\"dcm_file\", StringType(), True),\n",
    "    StructField(\"scan_dir\", StringType(), True),\n",
    "    StructField(\"accession_number\", StringType(), True),\n",
    "    StructField(\"study_id\", StringType(), True),\n",
    "    StructField(\"code_value\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True)\n",
    "])\n",
    "\n",
    "md_df = scan_df.mapInPandas(process_dicom_batch, schema=output_schema)\n",
    "display(md_df.limit(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c7a46c-b64a-4f35-a051-bb757800c0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "md_df = md_df.drop(\"dcm_file\", \"study_id\")\n",
    "md_collected = md_df.collect()\n",
    "\n",
    "md_collected_df = pd.DataFrame(md_collected, columns=[\"scan_dir\", \"accession_number\", \"code_value\", \"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f786ea-a859-410a-8392-9da3bcdb88bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "md_collected_df.sort_values(\"scan_dir\").to_csv(\n",
    "    os.path.join(full_proj_path, 'dcm_metadata.csv'),\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "tqdm==4.66.4",
     "pydicom==3.0.1"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7766781477897800,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ExtractImageInfoPerScanV3",
   "widgets": {
    "proj_dir": {
     "currentValue": "pharos_20260114",
     "nuid": "a3d1de53-336f-41fc-9ef3-b705b63f77d9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "proj_dir",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "proj_dir",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
