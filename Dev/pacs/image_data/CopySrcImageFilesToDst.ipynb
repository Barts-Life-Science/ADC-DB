{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5455d0b1-9d3e-47cd-a63b-f0edb81b49f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.fileshare import ShareServiceClient\n",
    "import os\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F\n",
    "import asyncio\n",
    "from delta.tables import *\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a98879-b7a5-45b9-8599-94b9c382d9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_COPIES = 30 # when it was 10, it took avg 9s/iter; when 20, it took avg 12s/iter; when 30, it took avg 18s/iter\n",
    "RETRY_INTERVAL_HOURS = 12\n",
    "MAX_ITERATIONS = 1000\n",
    "MAX_TRIES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273e3efd-a6ab-4802-8ad9-97a856cd8d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = spark.sql(\"SELECT CURRENT_TIMESTAMP() AS start_time\").collect()[0][\"start_time\"]\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f25d685-b5ed-4ace-967a-e93cf60b5606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getSrcFileClient(src_path):\n",
    "    acc_name = dbutils.secrets.get(scope = \"adc_store\", key = \"pacs_intfileshare_accname\")\n",
    "    acc_key = dbutils.secrets.get(scope = \"adc_store\", key = \"pacs_intfileshare_acckey\")\n",
    "\n",
    "    # Connection string\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={acc_name};AccountKey={acc_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "    # File share name\n",
    "    share_name = \"intfileshare\"\n",
    "\n",
    "    # Get a share client via connection string\n",
    "    share_client = ShareServiceClient.from_connection_string(connection_string).get_share_client(share_name)\n",
    "\n",
    "    file_client = share_client.get_file_client(src_path)\n",
    "\n",
    "    return file_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112fd928-8310-4c82-88e8-fee829fc0e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "async def copySrcFileToDst(row):\n",
    "    src_path = os.path.join(row[\"src_root\"], row[\"src_proj_dir\"], row[\"src_subdirs\"], row[\"src_filename\"])\n",
    "\n",
    "    try:\n",
    "        file_client = getSrcFileClient(src_path)\n",
    "        file_bytes = file_client.download_file().readall()\n",
    "\n",
    "        # Make parent directory if not exist\n",
    "        Path(row[\"dst_filepath\"]).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Write to Databricks\n",
    "        with open(row[\"dst_filepath\"], \"wb\") as f:\n",
    "            f.write(file_bytes)\n",
    "    except Exception as e:\n",
    "        return (row[\"item_id\"], 'failed')\n",
    "    \n",
    "    return (row[\"item_id\"], 'done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231271af-26a2-4f1a-8c0a-03cd949b4e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_table(results):\n",
    "    result_df = spark.createDataFrame(data=results, schema=[\"item_id\", \"status\"])\n",
    "\n",
    "    dt = DeltaTable.forName(spark, \"1_inland.sectra.pacs_file_copy\")\n",
    "    merge_ret = dt.alias(\"t\")\\\n",
    "        .merge(result_df.alias(\"s\"), \"t.item_id == s.item_id\")\\\n",
    "        .whenMatchedUpdate(set ={\n",
    "            \"copy_status\": \"s.status\",\n",
    "            \"last_copy_run_at\": \"CURRENT_TIMESTAMP()\",\n",
    "            \"num_copy_tries\": \"t.num_copy_tries+1\",\n",
    "            \"process_status\": \"CASE WHEN s.status = 'done' THEN 'pending' ELSE t.process_status END\"\n",
    "        }).execute()\n",
    "\n",
    "async def async_update_table(results):\n",
    "    result_df = spark.createDataFrame(data=results, schema=[\"item_id\", \"status\"])\n",
    "\n",
    "    dt = DeltaTable.forName(spark, \"1_inland.sectra.pacs_file_copy\")\n",
    "    merge_ret = dt.alias(\"t\")\\\n",
    "        .merge(result_df.alias(\"s\"), \"t.item_id == s.item_id\")\\\n",
    "        .whenMatchedUpdate(set ={\n",
    "            \"copy_status\": \"s.status\",\n",
    "            \"last_copy_run_at\": \"CURRENT_TIMESTAMP()\",\n",
    "            \"num_copy_tries\": \"t.num_copy_tries+1\",\n",
    "            \"process_status\": \"CASE WHEN s.status = 'done' THEN 'pending' ELSE t.process_status END\"\n",
    "        }).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cfe736a-25ea-4b5b-8f1d-8d44233a523b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "est_num_iter = spark.sql(f\"\"\"\n",
    "    SELECT CAST(COUNT(*)/{MAX_CONCURRENT_COPIES} AS INT) + 1 AS estimation\n",
    "    FROM 1_inland.sectra.pacs_file_copy\n",
    "    WHERE \n",
    "        active_ind = 1 \n",
    "        AND LOWER(copy_status) != 'done'\n",
    "        AND num_copy_tries < {MAX_TRIES}\n",
    "        AND (\n",
    "            TIMEDIFF(HOUR, last_copy_run_at, CURRENT_TIMESTAMP()) > {RETRY_INTERVAL_HOURS}\n",
    "            OR last_copy_run_at IS NULL)\n",
    "\"\"\").collect()[0][\"estimation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c4ed29-eae4-43e8-87f9-26981b1ef6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bg_update_tasks = set()\n",
    "\n",
    "\n",
    "for _ in tqdm.tqdm(range(min(MAX_ITERATIONS, est_num_iter))):\n",
    "    df = spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM 1_inland.sectra.pacs_file_copy\n",
    "        WHERE \n",
    "            active_ind = 1 \n",
    "            AND LOWER(copy_status) != 'done'\n",
    "            AND num_copy_tries < {MAX_TRIES}\n",
    "            AND (\n",
    "                TIMEDIFF(HOUR, last_copy_run_at, CURRENT_TIMESTAMP()) > {RETRY_INTERVAL_HOURS}\n",
    "                OR last_copy_run_at IS NULL)\n",
    "        LIMIT {MAX_CONCURRENT_COPIES}     \n",
    "    \"\"\")\n",
    "\n",
    "    if df.count() > 0:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"\\nNo pending job. Exiting loop.\\n\")\n",
    "        break\n",
    "\n",
    "    coros = [copySrcFileToDst(row) for row in df.collect()]\n",
    "    results = await asyncio.gather(*coros)\n",
    "\n",
    "    # nonasychonous update\n",
    "    update_table(results)\n",
    "    \n",
    "    \n",
    "    # asynchronous update: ~10% faster\n",
    "    #task = asyncio.create_task(async_update_table(results))\n",
    "    #bg_update_tasks.add(task)\n",
    "    #task.add_done_callback(bg_update_tasks.discard)\n",
    "\n",
    "while len(bg_update_tasks) > 0:\n",
    "    print(f\"\\nwaiting for table update jobs (n={len(bg_update_tasks)})\")\n",
    "    time.sleep(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49902d5-5cfe-43dd-91b0-b83c90851eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "done_jobs = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS job_count\n",
    "    FROM 1_inland.sectra.pacs_file_copy\n",
    "    WHERE \n",
    "    copy_status = 'done'\n",
    "    AND last_copy_run_at > '{start_time}'\n",
    "\"\"\").collect()[0][\"job_count\"]\n",
    "print(\"Number of successful jobs:\", done_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909c2e0c-94bc-4225-aa51-5ce3b4509257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "failed_jobs = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS job_count\n",
    "    FROM 1_inland.sectra.pacs_file_copy\n",
    "    WHERE \n",
    "    copy_status = 'failed'\n",
    "    AND last_copy_run_at > '{start_time}'\n",
    "\"\"\").collect()[0][\"job_count\"]\n",
    "print(\"Number of failed jobs:\", failed_jobs)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8249580362378733,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CopySrcImageFilesToDst",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
