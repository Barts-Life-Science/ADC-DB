{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5455d0b1-9d3e-47cd-a63b-f0edb81b49f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.fileshare import ShareServiceClient\n",
    "import os\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F\n",
    "import asyncio\n",
    "from delta.tables import *\n",
    "import time\n",
    "import pydicom\n",
    "from functools import lru_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a98879-b7a5-45b9-8599-94b9c382d9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_COPIES = 20 # when it was 10, it took avg 9s/iter; when 20, it took avg 12s/iter; when 30, it took avg 18s/iter\n",
    "RETRY_INTERVAL_HOURS = 12\n",
    "MAX_ITERATIONS = 1000\n",
    "MAX_TRIES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273e3efd-a6ab-4802-8ad9-97a856cd8d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = spark.sql(\"SELECT CURRENT_TIMESTAMP() AS start_time\").collect()[0][\"start_time\"]\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f25d685-b5ed-4ace-967a-e93cf60b5606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getSrcFileClient(src_path):\n",
    "    acc_name = dbutils.secrets.get(scope = \"adc_store\", key = \"pacs_intfileshare_accname\")\n",
    "    acc_key = dbutils.secrets.get(scope = \"adc_store\", key = \"pacs_intfileshare_acckey\")\n",
    "\n",
    "    # Connection string\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={acc_name};AccountKey={acc_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "    # File share name\n",
    "    share_name = \"intfileshare\"\n",
    "\n",
    "    # Get a share client via connection string\n",
    "    share_client = ShareServiceClient.from_connection_string(connection_string).get_share_client(share_name)\n",
    "\n",
    "    file_client = share_client.get_file_client(src_path)\n",
    "\n",
    "    return file_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb521b2-bbcd-431a-b2f9-c706f3ba9275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=128, typed=False)\n",
    "def retrievePersonId(accession_nbr):\n",
    "    if accession_nbr is None:\n",
    "        return 'unknown'\n",
    "    \n",
    "    try:\n",
    "        person_id = spark.sql(f\"\"\"\n",
    "            SELECT MAX(MillPersonId) AS MillPersonId\n",
    "            FROM 4_prod.pacs.all_pacs_ref_nbr\n",
    "            WHERE refnbr = '{accession_nbr}'\n",
    "        \"\"\").first()[\"MillPersonId\"]\n",
    "    except:\n",
    "        person_id = 'unknown'\n",
    "    \n",
    "    if person_id is None:\n",
    "        person_id = 'unknown'\n",
    "    \n",
    "    return str(person_id)\n",
    "\n",
    "    \n",
    "\n",
    "def updateDstDicomPatientId(dst_filepath):\n",
    "    dcm = pydicom.dcmread(dst_filepath)\n",
    "\n",
    "    if str(dst_filepath.split(\"/\")[-1]).upper() == \"DICOMDIR\":\n",
    "        for x in dcm.DirectoryRecordSequence:\n",
    "            try:\n",
    "                nbr = x[\"AccessionNumber\"].value\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        pid = retrievePersonId(nbr)\n",
    "\n",
    "        for x in dcm.DirectoryRecordSequence:\n",
    "            try:\n",
    "                x[\"PatientID\"].value = pid\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    elif str(dst_filepath.split(\"/\")[-1].split(\".\")[-1]).lower() == \"dcm\":\n",
    "        try:\n",
    "            nbr = dcm[\"AccessionNumber\"].value\n",
    "        except:\n",
    "            nbr = None\n",
    "\n",
    "        dcm[\"PatientID\"].value = retrievePersonId(nbr)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            nbr = dcm[\"AccessionNumber\"].value\n",
    "        except:\n",
    "            nbr = None\n",
    "\n",
    "        dcm[\"PatientID\"].value = retrievePersonId(nbr)\n",
    "\n",
    "\n",
    "    dcm.save_as(dst_filepath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112fd928-8310-4c82-88e8-fee829fc0e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "async def copySrcFileToDst(row):\n",
    "    src_path = os.path.join(row[\"src_root\"], row[\"src_proj_dir\"], row[\"src_subdirs\"], row[\"src_filename\"])\n",
    "\n",
    "    # Check if the item is already copied\n",
    "    if row[\"copy_status\"] == \"done\":\n",
    "        copy_status = \"done\"\n",
    "    else:\n",
    "        try:\n",
    "            file_client = getSrcFileClient(src_path)\n",
    "            file_bytes = file_client.download_file().readall()\n",
    "\n",
    "            # Make parent directory if not exist\n",
    "            Path(row[\"dst_filepath\"]).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Write to Databricks\n",
    "            with open(row[\"dst_filepath\"], \"wb\") as f:\n",
    "                f.write(file_bytes)\n",
    "            \n",
    "            copy_status = \"done\"\n",
    "\n",
    "        except Exception as e:\n",
    "            copy_status = \"failed\"\n",
    "\n",
    "\n",
    "    # Delete source file after copying\n",
    "    if row[\"src_delete_status\"] == \"done\":\n",
    "        delete_status = \"done\"\n",
    "    elif copy_status != \"done\":\n",
    "        delete_status = \"pending\"\n",
    "    elif copy_status == \"done\" and not file_client.exists():\n",
    "        delete_status = \"done\"\n",
    "    else:\n",
    "        try:\n",
    "            # use the file client to delete it\n",
    "            #file_client.delete_file()\n",
    "            delete_status = \"done\"\n",
    "        except:\n",
    "            delete_status = \"failed\"\n",
    "\n",
    "    delete_status = \"pending\"\n",
    "\n",
    "    # Update Pateint ID in dst file\n",
    "    if row[\"process_status\"] == \"done\":\n",
    "        process_status = \"done\"\n",
    "    else:\n",
    "        try:\n",
    "            updateDstDicomPatientId(row[\"dst_filepath\"])\n",
    "            process_status = \"done\"\n",
    "        except:\n",
    "            process_status = \"failed\"\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    return (row[\"item_id\"], copy_status, process_status, delete_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231271af-26a2-4f1a-8c0a-03cd949b4e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_table(results):\n",
    "    result_df = spark.createDataFrame(data=results, schema=[\"item_id\", \"copy_status\", \"process_status\", \"delete_status\"])\n",
    "\n",
    "    dt = DeltaTable.forName(spark, \"1_inland.sectra.pacs_file_copy\")\n",
    "    merge_ret = dt.alias(\"t\")\\\n",
    "        .merge(result_df.alias(\"s\"), \"t.item_id == s.item_id\")\\\n",
    "        .whenMatchedUpdate(set ={\n",
    "            \"copy_status\": \"s.copy_status\",\n",
    "            \"last_copy_run_at\": \"CURRENT_TIMESTAMP()\",\n",
    "            \"num_copy_tries\": \"t.num_copy_tries+1\",\n",
    "            \"process_status\": \"s.process_status\",\n",
    "            \"last_process_run_at\": \"CURRENT_TIMESTAMP()\",\n",
    "            \"num_process_tries\": \"t.num_process_tries+1\",\n",
    "            \"src_delete_status\": \"s.delete_status\",\n",
    "            \"last_delete_run_at\": \"CURRENT_TIMESTAMP()\",\n",
    "            \"num_delete_tries\": \"t.num_delete_tries+1\"\n",
    "        }).execute()\n",
    "\n",
    "async def async_update_table(results):\n",
    "    result_df = spark.createDataFrame(data=results, schema=[\"item_id\", \"status\"])\n",
    "\n",
    "    dt = DeltaTable.forName(spark, \"1_inland.sectra.pacs_file_copy\")\n",
    "    merge_ret = dt.alias(\"t\")\\\n",
    "        .merge(result_df.alias(\"s\"), \"t.item_id == s.item_id\")\\\n",
    "        .whenMatchedUpdate(set ={\n",
    "            \"copy_status\": \"s.status\",\n",
    "            \"last_copy_run_at\": \"CURRENT_TIMESTAMP()\",\n",
    "            \"num_copy_tries\": \"t.num_copy_tries+1\",\n",
    "            \"process_status\": \"CASE WHEN s.status = 'done' THEN 'pending' ELSE t.process_status END\"\n",
    "        }).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cfe736a-25ea-4b5b-8f1d-8d44233a523b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "est_num_iter = spark.sql(f\"\"\"\n",
    "    SELECT CAST(COUNT(*)/{MAX_CONCURRENT_COPIES} AS INT) + 1 AS estimation\n",
    "    FROM 1_inland.sectra.pacs_file_copy\n",
    "    WHERE \n",
    "        active_ind = 1 \n",
    "        AND LOWER(copy_status) != 'done'\n",
    "        AND num_copy_tries < {MAX_TRIES}\n",
    "        AND (\n",
    "            TIMEDIFF(HOUR, last_copy_run_at, CURRENT_TIMESTAMP()) > {RETRY_INTERVAL_HOURS}\n",
    "            OR last_copy_run_at IS NULL)\n",
    "\"\"\").collect()[0][\"estimation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c4ed29-eae4-43e8-87f9-26981b1ef6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bg_update_tasks = set()\n",
    "\n",
    "\n",
    "for _ in tqdm.tqdm(range(min(MAX_ITERATIONS, est_num_iter))):\n",
    "    df = spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM 1_inland.sectra.pacs_file_copy\n",
    "        WHERE \n",
    "            active_ind = 1\n",
    "            AND (\n",
    "                    (\n",
    "                        LOWER(copy_status) != 'done'\n",
    "                        AND num_copy_tries < {MAX_TRIES}\n",
    "                        AND (TIMEDIFF(HOUR, last_copy_run_at, CURRENT_TIMESTAMP()) > {RETRY_INTERVAL_HOURS} OR last_copy_run_at IS NULL)\n",
    "                    ) OR (\n",
    "                        LOWER(copy_status) = 'done'\n",
    "                        AND LOWER(process_status) != 'done'\n",
    "                        AND num_process_tries < {MAX_TRIES}\n",
    "                        AND (TIMEDIFF(HOUR, last_process_run_at, CURRENT_TIMESTAMP()) > {RETRY_INTERVAL_HOURS} OR last_process_run_at IS NULL)\n",
    "                    ) OR (\n",
    "                        LOWER(copy_status) = 'done'\n",
    "                        AND LOWER(src_delete_status) != 'done'\n",
    "                        AND num_delete_tries < {MAX_TRIES}\n",
    "                        AND (TIMEDIFF(HOUR, last_delete_run_at, CURRENT_TIMESTAMP()) > {RETRY_INTERVAL_HOURS} OR last_delete_run_at IS NULL)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "        LIMIT {MAX_CONCURRENT_COPIES}     \n",
    "    \"\"\")\n",
    "\n",
    "    if df.count() > 0:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"\\nNo pending job. Exiting loop.\\n\")\n",
    "        break\n",
    "\n",
    "    coros = [copySrcFileToDst(row) for row in df.collect()]\n",
    "    results = await asyncio.gather(*coros)\n",
    "\n",
    "    # nonasychonous update\n",
    "    update_table(results)\n",
    "    \n",
    "    \n",
    "    # asynchronous update: ~10% faster\n",
    "    #task = asyncio.create_task(async_update_table(results))\n",
    "    #bg_update_tasks.add(task)\n",
    "    #task.add_done_callback(bg_update_tasks.discard)\n",
    "\n",
    "while len(bg_update_tasks) > 0:\n",
    "    print(f\"\\nwaiting for table update jobs (n={len(bg_update_tasks)})\")\n",
    "    time.sleep(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87185ec-700c-4d82-9162-100c35ef6e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49902d5-5cfe-43dd-91b0-b83c90851eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "done_jobs = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS job_count\n",
    "    FROM 1_inland.sectra.pacs_file_copy\n",
    "    WHERE \n",
    "    (copy_status = 'done'AND last_copy_run_at > '{start_time}')\n",
    "    OR (process_status = 'done' AND last_process_run_at > '{start_time}')\n",
    "\"\"\").collect()[0][\"job_count\"]\n",
    "print(\"Number of successful jobs:\", done_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909c2e0c-94bc-4225-aa51-5ce3b4509257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "failed_jobs = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS job_count\n",
    "    FROM 1_inland.sectra.pacs_file_copy\n",
    "    WHERE \n",
    "    (copy_status = 'failed' AND last_copy_run_at > '{start_time}')\n",
    "    OR (process_status = 'failed' AND last_process_run_at > '{start_time}')\n",
    "\"\"\").collect()[0][\"job_count\"]\n",
    "print(\"Number of failed jobs:\", failed_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d35764a-8e82-4c29-9ff4-f468214d7116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "failed_jobs = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM 1_inland.sectra.pacs_file_copy\n",
    "    WHERE \n",
    "    (copy_status = 'failed' AND last_copy_run_at > '{start_time}')\n",
    "    OR (process_status = 'failed' AND last_process_run_at > '{start_time}')\n",
    "    LIMIT 1000\n",
    "\"\"\")\n",
    "display(failed_jobs)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8249580362378733,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CopySrcImageFilesToDst",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
