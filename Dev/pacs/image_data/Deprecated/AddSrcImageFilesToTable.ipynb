{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457e6c84-a779-4c84-9d56-3bb592ff2cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook parameters\n",
    "\n",
    "params = {\n",
    "    \"src_proj_dir\": \"\",\n",
    "}\n",
    "\n",
    "# create text widgets\n",
    "for k in params.keys():\n",
    "    dbutils.widgets.text(k, \"\", \"\")\n",
    "\n",
    "# fetch values\n",
    "for k in params.keys():\n",
    "    params[k] = dbutils.widgets.get(k)\n",
    "    print(k, \":\", params[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e3e004-20d4-4035-8e60-ca96bb4862b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.fileshare import ShareServiceClient\n",
    "import os\n",
    "import pydicom\n",
    "from io import BytesIO\n",
    "from pydicom.fileset import FileSet\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from pyspark.sql import functions as F\n",
    "from math import ceil\n",
    "from pyspark.sql import types as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62cedcfd-4836-4c26-994f-3fe77d51c281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "acc_name = dbutils.secrets.get(scope = \"adc_store\", key = \"pacs_intfileshare_accname\")\n",
    "acc_key = dbutils.secrets.get(scope = \"adc_store\", key = \"pacs_intfileshare_acckey\")\n",
    "\n",
    "# Connection string\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={acc_name};AccountKey={acc_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# File share name\n",
    "share_name = \"intfileshare\"\n",
    "\n",
    "# Get a share client via connection string\n",
    "share_client = ShareServiceClient.from_connection_string(connection_string).get_share_client(share_name)\n",
    "\n",
    "# Source folder in the file share\n",
    "src_root = \"sectra\"\n",
    "\n",
    "# project dir\n",
    "src_proj_dir = params[\"src_proj_dir\"]\n",
    "\n",
    "# parent dir at destination\n",
    "dst_parent_dir = f\"/Volumes/1_inland/sectra/vone/{src_proj_dir.replace(' ', '_')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1e0ff1-a65c-4c42-968f-bcf43a6c6e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add DICOMDIR in subdirectories to the table\n",
    "items = list(share_client.list_directories_and_files(f\"{src_root}/{src_proj_dir}\"))\n",
    "item_names = [x['name'] for x in items if x['name'] != 'DICOMDIR']\n",
    "schema = T.StructType([T.StructField(\"src_subdirs\", T.StringType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(data = item_names, schema = schema)\n",
    "df.createOrReplaceTempView(\"temp_new_files\")\n",
    "\n",
    "\n",
    "\n",
    "# Insert DICOMDIR at subdir level to table\n",
    "q = f\"\"\"\n",
    "    INSERT INTO 1_inland.sectra.pacs_file_copy (\n",
    "        src_root, src_proj_dir, src_subdirs, src_filename,\n",
    "        dst_filepath, active_ind, copy_status, added_at, num_copy_tries, \n",
    "        process_status, num_process_tries, src_delete_status, num_delete_tries\n",
    "    )\n",
    "    SELECT\n",
    "        '{src_root}', '{src_proj_dir}', src_subdirs, 'DICOMDIR',\n",
    "        CONCAT('{dst_parent_dir}/',src_subdirs,'/DICOMDIR'), 1, 'new', CURRENT_TIMESTAMP(), 0,\n",
    "        'new', 0, 'new', 0\n",
    "    FROM temp_new_files\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ef1202-6f26-43ce-b6f7-89b25c3778b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download DICOMDIR at project directory\n",
    "file_client = share_client.get_file_client(f\"{src_root}/{src_proj_dir}/DICOMDIR\")\n",
    "file_bytes = file_client.download_file().readall()\n",
    "\n",
    "os.makedirs(dst_parent_dir, exist_ok=True)\n",
    "\n",
    "# Write DICOMDIR to Databricks\n",
    "with open(f\"{dst_parent_dir}/DICOMDIR\", \"wb\") as f:\n",
    "    f.write(file_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01334704-a66a-4562-a69d-242eea28d0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Insert the DICOMDIR at proj_dir to table\n",
    "q = f\"\"\"\n",
    "    INSERT INTO 1_inland.sectra.pacs_file_copy (\n",
    "        src_root, src_proj_dir, src_subdirs, src_filename,\n",
    "        dst_filepath, active_ind, copy_status, added_at, last_copy_run_at, num_copy_tries,\n",
    "        process_status, num_process_tries, src_delete_status, num_delete_tries\n",
    "    )\n",
    "    VALUES (\n",
    "        '{src_root}', '{src_proj_dir}', '.', 'DICOMDIR',\n",
    "        '{dst_parent_dir}/DICOMDIR', 1, 'done', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), 1,\n",
    "        'pending', 0, 'pending', 0\n",
    "    );          \n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50ebef13-bead-4d8c-9b12-c4554ffd45b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define schema for loading DICOMDIR\n",
    "schema = T.StructType([\n",
    "        T.StructField(\"src_subdirs\", T.StringType(), True),\n",
    "        T.StructField(\"src_file\", T.StringType(), True),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7254b6aa-7bca-4650-8fe4-968f127ee785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read DICOMDIR\n",
    "dicomdir = pydicom.dcmread(f\"{dst_parent_dir}/DICOMDIR\")\n",
    "\n",
    "seq_len = len(dicomdir.DirectoryRecordSequence)\n",
    "batch_size = 5000\n",
    "max_iter = ceil(seq_len/batch_size)\n",
    "\n",
    "# Use batch processing to avoid \"maximum recursion depth exceeded\" error\n",
    "for iter_ind in tqdm.tqdm(range(max_iter)):\n",
    "    i = iter_ind*batch_size\n",
    "    j = min(i+batch_size, seq_len)\n",
    "\n",
    "    # Create an empty data frame\n",
    "    df = spark.createDataFrame(data = [], schema = schema)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for x in dicomdir.DirectoryRecordSequence[i:j]:\n",
    "\n",
    "        # Retrieve ReferencedFileID which contains dcm file path info if exists\n",
    "        try:\n",
    "            refFileID = x[\"ReferencedFileID\"].value\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        data.append((f\"{refFileID[1]}/{refFileID[2]}\", refFileID[3]))\n",
    "\n",
    "    # Add source subdirs and dcm files\n",
    "    df = spark.createDataFrame(\n",
    "        data=data,\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    # Add destination file paths\n",
    "    df = df.withColumn(\"dst_filepath\", F.concat(F.lit(dst_parent_dir), F.lit(\"/\"), F.col(\"src_subdirs\"), F.lit(\"/\"), F.col(\"src_file\")))\n",
    "\n",
    "    df.createOrReplaceTempView(\"temp_new_files\")\n",
    "\n",
    "    # Insert dcm file paths to table\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO 1_inland.sectra.pacs_file_copy\n",
    "    (src_root, src_proj_dir, src_subdirs, src_filename,\n",
    "    dst_filepath, active_ind, copy_status, added_at, num_copy_tries,\n",
    "    process_status, num_process_tries, src_delete_status, num_delete_tries)\n",
    "    SELECT\n",
    "    '{src_root}','{src_proj_dir}',src_subdirs, src_file,\n",
    "    dst_filepath, 1, 'new', current_timestamp(), 0,\n",
    "    'new', 0, 'new', 0\n",
    "    FROM temp_new_files\n",
    "    \"\"\")\n",
    "\n",
    "#del dcm_file"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4538028075438869,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AddSrcImageFilesToTable",
   "widgets": {
    "src_proj_dir": {
     "currentValue": "Sectra-Concetta_bc3a878523df4a51a867fb99432e820a",
     "nuid": "1fe978a0-590f-461c-a819-a0e1b09f930b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "src_proj_dir",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "src_proj_dir",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
