{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1835f642-e1b8-407f-9c2b-781605d10610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io, re, random, string, time, os, traceback, tempfile, shutil, subprocess\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import broadcast\n",
    "from concurrent.futures import ProcessPoolExecutor, TimeoutError as _FutTimeout  \n",
    "import multiprocessing  \n",
    "import threading  \n",
    "\n",
    "# Optional libs\n",
    "try:\n",
    "    import magic\n",
    "except Exception:\n",
    "    magic = None\n",
    "\n",
    "import chardet\n",
    "from charset_normalizer import detect as cn_detect\n",
    "from ocflzw_decompress.lzw import LzwDecompress\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "from bs4 import BeautifulSoup\n",
    "import docx2txt\n",
    "from openpyxl import load_workbook\n",
    "import xlrd\n",
    "import pdfplumber\n",
    "from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "\n",
    "# OLE/MSG support\n",
    "try:\n",
    "    import olefile\n",
    "    HAVE_OLE = True\n",
    "except Exception:\n",
    "    HAVE_OLE = False\n",
    "\n",
    "try:\n",
    "    import extract_msg\n",
    "    HAVE_EXTRACT_MSG = True\n",
    "except Exception:\n",
    "    HAVE_EXTRACT_MSG = False\n",
    "\n",
    "\n",
    "try:  \n",
    "    import fitz  # PyMuPDF  \n",
    "    HAVE_FITZ = True  \n",
    "except Exception:  \n",
    "    HAVE_FITZ = False  \n",
    "  \n",
    "try:  \n",
    "    import pypdf  \n",
    "    HAVE_PYPDF = True  \n",
    "except Exception:  \n",
    "    HAVE_PYPDF = False  \n",
    "  \n",
    "try:  \n",
    "    import ocrmypdf  \n",
    "    HAVE_OCRMYPDF = True  \n",
    "except Exception:  \n",
    "    HAVE_OCRMYPDF = False  \n",
    "  \n",
    "try:  \n",
    "    import pytesseract  \n",
    "    from PIL import Image  \n",
    "    HAVE_TESS = True  \n",
    "except Exception:  \n",
    "    HAVE_TESS = False  \n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "SOURCE_TABLE = \"4_prod.raw.mill_ce_blob\"\n",
    "TARGET_TABLE = \"4_prod.bronze.mill_blob_text\"\n",
    "MAX_BLOB_SIZE = 16 * 1024 * 1024  # 16 MB limit per individual blob\n",
    "LZW_TIMEOUT_SECONDS = 30\n",
    "EVENT_LIMIT = None  # Set to None for all events\n",
    "BATCH_SIZE_BYTES = 1 * 1024 * 1024 * 1024  # 2 GB per batch (based on compressed size)\n",
    "MIN_BATCH_SIZE = 10  # Minimum events per batch to avoid too many tiny batches\n",
    "MAX_BATCH_SIZE = 5000  # Maximum events per batch to avoid memory issues\n",
    "EVENT_TIMEOUT_SECONDS = 240   # Adjust per your environment  \n",
    "EVENT_TIMEOUT_MODE = \"signal\"  # \"process\" causes pickle errors.\n",
    "CANDIDATE_MULTIPLIER = 1\n",
    "MAX_PARALLEL_BATCHES = 25\n",
    "\n",
    "OCF_MARKER = b'ocf_blob\\0'\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def _supports_posix_alarm():  \n",
    "    try:  \n",
    "        import signal, os  \n",
    "        return hasattr(signal, \"SIGALRM\") and os.name == \"posix\"  \n",
    "    except Exception:  \n",
    "        return False  \n",
    "  \n",
    "def run_with_signal_timeout(seconds, fn, *args, **kwargs):  \n",
    "    # Fast, low-overhead, iff available  \n",
    "    import signal  \n",
    "    class _Timeout(Exception): pass  \n",
    "    def _handler(signum, frame): raise _Timeout()  \n",
    "    old = signal.signal(signal.SIGALRM, _handler)  \n",
    "    signal.setitimer(signal.ITIMER_REAL, seconds)  \n",
    "    try:  \n",
    "        return fn(*args, **kwargs)  \n",
    "    finally:  \n",
    "        try:  \n",
    "            signal.setitimer(signal.ITIMER_REAL, 0)  \n",
    "        except Exception:  \n",
    "            pass  \n",
    "        signal.signal(signal.SIGALRM, old)  \n",
    "  \n",
    "def run_with_process_timeout(seconds, fn, *args, **kwargs):  \n",
    "    # Hard timeout: isolate the work in a child process and kill it on timeout.  \n",
    "    # Use spawn for safety inside Spark workers.  \n",
    "    ctx = multiprocessing.get_context(\"spawn\")  \n",
    "    with ctx.Pool(1) as pool:  \n",
    "        async_res = pool.apply_async(fn, args, kwargs or {})  \n",
    "        try:  \n",
    "            return async_res.get(timeout=seconds)  \n",
    "        except multiprocessing.TimeoutError:  \n",
    "            pool.terminate()  \n",
    "            raise TimeoutException(f\"Per-event timeout after {seconds}s\")  \n",
    "        except Exception:  \n",
    "            # Ensure pool is shut down on any error  \n",
    "            pool.terminate()  \n",
    "            raise  \n",
    "        \n",
    "def run_with_event_timeout(seconds, fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Run fn(*args, **kwargs) with a timeout. \n",
    "    Automatically detects Spark worker context and disables signal-based timeout.\n",
    "    \"\"\"\n",
    "    # Check if we're in a Spark worker (not main thread)\n",
    "    import threading\n",
    "    is_main_thread = threading.current_thread() is threading.main_thread()\n",
    "    \n",
    "    # In Spark workers, avoid both signal and process timeouts due to issues\n",
    "    if not is_main_thread:\n",
    "        # Just run without timeout in worker processes\n",
    "        return fn(*args, **kwargs)\n",
    "    \n",
    "    # In main thread, try signal-based timeout\n",
    "    if EVENT_TIMEOUT_MODE == \"signal\" and _supports_posix_alarm() and is_main_thread:\n",
    "        try:\n",
    "            return run_with_signal_timeout(seconds, fn, *args, **kwargs)\n",
    "        except Exception as e:\n",
    "            if \"signal only works in main thread\" in str(e):\n",
    "                # Fallback to no timeout\n",
    "                return fn(*args, **kwargs)\n",
    "            raise\n",
    "    \n",
    "    # Try process-based timeout\n",
    "    if EVENT_TIMEOUT_MODE == \"process\":\n",
    "        try:\n",
    "            return run_with_process_timeout(seconds, fn, *args, **kwargs)\n",
    "        except Exception as e:\n",
    "            # If pickle error, just run without timeout\n",
    "            if \"pickle\" in repr(e).lower():\n",
    "                print(f\"WARN: Timeout disabled due to pickle error: {e}\")\n",
    "                return fn(*args, **kwargs)\n",
    "            raise\n",
    "    \n",
    "    # Default: run without timeout\n",
    "    return fn(*args, **kwargs)\n",
    "\n",
    "# --------------------------\n",
    "# Timing utilities\n",
    "# --------------------------\n",
    "def log_time(message, start_time=None):\n",
    "    \"\"\"Print timestamped message with optional duration\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "    if start_time:\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"[{timestamp}] {message} (took {duration:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"[{timestamp}] {message}\")\n",
    "    return time.time()\n",
    "\n",
    "def force_materialize(df, operation_name):\n",
    "    \"\"\"Force Spark to materialize a dataframe and return count with timing\"\"\"\n",
    "    start = time.time()\n",
    "    count = df.count()\n",
    "    log_time(f\"{operation_name}: {count} rows\", start)\n",
    "    return count, df\n",
    "\n",
    "# --------------------------\n",
    "# Output schema (explicit to avoid inference issues)\n",
    "# --------------------------\n",
    "result_schema = T.StructType([\n",
    "    T.StructField(\"EVENT_ID\", T.LongType(), True),\n",
    "    T.StructField(\"VALID_UNTIL_DT_TM\", T.TimestampType(), True),\n",
    "    T.StructField(\"VALID_FROM_DT_TM\", T.TimestampType(), True),\n",
    "    T.StructField(\"UPDT_DT_TM\", T.TimestampType(), True),\n",
    "    T.StructField(\"UPDT_ID\", T.LongType(), True),\n",
    "    T.StructField(\"UPDT_TASK\", T.LongType(), True),\n",
    "    T.StructField(\"UPDT_CNT\", T.LongType(), True),\n",
    "    T.StructField(\"UPDT_APPLCTX\", T.LongType(), True),\n",
    "    T.StructField(\"LAST_UTC_TS\", T.TimestampType(), True),\n",
    "    T.StructField(\"ADC_UPDT\", T.TimestampType(), True),\n",
    "    T.StructField(\"BLOB_BINARY\", T.BinaryType(), True),\n",
    "    T.StructField(\"CONTENT_TYPE\", T.StringType(), True),\n",
    "    T.StructField(\"ENCODING\", T.StringType(), True),\n",
    "    T.StructField(\"BLOB_TEXT\", T.StringType(), True),\n",
    "    T.StructField(\"BINARY_SIZE\", T.LongType(), True),\n",
    "    T.StructField(\"TEXT_LENGTH\", T.LongType(), True),\n",
    "    T.StructField(\"STATUS\", T.StringType(), True),\n",
    "    T.StructField(\"anon_text\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "OCF_MARKER = b'ocf_blob\\0'\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    \"\"\"Context manager for timing out operations\"\"\"\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(f\"Operation timed out after {seconds} seconds\")\n",
    "    \n",
    "    # Set the signal handler and a alarm\n",
    "    old_handler = signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "        signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "def get_max_adc_updt(table_name, default_dt=datetime(1980, 1, 1)):  \n",
    "    try:  \n",
    "        row = spark.sql(f\"SELECT MAX(ADC_UPDT) AS max_dt FROM {table_name}\").first()  \n",
    "        max_dt = row[\"max_dt\"]  \n",
    "    except Exception:  \n",
    "        return default_dt  \n",
    "  \n",
    "    if max_dt is None or max_dt > datetime.now():  \n",
    "        return default_dt  \n",
    "    return max_dt  \n",
    "\n",
    "def format_size(size_bytes):\n",
    "    size_bytes = float(size_bytes or 0)\n",
    "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_bytes < 1024.0:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024.0\n",
    "    return f\"{size_bytes:.2f} PB\"\n",
    "\n",
    "def combine_blob_chunks(chunks):\n",
    "    \"\"\"Combine blob chunks into single bytes object\"\"\"\n",
    "    combined = bytearray()\n",
    "    for chunk in chunks or []:\n",
    "        if chunk is not None:\n",
    "            combined.extend(chunk)\n",
    "    return bytes(combined)\n",
    "\n",
    "def remove_ocf_wrapper_aggressive(data: bytes):\n",
    "    \"\"\"Remove ALL occurrences of OCF marker - needed for multi-chunk files\"\"\"\n",
    "    try:\n",
    "        if not data:\n",
    "            return data\n",
    "        # Remove trailing marker first\n",
    "        if data.endswith(OCF_MARKER):\n",
    "            data = data[:-len(OCF_MARKER)]\n",
    "        # Remove ALL embedded markers (critical for multi-chunk files)\n",
    "        if OCF_MARKER in data:\n",
    "            data = b''.join(data.split(OCF_MARKER))\n",
    "        return data\n",
    "    except Exception:\n",
    "        return data\n",
    "\n",
    "def remove_ocf_wrapper_conservative(data: bytes):\n",
    "    \"\"\"Only remove trailing OCF marker\"\"\"\n",
    "    try:\n",
    "        if not data:\n",
    "            return data\n",
    "        if data.endswith(OCF_MARKER):\n",
    "            return data[:-len(OCF_MARKER)]\n",
    "        return data\n",
    "    except Exception:\n",
    "        return data\n",
    "\n",
    "def decompress_lzw_with_timeout(data: bytes, timeout_seconds=LZW_TIMEOUT_SECONDS):\n",
    "    \"\"\"Decompress LZW with timeout protection\"\"\"\n",
    "    try:\n",
    "        # Skip timeout on very small data or if timeout is disabled\n",
    "        if timeout_seconds <= 0 or len(data) < 100000:  # < 100KB\n",
    "            return bytes(LzwDecompress().decompress(data))\n",
    "        \n",
    "        # For larger data, use timeout but handle gracefully\n",
    "        try:\n",
    "            with time_limit(timeout_seconds):\n",
    "                return bytes(LzwDecompress().decompress(data))\n",
    "        except:\n",
    "            # If timeout fails (e.g., in Databricks), try without it\n",
    "            return bytes(LzwDecompress().decompress(data))\n",
    "    except TimeoutException as e:\n",
    "        raise TimeoutException(str(e))\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def decompress_blob_improved(raw: bytes, compression_cd, chunk_count=1, has_duplicates=False):\n",
    "    \"\"\"\n",
    "    Improved decompression with better handling of multi-chunk files.\n",
    "    Now considers chunk_count and duplicate sequences to determine strategy.\n",
    "    \"\"\"\n",
    "    if not raw:\n",
    "        return None, \"Empty content\"\n",
    "    \n",
    "    # Ensure int comparison\n",
    "    try:\n",
    "        cd = int(compression_cd) if compression_cd is not None else None\n",
    "    except Exception:\n",
    "        cd = None\n",
    "    \n",
    "    try:\n",
    "        if cd == 728:  # LZW\n",
    "            # Count OCF markers to determine strategy\n",
    "            ocf_count = raw.count(OCF_MARKER)\n",
    "            \n",
    "            # CRITICAL: For very large multi-chunk files, ONLY try aggressive cleanup\n",
    "            if chunk_count > 10 or ocf_count > 10:\n",
    "                try:\n",
    "                    cleaned_aggressive = remove_ocf_wrapper_aggressive(raw)\n",
    "                    # Fix D: Use decompress_lzw_with_timeout\n",
    "                    out = decompress_lzw_with_timeout(cleaned_aggressive)\n",
    "                    return out, None\n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)\n",
    "                    \n",
    "                    if \"list assignment index\" in error_msg:\n",
    "                        return None, f\"LZW decompression failed (likely corrupted data after {chunk_count} chunks): {error_msg}\"\n",
    "                    elif \"bytes must be in range\" in error_msg:\n",
    "                        return None, f\"LZW decompression failed (invalid byte values after OCF cleanup): {error_msg}\"\n",
    "                    else:\n",
    "                        # Try fallback methods only for non-corruption errors\n",
    "                        try:\n",
    "                            cleaned_conservative = remove_ocf_wrapper_conservative(raw)\n",
    "                            # Fix D: Use decompress_lzw_with_timeout\n",
    "                            out = decompress_lzw_with_timeout(cleaned_conservative)\n",
    "                            return out, None\n",
    "                        except:\n",
    "                            try:\n",
    "                                # Fix D: Use decompress_lzw_with_timeout\n",
    "                                out = decompress_lzw_with_timeout(raw)\n",
    "                                return out, None\n",
    "                            except:\n",
    "                                return None, f\"LZW failed all attempts for {chunk_count}-chunk file: {error_msg}\"\n",
    "            \n",
    "            else:\n",
    "                # For smaller files, try all methods and pick best\n",
    "                results = []\n",
    "                \n",
    "                # Method 1: Aggressive cleanup (usually best)\n",
    "                try:\n",
    "                    cleaned_aggressive = remove_ocf_wrapper_aggressive(raw)\n",
    "                    # Fix D: Use decompress_lzw_with_timeout\n",
    "                    out = decompress_lzw_with_timeout(cleaned_aggressive)\n",
    "                    results.append((out, \"aggressive\", len(out)))\n",
    "                except Exception as e:\n",
    "                    results.append((None, \"aggressive\", str(e)))\n",
    "                \n",
    "                # Method 2: Conservative cleanup\n",
    "                try:\n",
    "                    cleaned_conservative = remove_ocf_wrapper_conservative(raw)\n",
    "                    # Fix D: Use decompress_lzw_with_timeout\n",
    "                    out = decompress_lzw_with_timeout(cleaned_conservative)\n",
    "                    results.append((out, \"conservative\", len(out)))\n",
    "                except Exception as e:\n",
    "                    results.append((None, \"conservative\", str(e)))\n",
    "                \n",
    "                # Method 3: Raw (no cleanup)\n",
    "                try:\n",
    "                    # Fix D: Use decompress_lzw_with_timeout\n",
    "                    out = decompress_lzw_with_timeout(raw)\n",
    "                    results.append((out, \"raw\", len(out)))\n",
    "                except Exception as e:\n",
    "                    results.append((None, \"raw\", str(e)))\n",
    "                \n",
    "                # Pick the best result (prefer larger output)\n",
    "                successful = [(r, m, s) for r, m, s in results if r is not None]\n",
    "                \n",
    "                if successful:\n",
    "                    successful.sort(key=lambda x: x[2], reverse=True)\n",
    "                    best = successful[0]\n",
    "                    return best[0], None\n",
    "                else:\n",
    "                    errors = [f\"{m}: {s}\" for r, m, s in results if r is None]\n",
    "                    return None, f\"LZW failed all attempts - {'; '.join(errors)}\"\n",
    "                            \n",
    "        elif cd == 727:  # No compression\n",
    "            # For uncompressed data, OCF markers MUST be removed as they corrupt content\n",
    "            if raw.count(OCF_MARKER) > 0:\n",
    "                cleaned = remove_ocf_wrapper_aggressive(raw)\n",
    "                return cleaned, None\n",
    "            else:\n",
    "                return raw, None\n",
    "        else:\n",
    "            return None, f\"Unknown compression type: {compression_cd}\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Decompression error: {str(e)}\"\n",
    "\n",
    "def calculate_printable_ratio(text, sample_size=1000):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    if len(text) <= sample_size:\n",
    "        sample = text\n",
    "    else:\n",
    "        sample = ''.join(random.choice(text) for _ in range(sample_size))\n",
    "    printable = sum(1 for c in sample if c in string.printable)\n",
    "    return printable / len(sample) if sample else 0.0\n",
    "\n",
    "def guess_text(content: bytes):\n",
    "    if not content:\n",
    "        return None, None, 0.0\n",
    "    \n",
    "    ch = chardet.detect(content) or {}\n",
    "    cn = cn_detect(content) or {}\n",
    "    candidates = [ch.get('encoding'), cn.get('encoding'), 'utf-8', 'windows-1252', 'latin-1', 'ascii']\n",
    "    best_decoded, best_encoding, best_ratio = None, None, 0.0\n",
    "    \n",
    "    for enc in candidates:\n",
    "        if not enc:\n",
    "            continue\n",
    "        try:\n",
    "            decoded = content.decode(enc, errors='ignore')\n",
    "            r = calculate_printable_ratio(decoded)\n",
    "            if r > best_ratio:\n",
    "                best_ratio, best_decoded, best_encoding = r, decoded, enc\n",
    "            if best_ratio > 0.95:\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return best_decoded, best_encoding, best_ratio\n",
    "\n",
    "def detect_mime(content: bytes):\n",
    "    if not content:\n",
    "        return 'application/octet-stream'\n",
    "    \n",
    "    if content.startswith(b'%PDF-'):\n",
    "        return 'application/pdf'\n",
    "    \n",
    "    if content.startswith(b'\\x50\\x4B\\x03\\x04'):\n",
    "        head = content[:4096]\n",
    "        if b'word/' in head:\n",
    "            return 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n",
    "        if b'xl/' in head:\n",
    "            return 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "        if b'ppt/' in head:\n",
    "            return 'application/vnd.openxmlformats-officedocument.presentationml.presentation'\n",
    "        return 'application/zip'\n",
    "    \n",
    "    if content.startswith(b'\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1'):\n",
    "        return 'application/x-ole-storage'\n",
    "    \n",
    "    if content.startswith(b'{\\\\'):\n",
    "        return 'text/rtf'\n",
    "    \n",
    "    if magic:\n",
    "        try:\n",
    "            return magic.Magic(mime=True).from_buffer(content) or 'application/octet-stream'\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return 'application/octet-stream'\n",
    "\n",
    "# [Keep all the OLE helpers and extractors unchanged]\n",
    "def classify_ole(data: bytes):\n",
    "    if not (HAVE_OLE and data):\n",
    "        return 'application/x-ole-storage'\n",
    "    \n",
    "    try:\n",
    "        with olefile.OleFileIO(io.BytesIO(data)) as ole:\n",
    "            if ole.exists('WordDocument'):\n",
    "                return 'application/msword'\n",
    "            if ole.exists('Workbook') or ole.exists('Book'):\n",
    "                return 'application/vnd.ms-excel'\n",
    "            if ole.exists('PowerPoint Document'):\n",
    "                return 'application/vnd.ms-powerpoint'\n",
    "            if ole.exists('__properties_version1.0') and (\n",
    "                ole.exists('__recip_version1.0') or ole.exists('__attach_version1.0')\n",
    "            ):\n",
    "                return 'application/vnd.ms-outlook'\n",
    "            return 'application/x-ole-storage'\n",
    "    except Exception:\n",
    "        return 'application/x-ole-storage'\n",
    "\n",
    "def refine_mime_with_ole(content_type, data: bytes):\n",
    "    if content_type == 'application/x-ole-storage':\n",
    "        return classify_ole(data)\n",
    "    return content_type\n",
    "\n",
    "def extract_text_from_ole_doc(data: bytes):\n",
    "    exe = shutil.which('antiword')\n",
    "    if exe:\n",
    "        with tempfile.NamedTemporaryFile(suffix='.doc', delete=False) as tmp:\n",
    "            tmp.write(data)\n",
    "            path = tmp.name\n",
    "        try:\n",
    "            res = subprocess.run([exe, '-w', '0', path], capture_output=True, timeout=120)\n",
    "            if res.returncode == 0:\n",
    "                out = res.stdout.decode('utf-8', errors='ignore')\n",
    "                if out.strip():\n",
    "                    return out\n",
    "        finally:\n",
    "            try:\n",
    "                os.unlink(path)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    exe = shutil.which('catdoc')\n",
    "    if exe:\n",
    "        with tempfile.NamedTemporaryFile(suffix='.doc', delete=False) as tmp:\n",
    "            tmp.write(data)\n",
    "            path = tmp.name\n",
    "        try:\n",
    "            res = subprocess.run([exe, '-w', path], capture_output=True, timeout=120)\n",
    "            if res.returncode == 0:\n",
    "                out = res.stdout.decode('utf-8', errors='ignore')\n",
    "                if out.strip():\n",
    "                    return out\n",
    "        finally:\n",
    "            try:\n",
    "                os.unlink(path)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_text_from_ole_ppt(data: bytes):\n",
    "    exe = shutil.which('catppt')\n",
    "    if not exe:\n",
    "        return None\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.ppt', delete=False) as tmp:\n",
    "        tmp.write(data)\n",
    "        path = tmp.name\n",
    "    \n",
    "    try:\n",
    "        res = subprocess.run([exe, path], capture_output=True, timeout=120)\n",
    "        if res.returncode == 0:\n",
    "            return res.stdout.decode('utf-8', errors='ignore')\n",
    "    finally:\n",
    "        try:\n",
    "            os.unlink(path)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_text_from_msg(data: bytes):\n",
    "    if not HAVE_EXTRACT_MSG:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(suffix='.msg', delete=False) as tmp:\n",
    "            tmp.write(data)\n",
    "            path = tmp.name\n",
    "        \n",
    "        try:\n",
    "            m = extract_msg.Message(path)\n",
    "            subj = (m.subject or '').strip()\n",
    "            body = (m.body or '').strip()\n",
    "            text = (subj + '\\n\\n' + body).strip()\n",
    "            return text or None\n",
    "        finally:\n",
    "            try:\n",
    "                os.unlink(path)\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_docx(content):\n",
    "    try:\n",
    "        return docx2txt.process(io.BytesIO(content))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_excel(content):\n",
    "    try:\n",
    "        wb = load_workbook(io.BytesIO(content), read_only=True, data_only=True)\n",
    "        parts = []\n",
    "        for sheet in wb.sheetnames:\n",
    "            ws = wb[sheet]\n",
    "            for row in ws.iter_rows(values_only=True):\n",
    "                row_text = ' '.join(str(cell) for cell in row if cell is not None)\n",
    "                if row_text.strip():\n",
    "                    parts.append(row_text)\n",
    "        return '\\n'.join(parts)\n",
    "    except Exception:\n",
    "        try:\n",
    "            workbook = xlrd.open_workbook(file_contents=content)\n",
    "            parts = []\n",
    "            for sheet in workbook.sheets():\n",
    "                for r in range(sheet.nrows):\n",
    "                    parts.append(' '.join(str(cell.value) for cell in sheet.row(r)))\n",
    "            return '\\n'.join(parts)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def extract_pdf_with_pdftotext(content: bytes, layout=True):  \n",
    "    exe = shutil.which('pdftotext')  \n",
    "    if not exe:  \n",
    "        return None  \n",
    "    with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as f:  \n",
    "        f.write(content)  \n",
    "        in_path = f.name  \n",
    "    try:  \n",
    "        args = [exe, \"-enc\", \"UTF-8\"]  \n",
    "        if layout:  \n",
    "            args.append(\"-layout\")  \n",
    "        args.extend([in_path, \"-\"])  # write to stdout  \n",
    "        res = subprocess.run(args, capture_output=True, timeout=180)  \n",
    "        if res.returncode == 0:  \n",
    "            out = res.stdout.decode('utf-8', errors='ignore')  \n",
    "            return out if out.strip() else None  \n",
    "        return None  \n",
    "    finally:  \n",
    "        try: os.unlink(in_path)  \n",
    "        except: pass  \n",
    "  \n",
    "def extract_pdf_with_pypdf(content: bytes):  \n",
    "    if not HAVE_PYPDF:  \n",
    "        return None  \n",
    "    try:  \n",
    "        reader = pypdf.PdfReader(io.BytesIO(content), strict=False)  \n",
    "        if getattr(reader, \"is_encrypted\", False):  \n",
    "            try:  \n",
    "                reader.decrypt(\"\")  # empty password often works  \n",
    "            except Exception:  \n",
    "                pass  \n",
    "        parts = []  \n",
    "        for page in reader.pages:  \n",
    "            t = page.extract_text() or \"\"  \n",
    "            if t.strip():  \n",
    "                parts.append(t)  \n",
    "        return \"\\n\".join(parts) if parts else None  \n",
    "    except Exception:  \n",
    "        return None  \n",
    "  \n",
    "def extract_pdf_with_pymupdf(content: bytes):  \n",
    "    if not HAVE_FITZ:  \n",
    "        return None  \n",
    "    try:  \n",
    "        doc = fitz.open(stream=content, filetype=\"pdf\")  \n",
    "        if doc.needs_pass:  \n",
    "            try:  \n",
    "                doc.authenticate(\"\")  # empty password  \n",
    "            except Exception:  \n",
    "                pass  \n",
    "        parts = []  \n",
    "        for page in doc:  \n",
    "            t = page.get_text(\"text\") or \"\"  \n",
    "            if t.strip():  \n",
    "                parts.append(t)  \n",
    "        return \"\\n\".join(parts) if parts else None  \n",
    "    except Exception:  \n",
    "        return None  \n",
    "  \n",
    "def repair_pdf_with_qpdf(content: bytes):  \n",
    "    exe = shutil.which('qpdf')  \n",
    "    if not exe:  \n",
    "        return None  \n",
    "    with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as fi:  \n",
    "        fi.write(content)  \n",
    "        in_path = fi.name  \n",
    "    out_fd, out_path = tempfile.mkstemp(suffix='.pdf')  \n",
    "    os.close(out_fd)  \n",
    "    try:  \n",
    "        # --decrypt with empty password, disable object streams, uncompress streams  \n",
    "        args = [exe, \"--password=\", \"--decrypt\",  \n",
    "                \"--object-streams=disable\", \"--stream-data=uncompress\",  \n",
    "                in_path, out_path]  \n",
    "        res = subprocess.run(args, capture_output=True, timeout=180)  \n",
    "        if res.returncode == 0 and os.path.exists(out_path):  \n",
    "            with open(out_path, \"rb\") as f:  \n",
    "                return f.read()  \n",
    "        return None  \n",
    "    finally:  \n",
    "        for p in (in_path, out_path):  \n",
    "            try: os.unlink(p)  \n",
    "            except: pass  \n",
    "  \n",
    "def ocr_pdf_best_effort(content: bytes, max_pages=5, lang=\"eng\"):  \n",
    "    # Prefer ocrmypdf sidecar text if available (fast and good quality)  \n",
    "    if HAVE_OCRMYDF := HAVE_OCRMYPDF:  \n",
    "        in_f = tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False)  \n",
    "        out_pdf = tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False)  \n",
    "        sidecar = tempfile.NamedTemporaryFile(suffix=\".txt\", delete=False)  \n",
    "        in_f.write(content); in_f.close(); out_pdf.close(); sidecar.close()  \n",
    "        try:  \n",
    "            args = [\"ocrmypdf\", \"-l\", lang, \"--sidecar\", sidecar.name,  \n",
    "                    \"--optimize\", \"0\", \"--tesseract-timeout\", \"120\",  \n",
    "                    in_f.name, out_pdf.name]  \n",
    "            res = subprocess.run(args, capture_output=True, timeout=1200)  \n",
    "            if res.returncode == 0 and os.path.exists(sidecar.name):  \n",
    "                txt = open(sidecar.name, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()  \n",
    "                return txt if txt.strip() else None  \n",
    "        finally:  \n",
    "            for p in (in_f.name, out_pdf.name, sidecar.name):  \n",
    "                try: os.unlink(p)  \n",
    "                except: pass  \n",
    "        return None  \n",
    "    # Fallback: render pages with PyMuPDF and OCR via pytesseract  \n",
    "    if not (HAVE_FITZ and HAVE_TESS):  \n",
    "        return None  \n",
    "    try:  \n",
    "        doc = fitz.open(stream=content, filetype=\"pdf\")  \n",
    "        if doc.needs_pass:  \n",
    "            try: doc.authenticate(\"\")  \n",
    "            except Exception: pass  \n",
    "        parts = []  \n",
    "        n = min(max_pages, len(doc))  \n",
    "        for i in range(n):  \n",
    "            page = doc[i]  \n",
    "            # 300 DPI render is a reasonable trade-off  \n",
    "            pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))  \n",
    "            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))  \n",
    "            t = pytesseract.image_to_string(img, lang=lang) or \"\"  \n",
    "            if t.strip():  \n",
    "                parts.append(t)  \n",
    "        return \"\\n\".join(parts) if parts else None  \n",
    "    except Exception:  \n",
    "        return None  \n",
    "\n",
    "# Fix C: Add OCR for images\n",
    "def ocr_image_best_effort(content: bytes, lang=\"eng\", max_frames=10):\n",
    "    \"\"\"OCR images including multi-page TIFFs\"\"\"\n",
    "    if not HAVE_TESS:\n",
    "        return None\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(content))\n",
    "        texts = []\n",
    "        n_frames = getattr(img, \"n_frames\", 1)\n",
    "        for i in range(min(n_frames, max_frames)):\n",
    "            try:\n",
    "                img.seek(i)\n",
    "            except Exception:\n",
    "                break\n",
    "            # Convert to a good mode for OCR\n",
    "            frame = img.convert(\"L\")\n",
    "            t = pytesseract.image_to_string(frame, lang=lang) or \"\"\n",
    "            if t.strip():\n",
    "                texts.append(t)\n",
    "        return \"\\n\".join(texts) if texts else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def pdf_page_stats(content: bytes):\n",
    "    stats = {\n",
    "        'pages': None,\n",
    "        'text_pages': 0,\n",
    "        'image_only_pages': 0,\n",
    "        'images_total': 0,\n",
    "        'has_any_text': False,\n",
    "        'encrypted': False,\n",
    "    }\n",
    "    \n",
    "    # Prefer PyMuPDF\n",
    "    if HAVE_FITZ:\n",
    "        try:\n",
    "            doc = fitz.open(stream=content, filetype=\"pdf\")\n",
    "            stats['encrypted'] = doc.needs_pass\n",
    "            pages = len(doc)\n",
    "            stats['pages'] = pages\n",
    "            for page in doc:\n",
    "                t = page.get_text(\"text\") or \"\"\n",
    "                if t.strip():\n",
    "                    stats['text_pages'] += 1\n",
    "                else:\n",
    "                    imgs = page.get_images(full=True)\n",
    "                    if imgs:\n",
    "                        stats['image_only_pages'] += 1\n",
    "                        stats['images_total'] += len(imgs)\n",
    "            stats['has_any_text'] = stats['text_pages'] > 0\n",
    "            return stats\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Fallback to pypdf (no image counting, just text presence)\n",
    "    if HAVE_PYPDF:\n",
    "        try:\n",
    "            reader = pypdf.PdfReader(io.BytesIO(content), strict=False)\n",
    "            if getattr(reader, \"is_encrypted\", False):\n",
    "                try:\n",
    "                    reader.decrypt(\"\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            stats['pages'] = len(reader.pages)\n",
    "            for pg in reader.pages:\n",
    "                t = pg.extract_text() or \"\"\n",
    "                if t.strip():\n",
    "                    stats['text_pages'] += 1\n",
    "            stats['has_any_text'] = stats['text_pages'] > 0\n",
    "            return stats\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def parse_pdf(content: bytes):\n",
    "    # Quick classification: is there any real text?\n",
    "    stats = pdf_page_stats(content)\n",
    "\n",
    "    # If there appears to be text, try text extractors first  \n",
    "    if stats.get('has_any_text', False):  \n",
    "        # 1) pdfplumber  \n",
    "        try:  \n",
    "            if pdfplumber:  \n",
    "                with io.BytesIO(content) as f:  \n",
    "                    with pdfplumber.open(f) as pdf:  \n",
    "                        texts = []  \n",
    "                        for page in pdf.pages:  \n",
    "                            try:  \n",
    "                                # Prefer words -> lines  \n",
    "                                words = page.extract_words(  \n",
    "                                    x_tolerance=3, y_tolerance=3,  \n",
    "                                    keep_blank_chars=False, use_text_flow=False  \n",
    "                                )  \n",
    "                                if words:  \n",
    "                                    lines = []  \n",
    "                                    current_y = None  \n",
    "                                    line_words = []  \n",
    "                                    for w in words:  \n",
    "                                        y = float(w.get('top', 0))  \n",
    "                                        if current_y is None or abs(y - current_y) > 3:  \n",
    "                                            if line_words:  \n",
    "                                                line_words.sort(key=lambda ww: float(ww.get('x0', 0)))  \n",
    "                                                lines.append(' '.join(ww['text'] for ww in line_words))  \n",
    "                                            line_words = [w]  \n",
    "                                            current_y = y  \n",
    "                                        else:  \n",
    "                                            line_words.append(w)  \n",
    "                                    if line_words:  \n",
    "                                        line_words.sort(key=lambda ww: float(ww.get('x0', 0)))  \n",
    "                                        lines.append(' '.join(ww['text'] for ww in line_words))  \n",
    "                                    if lines:  \n",
    "                                        texts.extend(lines)  \n",
    "                                else:  \n",
    "                                    t = page.extract_text() or ''  \n",
    "                                    if t.strip():  \n",
    "                                        texts.append(t)  \n",
    "                            except Exception:  \n",
    "                                continue  \n",
    "                        txt = '\\n'.join(t for t in texts if t is not None)  \n",
    "                        if txt and txt.strip():  \n",
    "                            return txt  \n",
    "        except Exception:  \n",
    "            pass  \n",
    "        \n",
    "        # 2) pdfminer (standalone)  \n",
    "        try:  \n",
    "            if pdfminer_extract_text:  \n",
    "                txt = pdfminer_extract_text(io.BytesIO(content)) or ''  \n",
    "                if txt.strip():  \n",
    "                    return txt  \n",
    "        except Exception:  \n",
    "            pass  \n",
    "        \n",
    "        # 3) Poppler pdftotext  \n",
    "        txt = extract_pdf_with_pdftotext(content, layout=True)  \n",
    "        if txt:  \n",
    "            return txt  \n",
    "        \n",
    "        # Try -raw as a second attempt (helps some PDFs)  \n",
    "        txt = extract_pdf_with_pdftotext(content, layout=False)  \n",
    "        if txt:  \n",
    "            return txt  \n",
    "        \n",
    "        # 4) pypdf  \n",
    "        txt = extract_pdf_with_pypdf(content)  \n",
    "        if txt:  \n",
    "            return txt  \n",
    "        \n",
    "        # 5) PyMuPDF text  \n",
    "        txt = extract_pdf_with_pymupdf(content)  \n",
    "        if txt:  \n",
    "            return txt  \n",
    "        \n",
    "        # 6) Repair/decrypt then retry core extractors  \n",
    "        repaired = repair_pdf_with_qpdf(content)  \n",
    "        if repaired:  \n",
    "            txt = extract_pdf_with_pdftotext(repaired, layout=True) or extract_pdf_with_pdftotext(repaired, layout=False)  \n",
    "            if txt:  \n",
    "                return txt  \n",
    "            txt = extract_pdf_with_pypdf(repaired) or extract_pdf_with_pymupdf(repaired)  \n",
    "            if txt:  \n",
    "                return txt  \n",
    "        \n",
    "        # If we thought it had text but none of the above worked, fall through to OCR as a last resort  \n",
    "        ocr = ocr_pdf_best_effort(content, max_pages=min(10, stats.get('pages') or 10), lang=\"eng\")  \n",
    "        if ocr and ocr.strip():  \n",
    "            return \"[OCR]\\n\" + ocr  \n",
    "        return \"[PDF Content - Error extracting text]\"  \n",
    "\n",
    "    # If it looks like image-only (no real text detected), OCR immediately  \n",
    "    if stats.get('pages') is not None and stats['text_pages'] == 0:  \n",
    "        ocr = ocr_pdf_best_effort(content, max_pages=min(20, stats['pages']), lang=\"eng\")  \n",
    "        if ocr and ocr.strip():  \n",
    "            return \"[OCR]\\n\" + ocr  \n",
    "        # Provide a clearer note when OCR is unavailable  \n",
    "        if HAVE_OCRMYPDF or HAVE_TESS:  \n",
    "            return \"[PDF appears image-only; OCR attempted but no text was produced]\"  \n",
    "        else:  \n",
    "            return \"[PDF appears image-only; OCR not available on this cluster]\"  \n",
    "\n",
    "    # If we couldn't classify, try the extractors anyway  \n",
    "    txt = extract_pdf_with_pdftotext(content, layout=True) or extract_pdf_with_pypdf(content) or extract_pdf_with_pymupdf(content)  \n",
    "    if txt and txt.strip():  \n",
    "        return txt  \n",
    "    \n",
    "    repaired = repair_pdf_with_qpdf(content)  \n",
    "    if repaired:  \n",
    "        txt = extract_pdf_with_pdftotext(repaired, layout=True) or extract_pdf_with_pypdf(repaired) or extract_pdf_with_pymupdf(repaired)  \n",
    "        if txt and txt.strip():  \n",
    "            return txt  \n",
    "    \n",
    "    # Final OCR attempt  \n",
    "    ocr = ocr_pdf_best_effort(content, max_pages=10, lang=\"eng\")  \n",
    "    if ocr and ocr.strip():  \n",
    "        return \"[OCR]\\n\" + ocr  \n",
    "    \n",
    "    return \"[PDF Content - Error extracting text]\"  \n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    cleaned = re.sub(r'<%.*?%>', '', text, flags=re.DOTALL)\n",
    "    cleaned = cleaned.replace('|', '\\n')\n",
    "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
    "    cleaned = re.sub(r'\\n+', '\\n', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_text_from_binary(content: bytes, content_type: str):\n",
    "    if not content or len(content) < 1:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if content_type == 'application/pdf':\n",
    "            return parse_pdf(content)\n",
    "        elif content_type in ('application/vnd.openxmlformats-officedocument.wordprocessingml.document',):\n",
    "            return extract_text_from_docx(content)\n",
    "        elif content_type in ('application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'):\n",
    "            return extract_text_from_excel(content)\n",
    "        elif content_type == 'text/rtf':\n",
    "            try:\n",
    "                return rtf_to_text(content.decode('latin-1', errors='ignore'))\n",
    "            except Exception:\n",
    "                return None\n",
    "        elif content_type in ('text/html', 'text/xml', 'application/xhtml+xml'):\n",
    "            txt, enc, ratio = guess_text(content)\n",
    "            if txt:\n",
    "                soup = BeautifulSoup(txt, 'html.parser')\n",
    "                return soup.get_text(separator='\\n', strip=True)\n",
    "            return None\n",
    "        elif content_type.startswith('text/'):\n",
    "            txt, enc, ratio = guess_text(content)\n",
    "            return clean_text(txt) if txt else None\n",
    "        elif content_type in ('application/msword',):\n",
    "            return extract_text_from_ole_doc(content)\n",
    "        elif content_type in ('application/vnd.ms-powerpoint',):\n",
    "            return extract_text_from_ole_ppt(content)\n",
    "        elif content_type in ('application/vnd.ms-outlook',):\n",
    "            return extract_text_from_msg(content)\n",
    "        elif content_type in ('application/x-ole-storage',):\n",
    "            refined = refine_mime_with_ole(content_type, content)\n",
    "            if refined != content_type:\n",
    "                return extract_text_from_binary(content, refined)\n",
    "            return None\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return f\"[Binary Content - Extraction Error: {str(e)}]\"\n",
    "\n",
    "# Fix C: Updated parse_blob_content with OCR support\n",
    "def parse_blob_content(content: bytes, provided_type=None):\n",
    "    if not content:\n",
    "        return None, None, None\n",
    "    \n",
    "    content_type = provided_type or detect_mime(content)\n",
    "    content_type = refine_mime_with_ole(content_type, content)\n",
    "    \n",
    "    # OCR images\n",
    "    if content_type and content_type.startswith('image/'):\n",
    "        txt = ocr_image_best_effort(content, lang=\"eng\")\n",
    "        if txt and txt.strip():\n",
    "            return clean_text(txt), content_type, \"utf-8\"\n",
    "        else:\n",
    "            return (\"[Image appears to require OCR; OCR {}]\".format(\n",
    "                        \"not available\" if not HAVE_TESS else \"attempted but produced no text\"\n",
    "                    ),\n",
    "                    content_type, None)\n",
    "    \n",
    "    if content_type == 'application/zip':\n",
    "        return f\"[{content_type} Content]\", content_type, None\n",
    "    \n",
    "    extracted_text = extract_text_from_binary(content, content_type)\n",
    "    if extracted_text and isinstance(extracted_text, str):\n",
    "        return clean_text(extracted_text), content_type, 'utf-8'\n",
    "    \n",
    "    decoded, best_enc, ratio = guess_text(content)\n",
    "    if not decoded:\n",
    "        return f\"[Binary data, unable to decode. Best printable ratio: {ratio:.2f}]\", content_type, None\n",
    "    \n",
    "    if content_type == \"text/rtf\":\n",
    "        return rtf_to_text(decoded), content_type, best_enc\n",
    "    elif content_type in [\"text/html\", \"text/xml\", \"application/xhtml+xml\"]:\n",
    "        soup = BeautifulSoup(decoded, 'html.parser')\n",
    "        return soup.get_text(separator='\\n', strip=True), content_type, best_enc\n",
    "    else:\n",
    "        return clean_text(decoded), content_type, best_enc\n",
    "\n",
    "def safe_numeric(value, default=None):\n",
    "    if value is None or value == '':\n",
    "        return default\n",
    "    try:\n",
    "        if isinstance(value, (int, float)):\n",
    "            return int(value)\n",
    "        return int(float(str(value)))\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def create_result_dict(row, decompressed=None, content_type=None, encoding=None, blob_text=None, status=\"Error\"):\n",
    "    event_id = safe_numeric(row['EVENT_ID'])\n",
    "    updt_id = safe_numeric(row['UPDT_ID'])\n",
    "    updt_task = safe_numeric(row['UPDT_TASK'])\n",
    "    updt_cnt = safe_numeric(row['UPDT_CNT'])\n",
    "    updt_applctx = safe_numeric(row['UPDT_APPLCTX'])\n",
    "    \n",
    "    binary_size = len(decompressed) if isinstance(decompressed, (bytes, bytearray)) else None\n",
    "    text_length = len(blob_text) if isinstance(blob_text, str) else None\n",
    "    \n",
    "    def safe_encode(text):\n",
    "        if text is None:\n",
    "            return None\n",
    "        if isinstance(text, str):\n",
    "            return text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "        try:\n",
    "            return str(text)\n",
    "        except Exception:\n",
    "            return \"[Non-string content]\"\n",
    "    \n",
    "    return {\n",
    "        \"EVENT_ID\": event_id,\n",
    "        \"VALID_UNTIL_DT_TM\": row['VALID_UNTIL_DT_TM'],\n",
    "        \"VALID_FROM_DT_TM\": row['VALID_FROM_DT_TM'],\n",
    "        \"UPDT_DT_TM\": row['UPDT_DT_TM'],\n",
    "        \"UPDT_ID\": updt_id,\n",
    "        \"UPDT_TASK\": updt_task,\n",
    "        \"UPDT_CNT\": updt_cnt,\n",
    "        \"UPDT_APPLCTX\": updt_applctx,\n",
    "        \"LAST_UTC_TS\": row['LAST_UTC_TS'],\n",
    "        \"ADC_UPDT\": row['ADC_UPDT'],\n",
    "        \"BLOB_BINARY\": bytes(decompressed) if isinstance(decompressed, (bytes, bytearray)) else None,\n",
    "        \"CONTENT_TYPE\": content_type,\n",
    "        \"ENCODING\": encoding,\n",
    "        \"BLOB_TEXT\": safe_encode(blob_text),\n",
    "        \"BINARY_SIZE\": binary_size,\n",
    "        \"TEXT_LENGTH\": text_length,\n",
    "        \"STATUS\": str(status),\n",
    "        \"anon_text\": None\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "udf_output_schema = T.StructType([\n",
    "    T.StructField(\"EVENT_ID\", T.LongType(), True),\n",
    "    T.StructField(\"VALID_UNTIL_DT_TM\", T.TimestampType(), True),\n",
    "    T.StructField(\"VALID_FROM_DT_TM\", T.TimestampType(), True),\n",
    "    T.StructField(\"UPDT_DT_TM\", T.TimestampType(), True),\n",
    "    T.StructField(\"UPDT_ID\", T.LongType(), True),\n",
    "    T.StructField(\"UPDT_TASK\", T.LongType(), True),\n",
    "    T.StructField(\"UPDT_CNT\", T.LongType(), True),\n",
    "    T.StructField(\"UPDT_APPLCTX\", T.LongType(), True),\n",
    "    T.StructField(\"LAST_UTC_TS\", T.TimestampType(), True),\n",
    "    T.StructField(\"ADC_UPDT\", T.TimestampType(), True),\n",
    "    T.StructField(\"BLOB_BINARY\", T.BinaryType(), True),\n",
    "    T.StructField(\"CONTENT_TYPE\", T.StringType(), True),\n",
    "    T.StructField(\"ENCODING\", T.StringType(), True),\n",
    "    T.StructField(\"BLOB_TEXT\", T.StringType(), True),\n",
    "    T.StructField(\"BINARY_SIZE\", T.LongType(), True),\n",
    "    T.StructField(\"TEXT_LENGTH\", T.LongType(), True),\n",
    "    T.StructField(\"STATUS\", T.StringType(), True),\n",
    "    T.StructField(\"anon_text\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "def _build_result_for_event(first_row: dict, chunks: list):  \n",
    "    \"\"\"  \n",
    "    Pure function that builds a single output row (dict) for an EVENT_ID.  \n",
    "    'first_row' is a plain dict of the group's first row (no pandas objects).  \n",
    "    'chunks' is a list of bytes (blob chunks) for this event.  \n",
    "    \"\"\"  \n",
    "    event_id = safe_numeric(first_row.get('EVENT_ID'))  \n",
    "  \n",
    "    # Aggregate chunk info  \n",
    "    total_blob_length = sum(len(c) for c in chunks if c is not None)  \n",
    "    chunk_count = len(chunks)  \n",
    "    compression_cd = first_row.get('COMPRESSION_CD')  \n",
    "  \n",
    "    # Enforce compressed size  \n",
    "    if total_blob_length > MAX_BLOB_SIZE:  \n",
    "        return create_result_dict(first_row, status=f\"Compressed Too Large: {total_blob_length} bytes\")  \n",
    "  \n",
    "    # Combine and decompress  \n",
    "    blob_contents = combine_blob_chunks(chunks)  \n",
    "    decompressed, dec_err = decompress_blob_improved(  \n",
    "        blob_contents,  \n",
    "        compression_cd,  \n",
    "        chunk_count,  \n",
    "        False  # has_duplicates  \n",
    "    )  \n",
    "    if decompressed is None:  \n",
    "        return create_result_dict(first_row, status=dec_err or \"Decompression returned None\")  \n",
    "  \n",
    "    # Enforce decompressed size  \n",
    "    if isinstance(decompressed, (bytes, bytearray)) and len(decompressed) > MAX_BLOB_SIZE:  \n",
    "        return create_result_dict(first_row, status=f\"Decompressed too large: {len(decompressed)} bytes\")  \n",
    "  \n",
    "    # MIME + parse  \n",
    "    content_type = detect_mime(decompressed)  \n",
    "    content_type = refine_mime_with_ole(content_type, decompressed)  \n",
    "    blob_text, detected_type, encoding = parse_blob_content(decompressed, content_type)  \n",
    "    if detected_type:  \n",
    "        content_type = detected_type  \n",
    "  \n",
    "\n",
    "    if blob_text:\n",
    "        # Check for known error indicators\n",
    "        if isinstance(blob_text, str):\n",
    "            if blob_text.startswith(\"[Binary Content - \"):\n",
    "                status = 'Binary extraction error'\n",
    "            elif blob_text.startswith(\"[PDF Content - Error\"):\n",
    "                status = 'PDF extraction error'\n",
    "            elif blob_text.startswith(\"[Binary data, unable to decode\"):\n",
    "                status = 'Unable to decode binary'\n",
    "            elif blob_text.startswith(\"[LZW failed all attempts - aggressive\"):\n",
    "                status = 'LZW Failed all attempts'\n",
    "            elif blob_text.startswith(\"[LZW decompression failed\"):\n",
    "                status = 'LZW decompression failed'\n",
    "            elif blob_text.startswith(\"[PDF appears image-only\"):\n",
    "                status = 'PDF appears image-only'\n",
    "            elif blob_text.startswith(\"[Image appears to require OCR\"):\n",
    "                status = 'Image appears to require OCR'            \n",
    "            elif blob_text.startswith(\"Error: 'charmap'\"):\n",
    "                status = 'Charmapping Error'  \n",
    "            elif blob_text.startswith(\"Error:\"):\n",
    "                status = 'Error:' + blob_text[:200]  \n",
    "            elif blob_text.startswith(\"[\") and \"appears\" in blob_text and \"]\" in blob_text[:200]:\n",
    "                status = 'Error: ' + blob_text[:200]\n",
    "            elif blob_text.startswith(\"[OCR]\"):\n",
    "                status = 'Decoded'\n",
    "            else:\n",
    "                status = 'Decoded'\n",
    "        else:\n",
    "            status = 'Decoded'\n",
    "    else:\n",
    "        status = 'Failed to decode'\n",
    "\n",
    "  \n",
    "    # Safe encode  \n",
    "    if blob_text is not None and isinstance(blob_text, str):  \n",
    "        blob_text = blob_text.encode('utf-8', errors='ignore').decode('utf-8')  \n",
    "  \n",
    "    return create_result_dict(  \n",
    "        first_row,  \n",
    "        decompressed=decompressed,  \n",
    "        content_type=content_type,  \n",
    "        encoding=encoding,  \n",
    "        blob_text=blob_text,  \n",
    "        status=str(status)  \n",
    "    )  \n",
    "\n",
    "# Fix B: Updated process_blob_batch to use process-based timeout\n",
    "@pandas_udf(returnType=udf_output_schema, functionType=PandasUDFType.GROUPED_MAP)  \n",
    "def process_blob_batch(pdf):  \n",
    "    \"\"\"  \n",
    "    Pandas UDF to process a batch of blob records.  \n",
    "    Each group represents one EVENT_ID with all its chunks.  \n",
    "    Uses process-based timeout instead of SIGALRM.  \n",
    "    \"\"\"  \n",
    "    results = []  \n",
    "  \n",
    "    for _, group_data in pdf.groupby('EVENT_ID'):  \n",
    "        first_row = group_data.iloc[0]  \n",
    "        event_id = first_row['EVENT_ID']  \n",
    "  \n",
    "        try:  \n",
    "            chunks = []  \n",
    "            for _, row in group_data.iterrows():  \n",
    "                if pd.notna(row.get('BLOB_CONTENTS')):  \n",
    "                    chunks.append(row['BLOB_CONTENTS'])  \n",
    "  \n",
    "            event_timeout_seconds = int(globals().get('EVENT_TIMEOUT_SECONDS', 240) or 0)  \n",
    "  \n",
    "            # Convert first_row to a plain dict so it's picklable for subprocess  \n",
    "            first_row_dict = {k: (v.item() if hasattr(v, \"item\") else v) for k, v in first_row.to_dict().items()}  \n",
    "  \n",
    "            if event_timeout_seconds > 0:  \n",
    "                try:  \n",
    "                    result = run_with_event_timeout(event_timeout_seconds, _build_result_for_event, first_row_dict, chunks)  \n",
    "                except TimeoutException:  \n",
    "                    result = {  \n",
    "                        \"EVENT_ID\": event_id,  \n",
    "                        \"VALID_UNTIL_DT_TM\": first_row.get('VALID_UNTIL_DT_TM'),  \n",
    "                        \"VALID_FROM_DT_TM\": first_row.get('VALID_FROM_DT_TM'),  \n",
    "                        \"UPDT_DT_TM\": first_row.get('UPDT_DT_TM'),  \n",
    "                        \"UPDT_ID\": safe_numeric(first_row.get('UPDT_ID')),  \n",
    "                        \"UPDT_TASK\": safe_numeric(first_row.get('UPDT_TASK')),  \n",
    "                        \"UPDT_CNT\": safe_numeric(first_row.get('UPDT_CNT')),  \n",
    "                        \"UPDT_APPLCTX\": safe_numeric(first_row.get('UPDT_APPLCTX')),  \n",
    "                        \"LAST_UTC_TS\": first_row.get('LAST_UTC_TS'),  \n",
    "                        \"ADC_UPDT\": first_row.get('ADC_UPDT'),  \n",
    "                        \"BLOB_BINARY\": None,  \n",
    "                        \"CONTENT_TYPE\": None,  \n",
    "                        \"ENCODING\": None,  \n",
    "                        \"BLOB_TEXT\": None,  \n",
    "                        \"BINARY_SIZE\": None,  \n",
    "                        \"TEXT_LENGTH\": None,  \n",
    "                        \"STATUS\": f\"Timeout after {event_timeout_seconds}s\",  \n",
    "                        \"anon_text\": None  \n",
    "                    }  \n",
    "            else:  \n",
    "                result = _build_result_for_event(first_row_dict, chunks)  \n",
    "  \n",
    "            results.append(result)  \n",
    "  \n",
    "        except Exception as e:  \n",
    "            results.append({  \n",
    "                \"EVENT_ID\": event_id,  \n",
    "                \"VALID_UNTIL_DT_TM\": first_row.get('VALID_UNTIL_DT_TM'),  \n",
    "                \"VALID_FROM_DT_TM\": first_row.get('VALID_FROM_DT_TM'),  \n",
    "                \"UPDT_DT_TM\": first_row.get('UPDT_DT_TM'),  \n",
    "                \"UPDT_ID\": safe_numeric(first_row.get('UPDT_ID')),  \n",
    "                \"UPDT_TASK\": safe_numeric(first_row.get('UPDT_TASK')),  \n",
    "                \"UPDT_CNT\": safe_numeric(first_row.get('UPDT_CNT')),  \n",
    "                \"UPDT_APPLCTX\": safe_numeric(first_row.get('UPDT_APPLCTX')),  \n",
    "                \"LAST_UTC_TS\": first_row.get('LAST_UTC_TS'),  \n",
    "                \"ADC_UPDT\": first_row.get('ADC_UPDT'),  \n",
    "                \"BLOB_BINARY\": None,  \n",
    "                \"CONTENT_TYPE\": None,  \n",
    "                \"ENCODING\": None,  \n",
    "                \"BLOB_TEXT\": None,  \n",
    "                \"BINARY_SIZE\": None,  \n",
    "                \"TEXT_LENGTH\": None,  \n",
    "                \"STATUS\": f\"Error: {str(e)}\",  \n",
    "                \"anon_text\": None  \n",
    "            })  \n",
    "  \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def run_parallel_processing(batch_limit=100000, cutoff_date=None):\n",
    "    \"\"\"Main function to run parallel blob processing with incremental ADC_UPDT processing\"\"\"\n",
    "    \n",
    "    script_start = time.time()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PARALLEL BLOB PROCESSOR - INCREMENTAL MODE\")\n",
    "    print(f\"Processing batch of up to {batch_limit:,} new events\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Get the cutoff date if not provided\n",
    "    if cutoff_date is None:\n",
    "        cutoff_date = get_max_adc_updt(TARGET_TABLE, default_dt=datetime(1980, 1, 1))\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Processing events with ADC_UPDT > {cutoff_date}\")\n",
    "    \n",
    "    # Get new EVENT_IDs from source table\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Finding new events...\")\n",
    "    new_events = (spark.table(SOURCE_TABLE)\n",
    "                  .filter(F.col(\"ADC_UPDT\") > cutoff_date)\n",
    "                  .select(\"EVENT_ID\")\n",
    "                  .distinct())\n",
    "    \n",
    "    new_count = new_events.count()\n",
    "    if new_count == 0:\n",
    "        print(\"No new events to process\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Found {new_count:,} new events total\")\n",
    "    \n",
    "    # Inspect more candidates than we will actually process to build a bytes-based batch\n",
    "    candidate_limit = min(max(batch_limit * CANDIDATE_MULTIPLIER, batch_limit), new_count)\n",
    "    print(f\"Pulling {candidate_limit:,} candidate EVENT_IDs for prefiltering...\")\n",
    "    \n",
    "    # Order by ADC_UPDT to process oldest first\n",
    "    candidates = (spark.table(SOURCE_TABLE)\n",
    "                  .filter(F.col(\"ADC_UPDT\") > cutoff_date)\n",
    "                  .select(\"EVENT_ID\", \"ADC_UPDT\")\n",
    "                  .distinct()\n",
    "                  .orderBy(\"ADC_UPDT\", \"EVENT_ID\")\n",
    "                  .limit(candidate_limit)\n",
    "                  .select(\"EVENT_ID\")\n",
    "                  .cache())\n",
    "    _ = candidates.count()\n",
    "\n",
    "    # Step 2: Read only metadata needed for dedup + sizing (NO BLOB_CONTENTS)\n",
    "    META_COLS = [\n",
    "        \"EVENT_ID\", \"BLOB_SEQ_NUM\",\n",
    "        \"VALID_UNTIL_DT_TM\", \"VALID_FROM_DT_TM\",\n",
    "        \"UPDT_DT_TM\", \"UPDT_ID\", \"UPDT_TASK\", \"UPDT_CNT\", \"UPDT_APPLCTX\",\n",
    "        \"LAST_UTC_TS\", \"ADC_UPDT\", \"COMPRESSION_CD\", \"BLOB_LENGTH\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loading lightweight metadata for candidates...\")\n",
    "    raw_meta = (spark.table(SOURCE_TABLE)\n",
    "            .join(F.broadcast(candidates), on=\"EVENT_ID\", how=\"inner\")\n",
    "            .select(*[c for c in META_COLS if c in spark.table(SOURCE_TABLE).columns]))\n",
    "    \n",
    "    # Ensure BLOB_LENGTH is present and long\n",
    "    raw_meta = raw_meta.withColumn(\n",
    "        \"chunk_size\",\n",
    "        F.coalesce(F.col(\"BLOB_LENGTH\").cast(\"long\"), F.lit(0))\n",
    "    )\n",
    "    \n",
    "    # Step 3: Deduplicate on metadata only (most recent version per EVENT_ID, BLOB_SEQ_NUM)\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Deduplicating metadata...\")\n",
    "    w_temporal = Window.partitionBy(\"EVENT_ID\", \"BLOB_SEQ_NUM\").orderBy(\n",
    "        F.col(\"VALID_UNTIL_DT_TM\").desc(),\n",
    "        F.col(\"UPDT_DT_TM\").desc(),\n",
    "        F.col(\"LAST_UTC_TS\").desc()\n",
    "    )\n",
    "    \n",
    "    raw_meta_deduped = (raw_meta\n",
    "                    .withColumn(\"version_rank\", F.row_number().over(w_temporal))\n",
    "                    .filter(F.col(\"version_rank\") == 1)\n",
    "                    .drop(\"version_rank\")\n",
    "                    .cache())\n",
    "    _ = raw_meta_deduped.count()\n",
    "    \n",
    "    # Step 4: Per-event sizes and early filtering\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Calculating per-event compressed sizes (from BLOB_LENGTH)...\")\n",
    "    event_sizes = (raw_meta_deduped\n",
    "               .groupBy(\"EVENT_ID\")\n",
    "               .agg(\n",
    "                   F.sum(\"chunk_size\").alias(\"total_compressed_size\"),\n",
    "                   F.count(\"*\").alias(\"chunk_count\"),\n",
    "                   F.max(\"ADC_UPDT\").alias(\"event_adc_updt\")\n",
    "               )\n",
    "               .cache())\n",
    "    _ = event_sizes.count()\n",
    "    \n",
    "    # Track oversized events (we'll skip them but note them)\n",
    "    oversized_events_df = event_sizes.filter(F.col(\"total_compressed_size\") > MAX_BLOB_SIZE).select(\"EVENT_ID\")\n",
    "    oversized_count = oversized_events_df.count()\n",
    "    if oversized_count > 0:\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping {oversized_count} oversized events\")\n",
    "        oversized_events_df.unpersist()\n",
    "    \n",
    "    # Keep only within-size candidates\n",
    "    within_size = event_sizes.filter(F.col(\"total_compressed_size\") <= MAX_BLOB_SIZE)\n",
    "    \n",
    "    # Step 5: Build byte-aware batches from within-size candidates\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Assigning byte-aware batches...\")\n",
    "    cumulative_window = Window.orderBy(\"event_adc_updt\", \"EVENT_ID\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    \n",
    "    batched_events = (within_size\n",
    "                  .withColumn(\"cumulative_size\", F.sum(\"total_compressed_size\").over(cumulative_window))\n",
    "                  .withColumn(\"cumulative_count\", F.row_number().over(Window.orderBy(\"event_adc_updt\", \"EVENT_ID\")))\n",
    "                  .withColumn(\"size_batch\", F.floor(F.col(\"cumulative_size\") / BATCH_SIZE_BYTES).cast(\"int\"))\n",
    "                  .withColumn(\"count_batch\", F.floor((F.col(\"cumulative_count\") - 1) / MAX_BATCH_SIZE).cast(\"int\"))\n",
    "                  .withColumn(\"batch_id\", F.greatest(\"size_batch\", \"count_batch\"))\n",
    "                  .cache())\n",
    "    _ = batched_events.count()\n",
    "    \n",
    "    # We only process the first batch_id this iteration\n",
    "    first_batch_id = batched_events.agg(F.min(\"batch_id\").alias(\"min_batch\")).collect()[0][\"min_batch\"]\n",
    "    if first_batch_id is None:\n",
    "        print(\"No within-size events to process in this iteration.\")\n",
    "        return True\n",
    "    \n",
    "    last_batch_id = first_batch_id + MAX_PARALLEL_BATCHES - 1\n",
    "    chosen_events = (\n",
    "        batched_events\n",
    "        .filter((F.col(\"batch_id\") >= first_batch_id) & (F.col(\"batch_id\") <= last_batch_id))\n",
    "        .select(\"EVENT_ID\")\n",
    "        .cache()\n",
    "    )\n",
    "    chosen_count = chosen_events.count()\n",
    "    print(f\"Selected {chosen_count:,} events across batch_ids [{first_batch_id}, {last_batch_id}]\")\n",
    "    \n",
    "    # Step 6: Filter source by chosen EVENT_IDs, then deduplicate with the same window\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Fetching chosen rows (with BLOB_CONTENTS) and deduplicating...\")\n",
    "    \n",
    "    META_AND_BLOB_COLS = [\n",
    "        \"EVENT_ID\", \"BLOB_SEQ_NUM\",\n",
    "        \"VALID_UNTIL_DT_TM\", \"VALID_FROM_DT_TM\",\n",
    "        \"UPDT_DT_TM\", \"UPDT_ID\", \"UPDT_TASK\", \"UPDT_CNT\", \"UPDT_APPLCTX\",\n",
    "        \"LAST_UTC_TS\", \"ADC_UPDT\", \"COMPRESSION_CD\", \"BLOB_CONTENTS\"\n",
    "    ]\n",
    "    \n",
    "    # Get all rows for chosen EVENT_IDs\n",
    "    source_filtered = (\n",
    "        spark.table(SOURCE_TABLE)\n",
    "        .join(F.broadcast(chosen_events), on=\"EVENT_ID\", how=\"inner\")\n",
    "        .select(*META_AND_BLOB_COLS)\n",
    "    )\n",
    "    \n",
    "    # Apply the same deduplication window\n",
    "    w_temporal = (\n",
    "        Window.partitionBy(\"EVENT_ID\", \"BLOB_SEQ_NUM\")\n",
    "        .orderBy(F.col(\"VALID_UNTIL_DT_TM\").desc(),\n",
    "                 F.col(\"UPDT_DT_TM\").desc(),\n",
    "                 F.col(\"LAST_UTC_TS\").desc())\n",
    "    )\n",
    "    \n",
    "    source_deduped = (\n",
    "        source_filtered\n",
    "        .withColumn(\"version_rank\", F.row_number().over(w_temporal))\n",
    "        .filter(F.col(\"version_rank\") == 1)\n",
    "        .drop(\"version_rank\")\n",
    "        .cache()\n",
    "    )\n",
    "    \n",
    "    deduped_count = source_deduped.count()\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Deduped source rows: {deduped_count:,}\")\n",
    "    \n",
    "    if deduped_count == 0:\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] WARNING: No rows after deduplication. Skipping this batch.\")\n",
    "        # Clean up and return True to continue with next batch\n",
    "        for df in [source_deduped, chosen_events, batched_events, event_sizes, raw_meta_deduped, raw_meta, candidates]:\n",
    "            try:\n",
    "                df.unpersist()\n",
    "            except:\n",
    "                pass\n",
    "        return True\n",
    "    \n",
    "    final_data = (\n",
    "        source_deduped\n",
    "        .orderBy(\"EVENT_ID\", F.col(\"BLOB_SEQ_NUM\").asc_nulls_last())\n",
    "    )\n",
    "    \n",
    "    # Step 7: Parallel processing via Pandas UDF\n",
    "    default_shuffle_partitions = int(spark.conf.get(\"spark.sql.shuffle.partitions\", \"200\"))\n",
    "    optimal_partitions = max(200, default_shuffle_partitions * 2, chosen_count // 10 + 1)\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Repartitioning to {optimal_partitions} partitions...\")\n",
    "    final_data = final_data.repartition(optimal_partitions, \"EVENT_ID\").persist()\n",
    "    final_count = final_data.count()\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Final dataset ready: {final_count} rows for processing\")\n",
    "    \n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting processing...\")\n",
    "    processing_start = time.time()\n",
    "    \n",
    "    processed_df = (final_data\n",
    "        .groupBy(\"EVENT_ID\")\n",
    "        .apply(process_blob_batch))\n",
    "    \n",
    "    final_columns = [\n",
    "        \"EVENT_ID\", \"VALID_UNTIL_DT_TM\", \"VALID_FROM_DT_TM\", \"UPDT_DT_TM\",\n",
    "        \"UPDT_ID\", \"UPDT_TASK\", \"UPDT_CNT\", \"UPDT_APPLCTX\",\n",
    "        \"LAST_UTC_TS\", \"ADC_UPDT\", \"BLOB_BINARY\", \"CONTENT_TYPE\",\n",
    "        \"ENCODING\", \"BLOB_TEXT\", \"BINARY_SIZE\", \"TEXT_LENGTH\",\n",
    "        \"STATUS\", \"anon_text\"\n",
    "    ]\n",
    "    processed_df = processed_df.select(*final_columns).cache()\n",
    "    processed_count = processed_df.count()\n",
    "    processing_time = time.time() - processing_start\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Processed {processed_count} events (took {processing_time:.2f}s)\")\n",
    "    \n",
    "    # Check if we have any results to write\n",
    "    if processed_count == 0:\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] No processed rows; skipping write.\")\n",
    "        # Clean up\n",
    "        for df in [processed_df, final_data, source_deduped, chosen_events, batched_events, event_sizes, raw_meta_deduped, raw_meta, candidates]:\n",
    "            try:\n",
    "                df.unpersist()\n",
    "            except:\n",
    "                pass\n",
    "        return True  # Return True to continue with next batch\n",
    "    \n",
    "    # Step 8: Stats and write\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Gathering statistics...\")\n",
    "    stats_start = time.time()\n",
    "    stats_df = processed_df.agg(\n",
    "        F.count(\"*\").alias(\"total_processed\"),\n",
    "        F.sum(F.when(F.col(\"STATUS\") == \"Decoded\", 1).otherwise(0)).alias(\"successful\"),\n",
    "        F.sum(F.when(F.col(\"STATUS\") != \"Decoded\", 1).otherwise(0)).alias(\"failed\"),\n",
    "        F.sum(\"BINARY_SIZE\").alias(\"total_decompressed_bytes\")\n",
    "    ).collect()[0]\n",
    "    stats_time = time.time() - stats_start\n",
    "    \n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Writing results to {TARGET_TABLE}...\")\n",
    "    write_start = time.time()\n",
    "    write_partitions = min(max(50, chosen_count // 100 + 1), 400)\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Using {write_partitions} partitions for write...\")\n",
    "    \n",
    "    write_success = False\n",
    "    try:\n",
    "        (processed_df\n",
    "            .repartition(write_partitions)\n",
    "            .write\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"false\")\n",
    "            .insertInto(TARGET_TABLE))\n",
    "        write_success = True\n",
    "        write_time = time.time() - write_start\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Write completed (took {write_time:.2f}s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Write failed: {e}\")\n",
    "        write_time = 0\n",
    "    \n",
    "    total_time = time.time() - script_start\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BATCH PROCESSING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total time: {total_time:.2f}s\")\n",
    "    print(f\"Processed: {stats_df['total_processed']}, successful: {stats_df['successful']}, failed: {stats_df['failed']}\")\n",
    "    if stats_df['total_decompressed_bytes']:\n",
    "        print(f\"Total decompressed size: {stats_df['total_decompressed_bytes']/(1024**3):.2f} GB\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Cleanup\n",
    "    for df in [processed_df, final_data, source_deduped, chosen_events, batched_events, event_sizes, raw_meta_deduped, raw_meta, candidates]:\n",
    "        try:\n",
    "            df.unpersist()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return write_success\n",
    "\n",
    "\n",
    "def process_all_pending_blobs(batch_size=100000, max_iterations=None):\n",
    "    \"\"\"\n",
    "    Process all new blobs incrementally based on ADC_UPDT timestamp.\n",
    "    Processes events from source table where ADC_UPDT > max(ADC_UPDT) from target table.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of events to process in each batch\n",
    "        max_iterations: Maximum number of iterations (None for unlimited)\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    total_start = time.time()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"STARTING INCREMENTAL BLOB PROCESSING\")\n",
    "    print(f\"Batch size: {batch_size:,} events per iteration\")\n",
    "    print(f\"Max iterations: {max_iterations if max_iterations else 'Unlimited'}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get initial cutoff date\n",
    "    initial_cutoff = get_max_adc_updt(TARGET_TABLE, default_dt=datetime(1980, 1, 1))\n",
    "    print(f\"\\nInitial cutoff date: {initial_cutoff}\")\n",
    "    \n",
    "    # Check initial count of new events\n",
    "    initial_new = (spark.table(SOURCE_TABLE)\n",
    "                   .filter(F.col(\"ADC_UPDT\") > initial_cutoff)\n",
    "                   .select(\"EVENT_ID\")\n",
    "                   .distinct()\n",
    "                   .count())\n",
    "    print(f\"Initial new events to process: {initial_new:,}\")\n",
    "    \n",
    "    if initial_new == 0:\n",
    "        print(\"No new events to process!\")\n",
    "        return\n",
    "    \n",
    "    successful_iterations = 0\n",
    "    failed_iterations = 0\n",
    "    total_events_processed = 0\n",
    "    consecutive_failures = 0\n",
    "    max_consecutive_failures = 5  # Stop after 5 consecutive failures\n",
    "    last_cutoff = initial_cutoff\n",
    "    \n",
    "    while True:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Check if we've hit max iterations\n",
    "        if max_iterations and iteration > max_iterations:\n",
    "            print(f\"\\nReached maximum iterations ({max_iterations}). Stopping.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ITERATION {iteration}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Get current cutoff date\n",
    "        current_cutoff = get_max_adc_updt(TARGET_TABLE, default_dt=datetime(1980, 1, 1))\n",
    "        print(f\"Current cutoff date: {current_cutoff}\")\n",
    "        \n",
    "        # Check remaining count\n",
    "        remaining_count = (spark.table(SOURCE_TABLE)\n",
    "                          .filter(F.col(\"ADC_UPDT\") > current_cutoff)\n",
    "                          .select(\"EVENT_ID\")\n",
    "                          .distinct()\n",
    "                          .count())\n",
    "        print(f\"Remaining new events: {remaining_count:,}\")\n",
    "        \n",
    "        if remaining_count == 0:\n",
    "            print(\"All new events have been processed!\")\n",
    "            break\n",
    "        \n",
    "        # Check if we're making progress\n",
    "        if current_cutoff == last_cutoff and iteration > 1:\n",
    "            print(\"No progress made in last iteration (cutoff date unchanged)\")\n",
    "            consecutive_failures += 1\n",
    "            if consecutive_failures >= max_consecutive_failures:\n",
    "                print(f\"Too many iterations without progress. Stopping.\")\n",
    "                break\n",
    "        else:\n",
    "            last_cutoff = current_cutoff\n",
    "        \n",
    "        # Run processing for this batch\n",
    "        try:\n",
    "            success = run_parallel_processing(batch_limit=batch_size, cutoff_date=current_cutoff)\n",
    "            \n",
    "            if success is False:  # No more new events\n",
    "                print(\"No more new events to process\")\n",
    "                break\n",
    "            elif success:\n",
    "                successful_iterations += 1\n",
    "                consecutive_failures = 0  # Reset consecutive failure counter\n",
    "                \n",
    "                # Get the new cutoff after processing\n",
    "                new_cutoff = get_max_adc_updt(TARGET_TABLE, default_dt=datetime(1980, 1, 1))\n",
    "                new_remaining = (spark.table(SOURCE_TABLE)\n",
    "                                .filter(F.col(\"ADC_UPDT\") > new_cutoff)\n",
    "                                .select(\"EVENT_ID\")\n",
    "                                .distinct()\n",
    "                                .count())\n",
    "                events_processed = remaining_count - new_remaining\n",
    "                total_events_processed += events_processed\n",
    "                \n",
    "                print(f\"\\nIteration {iteration} completed successfully\")\n",
    "                print(f\"Events processed in this iteration: ~{events_processed:,}\")\n",
    "                print(f\"Cutoff date advanced from {current_cutoff} to {new_cutoff}\")\n",
    "            else:\n",
    "                failed_iterations += 1\n",
    "                consecutive_failures += 1\n",
    "                print(f\"\\nIteration {iteration} failed\")\n",
    "                \n",
    "                if consecutive_failures >= max_consecutive_failures:\n",
    "                    print(f\"\\nToo many consecutive failures ({consecutive_failures}). Stopping.\")\n",
    "                    break\n",
    "        \n",
    "        except Exception as e:\n",
    "            failed_iterations += 1\n",
    "            consecutive_failures += 1\n",
    "            print(f\"\\nIteration {iteration} encountered an error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            if consecutive_failures >= max_consecutive_failures:\n",
    "                print(f\"\\nToo many consecutive failures ({consecutive_failures}). Stopping.\")\n",
    "                break\n",
    "            \n",
    "            # Decide whether to continue or stop on error\n",
    "            if iteration == 1:\n",
    "                # If first iteration fails, likely a configuration issue\n",
    "                print(\"\\nFirst iteration failed. Stopping.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"\\nContinuing despite error...\")\n",
    "                # Add longer delay after error\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "        \n",
    "        # Add a small delay between iterations to avoid overwhelming the system\n",
    "        if remaining_count > batch_size:  # More work to do\n",
    "            print(f\"\\nWaiting 5 seconds before next iteration...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - total_start\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INCREMENTAL PROCESSING SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total iterations: {iteration}\")\n",
    "    print(f\"Successful iterations: {successful_iterations}\")\n",
    "    print(f\"Failed iterations: {failed_iterations}\")\n",
    "    print(f\"Total events processed: ~{total_events_processed:,}\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"Final cutoff date: {get_max_adc_updt(TARGET_TABLE, default_dt=datetime(1980, 1, 1))}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4fef06f-6e42-4d21-9e0c-001f52940838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execute the full processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all pending blobs in batches of 100,000\n",
    "    process_all_pending_blobs(batch_size=50000, max_iterations=5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5025579668527077,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Blob Decomp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
