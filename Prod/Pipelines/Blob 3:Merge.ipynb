{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64dd3e39-7d2c-46f5-850d-472f5328c47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook 3: Merge Batch Tables and Cleanup\n",
    "# Works with the shard-based processing from the optimized pipeline\n",
    "# Ensures ALL events end up in target table\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "# Configuration\n",
    "TARGET_TABLE = \"4_prod.bronze.mill_blob_text\"\n",
    "METRICS_TABLE = \"4_prod.logs.mill_blob_metrics\"\n",
    "STAGING_DB = \"4_prod.tmp\"\n",
    "METADATA_TABLE = f\"{STAGING_DB}.pipeline_metadata\"\n",
    "\n",
    "# Get the latest completed run from metadata\n",
    "latest_run = spark.sql(f\"\"\"\n",
    "    SELECT run_id, worklist_table, batch_tables, processed_events\n",
    "    FROM {METADATA_TABLE}\n",
    "    WHERE status = 'processing_complete'\n",
    "    ORDER BY created_ts DESC\n",
    "    LIMIT 1\n",
    "\"\"\").collect()\n",
    "\n",
    "if not latest_run:\n",
    "    raise Exception(\"No completed processing found! Run the combined processor notebook first.\")\n",
    "\n",
    "RUN_ID = latest_run[0]['run_id']\n",
    "WORKLIST_TABLE = latest_run[0]['worklist_table']\n",
    "batch_tables_str = latest_run[0]['batch_tables'] or \"\"\n",
    "total_events = latest_run[0]['processed_events'] or 0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"MERGE AND CLEANUP - RUN {RUN_ID}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Worklist: {WORKLIST_TABLE}\")\n",
    "\n",
    "# Parse batch tables from metadata\n",
    "if batch_tables_str:\n",
    "    batch_tables = [t.strip() for t in batch_tables_str.split(\",\") if t.strip()]\n",
    "else:\n",
    "    # If not in metadata, try to find them by pattern\n",
    "    print(\"Batch tables not in metadata, searching by pattern...\")\n",
    "    all_tables = spark.sql(f\"SHOW TABLES IN {STAGING_DB}\").collect()\n",
    "    pattern = f\"batch_{RUN_ID}_shard_\"\n",
    "    batch_tables = [f\"{STAGING_DB}.{row.tableName}\" \n",
    "                   for row in all_tables \n",
    "                   if row.tableName.startswith(pattern)]\n",
    "\n",
    "if not batch_tables:\n",
    "    print(\"WARNING: No batch tables found!\")\n",
    "else:\n",
    "    print(f\"Found {len(batch_tables)} batch tables:\")\n",
    "    for table in batch_tables:\n",
    "        try:\n",
    "            count = spark.table(table).count()\n",
    "            print(f\"  {table}: {count:,} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {table}: ERROR - {e}\")\n",
    "\n",
    "# Step 1: Get oversized events that were already written\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Checking for oversized events...\")\n",
    "oversized_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as cnt\n",
    "    FROM {WORKLIST_TABLE}\n",
    "    WHERE status = 'oversized'\n",
    "\"\"\").collect()[0]['cnt']\n",
    "\n",
    "if oversized_count > 0:\n",
    "    print(f\"  {oversized_count:,} oversized events were already written to target in Notebook 1\")\n",
    "\n",
    "# Step 2: Find events that were in batch tables\n",
    "if batch_tables:\n",
    "    # Build union query to get all processed EVENT_IDs\n",
    "    processed_events_query = \" UNION ALL \".join([f\"SELECT DISTINCT EVENT_ID FROM {t}\" for t in batch_tables])\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW processed_events AS\n",
    "        {processed_events_query}\n",
    "    \"\"\")\n",
    "    \n",
    "    processed_count = spark.sql(\"SELECT COUNT(DISTINCT EVENT_ID) as cnt FROM processed_events\").collect()[0]['cnt']\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Found {processed_count:,} events in batch tables\")\n",
    "else:\n",
    "    processed_count = 0\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] No events in batch tables\")\n",
    "\n",
    "# Step 3: Find any events that didn't get processed\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Checking for unprocessed events...\")\n",
    "\n",
    "if batch_tables:\n",
    "    unprocessed = spark.sql(f\"\"\"\n",
    "        SELECT w.EVENT_ID, w.ADC_UPDT, w.chunks_data, w.status\n",
    "        FROM {WORKLIST_TABLE} w\n",
    "        WHERE w.status NOT IN ('oversized', 'completed')\n",
    "        AND w.EVENT_ID NOT IN (SELECT EVENT_ID FROM processed_events)\n",
    "    \"\"\")\n",
    "else:\n",
    "    unprocessed = spark.sql(f\"\"\"\n",
    "        SELECT w.EVENT_ID, w.ADC_UPDT, w.chunks_data, w.status\n",
    "        FROM {WORKLIST_TABLE} w\n",
    "        WHERE w.status NOT IN ('oversized', 'completed')\n",
    "    \"\"\")\n",
    "\n",
    "unprocessed_count = unprocessed.count()\n",
    "\n",
    "if unprocessed_count > 0:\n",
    "    print(f\"  Found {unprocessed_count:,} unprocessed events - writing with failed status...\")\n",
    "    \n",
    "    # Create failure records for unprocessed events\n",
    "    failed_records = (unprocessed\n",
    "        .select(\n",
    "            \"EVENT_ID\",\n",
    "            F.element_at(F.col(\"chunks_data\"), 1).alias(\"first_chunk\")\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"EVENT_ID\").cast(\"long\"),\n",
    "            F.col(\"first_chunk.VALID_UNTIL_DT_TM\").alias(\"VALID_UNTIL_DT_TM\"),\n",
    "            F.col(\"first_chunk.VALID_FROM_DT_TM\").alias(\"VALID_FROM_DT_TM\"),\n",
    "            F.col(\"first_chunk.UPDT_DT_TM\").alias(\"UPDT_DT_TM\"),\n",
    "            F.col(\"first_chunk.UPDT_ID\").cast(\"long\").alias(\"UPDT_ID\"),\n",
    "            F.col(\"first_chunk.UPDT_TASK\").cast(\"long\").alias(\"UPDT_TASK\"),\n",
    "            F.col(\"first_chunk.UPDT_CNT\").cast(\"long\").alias(\"UPDT_CNT\"),\n",
    "            F.col(\"first_chunk.UPDT_APPLCTX\").cast(\"long\").alias(\"UPDT_APPLCTX\"),\n",
    "            F.col(\"first_chunk.LAST_UTC_TS\").alias(\"LAST_UTC_TS\"),\n",
    "            F.col(\"first_chunk.ADC_UPDT\").alias(\"ADC_UPDT\"),\n",
    "            F.lit(None).cast(\"binary\").alias(\"BLOB_BINARY\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"CONTENT_TYPE\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"ENCODING\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"BLOB_TEXT\"),\n",
    "            F.lit(None).cast(\"long\").alias(\"BINARY_SIZE\"),\n",
    "            F.lit(None).cast(\"long\").alias(\"TEXT_LENGTH\"),\n",
    "            F.lit(\"Pipeline failed - not processed\").alias(\"STATUS\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"anon_text\")\n",
    "        ))\n",
    "    \n",
    "    # Write failed records directly to target\n",
    "    failed_records.write.mode(\"append\").insertInto(TARGET_TABLE)\n",
    "    print(f\"  Wrote {unprocessed_count:,} failed records to target\")\n",
    "else:\n",
    "    print(\"  All events were processed successfully\")\n",
    "\n",
    "# Step 4: Extract and save metrics (if batch tables exist and metrics table exists)\n",
    "if batch_tables:\n",
    "    try:\n",
    "        # Check if metrics table exists\n",
    "        spark.sql(f\"DESCRIBE TABLE {METRICS_TABLE}\")\n",
    "        \n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Extracting metrics from {len(batch_tables)} batch tables...\")\n",
    "        \n",
    "        metrics_dfs = []\n",
    "        for table in batch_tables:\n",
    "            try:\n",
    "                # Check if metrics column exists\n",
    "                columns = [col.name for col in spark.table(table).schema]\n",
    "                if \"metrics\" in columns:\n",
    "                    metrics_df = (spark.table(table)\n",
    "                                  .filter(F.col(\"metrics\").isNotNull())\n",
    "                                  .select(\n",
    "                                      \"EVENT_ID\",\n",
    "                                      \"ADC_UPDT\",\n",
    "                                      \"STATUS\",\n",
    "                                      F.col(\"metrics\"),\n",
    "                                      F.lit(RUN_ID).alias(\"RUN_ID\"),\n",
    "                                      F.current_timestamp().alias(\"process_ts\")\n",
    "                                  ))\n",
    "                    metrics_dfs.append(metrics_df)\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not extract metrics from {table}: {e}\")\n",
    "        \n",
    "        if metrics_dfs:\n",
    "            combined_metrics = reduce(lambda a, b: a.union(b), metrics_dfs)\n",
    "            combined_metrics = combined_metrics.select(\n",
    "                \"EVENT_ID\", \"RUN_ID\", \"ADC_UPDT\", \"STATUS\", \"metrics\", \"process_ts\"\n",
    "            )\n",
    "            combined_metrics.write.mode(\"append\").insertInto(METRICS_TABLE)\n",
    "            metrics_count = combined_metrics.count()\n",
    "            print(f\"  Saved {metrics_count:,} metrics records\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Metrics table not found or error: {e}\")\n",
    "\n",
    "# Step 5: Merge batch tables to target\n",
    "if batch_tables:\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Merging {len(batch_tables)} batch tables to {TARGET_TABLE}...\")\n",
    "    merge_start = time.time()\n",
    "    \n",
    "    # Define output columns based on target table schema\n",
    "    output_columns = [\n",
    "        \"EVENT_ID\", \"VALID_UNTIL_DT_TM\", \"VALID_FROM_DT_TM\", \"UPDT_DT_TM\",\n",
    "        \"UPDT_ID\", \"UPDT_TASK\", \"UPDT_CNT\", \"UPDT_APPLCTX\",\n",
    "        \"LAST_UTC_TS\", \"ADC_UPDT\", \"BLOB_BINARY\", \"CONTENT_TYPE\",\n",
    "        \"ENCODING\", \"BLOB_TEXT\", \"BINARY_SIZE\", \"TEXT_LENGTH\",\n",
    "        \"STATUS\", \"anon_text\"\n",
    "    ]\n",
    "    \n",
    "    # Collect valid batch tables and their dataframes\n",
    "    valid_dfs = []\n",
    "    for table in batch_tables:\n",
    "        try:\n",
    "            df = spark.table(table)\n",
    "            # Ensure EVENT_ID is long type to match target\n",
    "            df = df.withColumn(\"EVENT_ID\", F.col(\"EVENT_ID\").cast(\"long\"))\n",
    "            # Select only columns that exist in both source and target\n",
    "            existing_columns = df.columns\n",
    "            select_columns = [col for col in output_columns if col in existing_columns]\n",
    "            \n",
    "            # Add missing columns as nulls\n",
    "            for col in output_columns:\n",
    "                if col not in existing_columns:\n",
    "                    if col == \"EVENT_ID\":\n",
    "                        df = df.withColumn(col, F.lit(None).cast(\"long\"))\n",
    "                    elif col in [\"UPDT_ID\", \"UPDT_TASK\", \"UPDT_CNT\", \"UPDT_APPLCTX\", \"BINARY_SIZE\", \"TEXT_LENGTH\"]:\n",
    "                        df = df.withColumn(col, F.lit(None).cast(\"long\"))\n",
    "                    elif col in [\"VALID_UNTIL_DT_TM\", \"VALID_FROM_DT_TM\", \"UPDT_DT_TM\", \"LAST_UTC_TS\", \"ADC_UPDT\"]:\n",
    "                        df = df.withColumn(col, F.lit(None).cast(\"timestamp\"))\n",
    "                    elif col == \"BLOB_BINARY\":\n",
    "                        df = df.withColumn(col, F.lit(None).cast(\"binary\"))\n",
    "                    else:\n",
    "                        df = df.withColumn(col, F.lit(None).cast(\"string\"))\n",
    "            \n",
    "            valid_dfs.append(df.select(*output_columns))\n",
    "            print(f\"  Added {table} to merge\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Skipping {table} due to error: {e}\")\n",
    "    \n",
    "    if valid_dfs:\n",
    "        if len(valid_dfs) == 1:\n",
    "            final_df = valid_dfs[0]\n",
    "        else:\n",
    "            final_df = reduce(lambda a, b: a.union(b), valid_dfs)\n",
    "        \n",
    "        # Repartition for optimal write performance\n",
    "        record_count = final_df.count()\n",
    "        optimal_partitions = max(1, min(200, record_count // 5000))\n",
    "        \n",
    "        (final_df\n",
    "         .repartition(optimal_partitions)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .option(\"mergeSchema\", \"false\")\n",
    "         .option(\"optimizeWrite\", \"true\")\n",
    "         .insertInto(TARGET_TABLE))\n",
    "        \n",
    "        merge_time = time.time() - merge_start\n",
    "        print(f\"  Merged {record_count:,} records in {merge_time:.1f}s\")\n",
    "else:\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] No batch tables to merge\")\n",
    "\n",
    "# Step 6: Final validation - ensure ALL events from worklist are in target\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Final validation...\")\n",
    "\n",
    "# Count worklist events (excluding duplicates)\n",
    "worklist_total = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT EVENT_ID) as cnt\n",
    "    FROM {WORKLIST_TABLE}\n",
    "\"\"\").collect()[0]['cnt']\n",
    "\n",
    "# Count how many of those events are now in target\n",
    "target_total = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT t.EVENT_ID) as cnt\n",
    "    FROM {TARGET_TABLE} t\n",
    "    INNER JOIN {WORKLIST_TABLE} w ON t.EVENT_ID = w.EVENT_ID\n",
    "\"\"\").collect()[0]['cnt']\n",
    "\n",
    "print(f\"  Worklist events: {worklist_total:,}\")\n",
    "print(f\"  Events now in target: {target_total:,}\")\n",
    "\n",
    "missing_count = worklist_total - target_total\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"\\nWARNING: {missing_count} events still missing from target table!\")\n",
    "    \n",
    "    # Force write any remaining missing events\n",
    "    print(\"Force-writing missing events with error status...\")\n",
    "    \n",
    "    missing_events = spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT w.EVENT_ID, w.chunks_data\n",
    "        FROM {WORKLIST_TABLE} w\n",
    "        LEFT ANTI JOIN {TARGET_TABLE} t ON w.EVENT_ID = t.EVENT_ID\n",
    "    \"\"\")\n",
    "    \n",
    "    forced_records = (missing_events\n",
    "        .select(\n",
    "            F.col(\"EVENT_ID\").cast(\"long\"),\n",
    "            F.element_at(F.col(\"chunks_data\"), 1).alias(\"first_chunk\")\n",
    "        )\n",
    "        .select(\n",
    "            \"EVENT_ID\",\n",
    "            F.col(\"first_chunk.VALID_UNTIL_DT_TM\").alias(\"VALID_UNTIL_DT_TM\"),\n",
    "            F.col(\"first_chunk.VALID_FROM_DT_TM\").alias(\"VALID_FROM_DT_TM\"),\n",
    "            F.col(\"first_chunk.UPDT_DT_TM\").alias(\"UPDT_DT_TM\"),\n",
    "            F.col(\"first_chunk.UPDT_ID\").cast(\"long\").alias(\"UPDT_ID\"),\n",
    "            F.col(\"first_chunk.UPDT_TASK\").cast(\"long\").alias(\"UPDT_TASK\"),\n",
    "            F.col(\"first_chunk.UPDT_CNT\").cast(\"long\").alias(\"UPDT_CNT\"),\n",
    "            F.col(\"first_chunk.UPDT_APPLCTX\").cast(\"long\").alias(\"UPDT_APPLCTX\"),\n",
    "            F.col(\"first_chunk.LAST_UTC_TS\").alias(\"LAST_UTC_TS\"),\n",
    "            F.col(\"first_chunk.ADC_UPDT\").alias(\"ADC_UPDT\"),\n",
    "            F.lit(None).cast(\"binary\").alias(\"BLOB_BINARY\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"CONTENT_TYPE\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"ENCODING\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"BLOB_TEXT\"),\n",
    "            F.lit(None).cast(\"long\").alias(\"BINARY_SIZE\"),\n",
    "            F.lit(None).cast(\"long\").alias(\"TEXT_LENGTH\"),\n",
    "            F.lit(\"Force-written: processing incomplete\").alias(\"STATUS\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"anon_text\")\n",
    "        ))\n",
    "    \n",
    "    forced_records.write.mode(\"append\").insertInto(TARGET_TABLE)\n",
    "    print(f\"  Force-wrote {missing_count} missing events\")\n",
    "    \n",
    "    # Re-validate\n",
    "    target_total = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(DISTINCT t.EVENT_ID) as cnt\n",
    "        FROM {TARGET_TABLE} t\n",
    "        INNER JOIN {WORKLIST_TABLE} w ON t.EVENT_ID = w.EVENT_ID\n",
    "    \"\"\").collect()[0]['cnt']\n",
    "\n",
    "# Step 7: Cleanup staging tables (optional - can be disabled for debugging)\n",
    "CLEANUP_ENABLED = True  # Set to False to keep staging tables for debugging\n",
    "\n",
    "if CLEANUP_ENABLED:\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Cleaning up staging tables...\")\n",
    "    \n",
    "    # Drop worklist table\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {WORKLIST_TABLE}\")\n",
    "        print(f\"  Dropped {WORKLIST_TABLE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not drop {WORKLIST_TABLE}: {e}\")\n",
    "    \n",
    "    # Drop batch tables\n",
    "    for table in batch_tables:\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "            print(f\"  Dropped {table}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not drop {table}: {e}\")\n",
    "else:\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Cleanup disabled - staging tables retained\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Total worklist events: {worklist_total:,}\")\n",
    "print(f\"Successfully added to target: {target_total:,}\")\n",
    "\n",
    "if worklist_total == target_total:\n",
    "    print(\"✓ ALL events accounted for in target table\")\n",
    "else:\n",
    "    print(f\"⚠ WARNING: {worklist_total - target_total} events may be missing\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Update metadata to mark run as complete\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {METADATA_TABLE}\n",
    "    SET status = 'complete',\n",
    "        merged_ts = current_timestamp()\n",
    "    WHERE run_id = '{RUN_ID}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Verify data quality in target table\")\n",
    "print(\"2. Run OPTIMIZE on target table if needed:\")\n",
    "print(f\"   OPTIMIZE {TARGET_TABLE} WHERE EVENT_ID IN (SELECT EVENT_ID FROM {WORKLIST_TABLE})\")\n",
    "print(\"3. Monitor for any failed records (STATUS != 'Decoded')\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Blob 3:Merge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
