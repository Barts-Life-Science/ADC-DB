{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c621a1c-5935-4f07-8f8e-3af1cc5bde45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY FUNCTION to_col_json_str(\n",
    "  col_name STRING, data_type STRING, sensitive STRING, description STRING\n",
    ")\n",
    "RETURNS STRING\n",
    "/*\n",
    "RETURN CONCAT(\n",
    "  '{', '''name'':''', col_name, ''',',\n",
    "  '''values'':null,',\n",
    "  '''dataType'':''', data_type, ''',',\n",
    "  '''sensitive'':', sensitive, ',',\n",
    "  '''description'':''', description, '''','}')*/\n",
    "\n",
    "RETURN TO_JSON(named_struct(\n",
    "  'name', col_name,\n",
    "  'values', null,\n",
    "  'dataType', data_type,\n",
    "  'sensitive', sensitive,\n",
    "  'description', description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e73d697-54b5-484f-92ad-d5c2dba2dbf6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762428499032}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "CATALOG = \"4_prod\"\n",
    "SCHEMA = \"pacs\"\n",
    "DATASET_ID = \"1491\"\n",
    "\n",
    "# Load column_tags table\n",
    "col_tags = spark.table(\"system.information_schema.column_tags\")\n",
    "\n",
    "# Prepare ig_risk and ig_severity DataFrames\n",
    "ig_risk = col_tags.filter(F.lower(F.col(\"tag_name\")) == \"ig_risk\")\n",
    "ig_severity = col_tags.filter(F.lower(F.col(\"tag_name\")) == \"ig_severity\")\n",
    "\n",
    "# Join and compute sensitive\n",
    "ig_sensitive = (\n",
    "    ig_risk.alias(\"r\")\n",
    "    .join(\n",
    "        ig_severity.alias(\"s\"),\n",
    "        [\n",
    "            F.col(\"r.catalog_name\") == F.col(\"s.catalog_name\"),\n",
    "            F.col(\"r.schema_name\") == F.col(\"s.schema_name\"),\n",
    "            F.col(\"r.table_name\") == F.col(\"s.table_name\"),\n",
    "            F.col(\"r.column_name\") == F.col(\"s.column_name\"),\n",
    "        ],\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"r.catalog_name\"),\n",
    "        F.col(\"r.schema_name\"),\n",
    "        F.col(\"r.table_name\"),\n",
    "        F.col(\"r.column_name\"),\n",
    "        F.col(\"r.tag_value\").alias(\"ig_risk\"),\n",
    "        F.col(\"s.tag_value\").alias(\"ig_severity\"),\n",
    "        F.when(F.upper(F.col(\"r.column_name\")) == \"ADC_UPDT\", F.lit(False))\n",
    "         .when(F.col(\"r.tag_value\").isNull() | F.col(\"s.tag_value\").isNull(), F.lit(None))\n",
    "         .when(F.col(\"r.tag_value\").cast(\"int\") >= 3, F.lit(True))\n",
    "         .when(F.col(\"s.tag_value\").cast(\"int\") >= 2, F.lit(True))\n",
    "         .otherwise(F.lit(False)).alias(\"sensitive\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load columns table\n",
    "columns = spark.table(\"system.information_schema.columns\")\n",
    "\n",
    "# Join with ig_sensitive\n",
    "col_df = (\n",
    "    columns.alias(\"c\")\n",
    "    .join(\n",
    "        ig_sensitive.alias(\"s\"),\n",
    "        [\n",
    "            F.col(\"c.table_catalog\") == F.col(\"s.catalog_name\"),\n",
    "            F.col(\"c.table_schema\") == F.col(\"s.schema_name\"),\n",
    "            F.col(\"c.table_name\") == F.col(\"s.table_name\"),\n",
    "            F.col(\"c.column_name\") == F.col(\"s.column_name\"),\n",
    "        ],\n",
    "        \"left\"\n",
    "    )\n",
    "    .filter(\n",
    "        (F.col(\"c.table_catalog\") == CATALOG) &\n",
    "        (F.col(\"c.table_schema\") == SCHEMA)\n",
    "        # Uncomment the next line to filter table_name with 'omop_%'\n",
    "        # & (F.col(\"c.table_name\").like(\"omop_%\"))\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"c.table_catalog\"),\n",
    "        F.col(\"c.table_schema\"),\n",
    "        F.col(\"c.table_name\"),\n",
    "        F.col(\"c.column_name\").alias(\"column_name\"),\n",
    "        F.col(\"c.ordinal_position\"),\n",
    "        F.col(\"c.data_type\").alias(\"dataType\"),\n",
    "        F.col(\"s.ig_risk\"),\n",
    "        F.col(\"s.ig_severity\"),\n",
    "        F.col(\"s.sensitive\"),\n",
    "        F.col(\"c.comment\").alias(\"column_description\")\n",
    "    )\n",
    ")\n",
    "\n",
    "col_df = col_df.withColumn(\n",
    "    \"json_str\",\n",
    "    F.when(\n",
    "        F.col(\"sensitive\").isNull() & F.col(\"column_description\").isNull(),\n",
    "        F.expr(\"to_col_json_str(column_name, dataType, 'null', 'null')\")\n",
    "    ).when(\n",
    "        F.col(\"sensitive\").isNull(),\n",
    "        F.expr(\"to_col_json_str(column_name, dataType, 'null', column_description)\")\n",
    "    ).when(\n",
    "        F.col(\"column_description\").isNull(),\n",
    "        F.expr(\"to_col_json_str(column_name, dataType, sensitive, 'null')\")\n",
    "    ).otherwise(\n",
    "        F.expr(\"to_col_json_str(column_name, dataType, sensitive, column_description)\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Insert into target table\n",
    "display(col_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d76865-6181-464a-bbf2-04d8dcb6b68a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "# Define the schema for the JSON struct\n",
    "json_schema = \"name STRING, values STRING, dataType STRING, sensitive STRING, description STRING\"\n",
    "# Not sure if sensitive is BOOLEAN or STRING\n",
    "\n",
    "# Add the json_struct column to the result DataFrame\n",
    "col_df = col_df.withColumn(\n",
    "    \"json_struct\",\n",
    "    from_json(\"json_str\", json_schema)\n",
    ")\n",
    "display(col_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5204cc6-ebd8-473f-9c99-bb9ad52916ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if any json_str is null\n",
    "col_df.filter(col_df.json_str.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7882eb-b793-4a0e-b574-280f04c6cbbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "tab_df = (\n",
    "    col_df.groupBy(\"table_name\")\n",
    "    .agg(\n",
    "        F.collect_set(\"json_struct\").alias(\"columns\"),\n",
    "    )\n",
    "    .withColumn(\"description\", F.lit(None))\n",
    ")\n",
    "\n",
    "display(tab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d9e11f-eb56-483a-b191-64f3304b563e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load table comments\n",
    "table_comments = (\n",
    "    spark.table(\"system.information_schema.tables\")\n",
    "    .filter(\n",
    "        (F.col(\"table_catalog\") == CATALOG) &\n",
    "        (F.col(\"table_schema\") == SCHEMA)\n",
    "        # .filter(F.col(\"table_name\").like(\"omop_%\"))  # Uncomment if needed\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"table_name\"),\n",
    "        F.col(\"comment\").alias(\"table_comment\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update description in tab_df\n",
    "tab_df = (\n",
    "    tab_df.alias(\"t\")\n",
    "    .join(\n",
    "        table_comments.alias(\"c\"),\n",
    "        F.col(\"t.table_name\") == F.col(\"c.table_name\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"description\",\n",
    "        F.col(\"c.table_comment\")\n",
    "    )\n",
    "    .select(\"t.*\", \"description\")\n",
    ")\n",
    "\n",
    "display(tab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff486c91-e949-493a-9d06-d7e6e4d961dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "api_path = f\"https://api.dev.hdruk.cloud/api/v1/integrations/datasets/{DATASET_ID}\"\n",
    "headers = {\n",
    "    \"x-application-id\": dbutils.secrets.get(scope=\"adc_store\", key=\"hdruk_app_id\"),\n",
    "    \"x-client-id\": dbutils.secrets.get(scope=\"adc_store\", key=\"hdruk_client_id\"),\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "response = requests.get(\n",
    "    f\"https://api.healthdatagateway.org/api/v1/datasets/{DATASET_ID}\",\n",
    "    headers=headers\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8427534845032680,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "update_hdruk_metadata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
