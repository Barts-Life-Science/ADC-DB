{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c621a1c-5935-4f07-8f8e-3af1cc5bde45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY FUNCTION to_col_json_str(\n",
    "  col_name STRING, data_type STRING, sensitive STRING, description STRING\n",
    ")\n",
    "RETURNS STRING\n",
    "/*\n",
    "RETURN CONCAT(\n",
    "  '{', '''name'':''', col_name, ''',',\n",
    "  '''values'':null,',\n",
    "  '''dataType'':''', data_type, ''',',\n",
    "  '''sensitive'':', sensitive, ',',\n",
    "  '''description'':''', description, '''','}')*/\n",
    "\n",
    "RETURN TO_JSON(named_struct(\n",
    "  'name', col_name,\n",
    "  'values', null,\n",
    "  'dataType', data_type,\n",
    "  'sensitive', sensitive,\n",
    "  'description', description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e73d697-54b5-484f-92ad-d5c2dba2dbf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "CATALOG = \"4_prod\"\n",
    "SCHEMA = \"pacs\"\n",
    "\n",
    "# Load column_tags table\n",
    "col_tags = spark.table(\"system.information_schema.column_tags\")\n",
    "\n",
    "# Prepare ig_risk and ig_severity DataFrames\n",
    "ig_risk = col_tags.filter(F.lower(F.col(\"tag_name\")) == \"ig_risk\")\n",
    "ig_severity = col_tags.filter(F.lower(F.col(\"tag_name\")) == \"ig_severity\")\n",
    "\n",
    "# Join and compute sensitive\n",
    "ig_sensitive = (\n",
    "    ig_risk.alias(\"r\")\n",
    "    .join(\n",
    "        ig_severity.alias(\"s\"),\n",
    "        [\n",
    "            F.col(\"r.catalog_name\") == F.col(\"s.catalog_name\"),\n",
    "            F.col(\"r.schema_name\") == F.col(\"s.schema_name\"),\n",
    "            F.col(\"r.table_name\") == F.col(\"s.table_name\"),\n",
    "            F.col(\"r.column_name\") == F.col(\"s.column_name\"),\n",
    "        ],\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"r.catalog_name\"),\n",
    "        F.col(\"r.schema_name\"),\n",
    "        F.col(\"r.table_name\"),\n",
    "        F.col(\"r.column_name\"),\n",
    "        F.col(\"r.tag_value\").alias(\"ig_risk\"),\n",
    "        F.col(\"s.tag_value\").alias(\"ig_severity\"),\n",
    "        F.when(F.upper(F.col(\"r.column_name\")) == \"ADC_UPDT\", F.lit(False))\n",
    "         .when(F.col(\"r.tag_value\").isNull() | F.col(\"s.tag_value\").isNull(), F.lit(None))\n",
    "         .when(F.col(\"r.tag_value\").cast(\"int\") >= 3, F.lit(True))\n",
    "         .when(F.col(\"s.tag_value\").cast(\"int\") >= 2, F.lit(True))\n",
    "         .otherwise(F.lit(False)).alias(\"sensitive\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load columns table\n",
    "columns = spark.table(\"system.information_schema.columns\")\n",
    "\n",
    "# Join with ig_sensitive\n",
    "result = (\n",
    "    columns.alias(\"c\")\n",
    "    .join(\n",
    "        ig_sensitive.alias(\"s\"),\n",
    "        [\n",
    "            F.col(\"c.table_catalog\") == F.col(\"s.catalog_name\"),\n",
    "            F.col(\"c.table_schema\") == F.col(\"s.schema_name\"),\n",
    "            F.col(\"c.table_name\") == F.col(\"s.table_name\"),\n",
    "            F.col(\"c.column_name\") == F.col(\"s.column_name\"),\n",
    "        ],\n",
    "        \"left\"\n",
    "    )\n",
    "    .filter(\n",
    "        (F.col(\"c.table_catalog\") == CATALOG) &\n",
    "        (F.col(\"c.table_schema\") == SCHEMA)\n",
    "        # Uncomment the next line to filter table_name with 'omop_%'\n",
    "        # & (F.col(\"c.table_name\").like(\"omop_%\"))\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"c.table_catalog\"),\n",
    "        F.col(\"c.table_schema\"),\n",
    "        F.col(\"c.table_name\"),\n",
    "        F.col(\"c.column_name\").alias(\"name\"),\n",
    "        F.col(\"c.ordinal_position\"),\n",
    "        F.col(\"c.data_type\").alias(\"dataType\"),\n",
    "        F.col(\"s.ig_risk\"),\n",
    "        F.col(\"s.ig_severity\"),\n",
    "        F.col(\"s.sensitive\"),\n",
    "        F.col(\"c.comment\").alias(\"description\"),\n",
    "        F.lit(None).alias(\"json_str\"),\n",
    "        F.lit(None).alias(\"json_struct\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Insert into target table\n",
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8427534845032680,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "update_hdruk_metadata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
