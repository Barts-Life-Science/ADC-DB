{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42f9302c-090b-4ee4-9481-36b9cab8c428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook  \n",
    "from pyspark.sql import functions as F  \n",
    "from datetime import datetime\n",
    "  \n",
    "# Config  \n",
    "CATALOG = \"4_prod\"  \n",
    "SCHEMA = \"raw\"  \n",
    "RAW_CE = f\"`{CATALOG}`.`raw`.`mill_clinical_event`\"  \n",
    "RAW_ENC = f\"`{CATALOG}`.`raw`.`mill_encounter`\"  \n",
    "  \n",
    "# Modes:  \n",
    "FULL_REPAIR = False   # Set True for the initial historical backfill  \n",
    "LOOKBACK_DAYS = 3     # Only used when FULL_REPAIR == False  \n",
    "  \n",
    "BARTS_ORG_IDS = [873843, 8367658, 669849, 9073614, 2681833, 4401825, 3203824, 2681830, 8061679, 669848, 9163579, 8467812, 2681824, 9161976, 2619824, 2681827, 3203825, 691988, 3125827, 8061682, 8061694, 2641824, 2641827, 669847, 8056759, 8061685, 2641830, 3201824, 691989, 669845, 669843, 8061691, 669846, 3199824, 669850, 6333825, 669844, 0, 8397458, 8152502, 671843, 613843,9163579, 9161983\n",
    "]  \n",
    "\n",
    "BHRUT_ORG_IDS = [-1]\n",
    "\n",
    "# Get current timestamp for this run\n",
    "RUN_TIMESTAMP = datetime.now()\n",
    "\n",
    "spark.sql(f\"USE CATALOG `{CATALOG}`\")  \n",
    "\n",
    "# Create logging table for BHRUT updates and NULL trusts\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `6_mgmt`.`logs`.`bhrt_updates` (\n",
    "    table_name STRING,\n",
    "    event_id BIGINT,\n",
    "    encntr_id BIGINT,\n",
    "    organization_id BIGINT,\n",
    "    trust STRING,\n",
    "    processed_timestamp TIMESTAMP,\n",
    "    run_timestamp TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "  \n",
    "# Lean lookup views with valid_until_dt_tm filter for clinical_event\n",
    "spark.sql(f\"\"\"  \n",
    "CREATE OR REPLACE TEMP VIEW v_event_to_encntr AS  \n",
    "SELECT EVENT_ID, ENCNTR_ID  \n",
    "FROM {RAW_CE}  \n",
    "WHERE EVENT_ID IS NOT NULL \n",
    "  AND ENCNTR_ID IS NOT NULL\n",
    "  AND valid_until_dt_tm > current_timestamp()\n",
    "\"\"\")  \n",
    "\n",
    "# Create deduplicated view for EVENT_ID to ENCNTR_ID mapping\n",
    "# Using MAX to ensure we get a single value per EVENT_ID\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW v_event_to_encntr_dedup AS\n",
    "SELECT EVENT_ID, \n",
    "       MAX(ENCNTR_ID) as ENCNTR_ID\n",
    "FROM v_event_to_encntr\n",
    "GROUP BY EVENT_ID\n",
    "\"\"\")\n",
    "  \n",
    "spark.sql(f\"\"\"  \n",
    "CREATE OR REPLACE TEMP VIEW v_encntr_to_org AS  \n",
    "SELECT ENCNTR_ID, ORGANIZATION_ID  \n",
    "FROM {RAW_ENC}  \n",
    "WHERE ENCNTR_ID IS NOT NULL \n",
    "  AND ORGANIZATION_ID IS NOT NULL  \n",
    "\"\"\")  \n",
    "\n",
    "# Create deduplicated view for ENCNTR_ID to ORGANIZATION_ID mapping\n",
    "# Using MAX to ensure we get a single value per ENCNTR_ID\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW v_encntr_to_org_dedup AS\n",
    "SELECT ENCNTR_ID, \n",
    "       MAX(ORGANIZATION_ID) as ORGANIZATION_ID\n",
    "FROM v_encntr_to_org\n",
    "GROUP BY ENCNTR_ID\n",
    "\"\"\")\n",
    "  \n",
    "# Barts orgs view  \n",
    "if len(BARTS_ORG_IDS) > 0:  \n",
    "    barts_df = spark.createDataFrame([(int(x),) for x in BARTS_ORG_IDS], [\"ORGANIZATION_ID\"])  \n",
    "    barts_df.createOrReplaceTempView(\"v_barts_orgs\")  \n",
    "else:  \n",
    "    spark.sql(\"CREATE OR REPLACE TEMP VIEW v_barts_orgs AS SELECT CAST(NULL AS BIGINT) AS ORGANIZATION_ID WHERE FALSE\")\n",
    "\n",
    "# BHRUT orgs view  \n",
    "if len(BHRUT_ORG_IDS) > 0:  \n",
    "    bhrut_df = spark.createDataFrame([(int(x),) for x in BHRUT_ORG_IDS], [\"ORGANIZATION_ID\"])  \n",
    "    bhrut_df.createOrReplaceTempView(\"v_bhrut_orgs\")  \n",
    "else:  \n",
    "    spark.sql(\"CREATE OR REPLACE TEMP VIEW v_bhrut_orgs AS SELECT CAST(NULL AS BIGINT) AS ORGANIZATION_ID WHERE FALSE\")\n",
    "  \n",
    "# Helpers  \n",
    "def get_columns_and_types(table_name: str):  \n",
    "    rows = spark.sql(f\"\"\"  \n",
    "        SELECT column_name, data_type  \n",
    "        FROM system.information_schema.columns  \n",
    "        WHERE table_catalog = '{CATALOG}'  \n",
    "          AND table_schema = '{SCHEMA}'  \n",
    "          AND table_name = '{table_name}'  \n",
    "    \"\"\").collect()  \n",
    "    return {r.column_name.upper(): r.data_type.upper() for r in rows}  \n",
    "  \n",
    "def add_column_if_missing(fqn: str, col_name: str, data_type: str, cols_upper: set):  \n",
    "    if col_name.upper() not in cols_upper:  \n",
    "        spark.sql(f\"ALTER TABLE {fqn} ADD COLUMNS (`{col_name}` {data_type})\")  \n",
    "        cols_upper.add(col_name.upper())  \n",
    "  \n",
    "def adc_cond(alias: str, col_types: dict) -> str:\n",
    "    \"\"\"Returns a condition to filter records based on ADC_UPDT age\"\"\"\n",
    "    if FULL_REPAIR or \"ADC_UPDT\" not in col_types:\n",
    "        return \"\"\n",
    "    \n",
    "    # ADC_UPDT exists and should always be a temporal field\n",
    "    return f\" AND {alias}.ADC_UPDT >= current_timestamp() - INTERVAL {LOOKBACK_DAYS} DAYS\"\n",
    "\n",
    "def trust_cond() -> str:\n",
    "    \"\"\"Returns the trust condition based on FULL_REPAIR mode\n",
    "    In FULL_REPAIR mode, we overwrite existing trust assignments\n",
    "    Otherwise, we only update NULL trust values\"\"\"\n",
    "    if FULL_REPAIR:\n",
    "        return \"\"  # No condition - overwrite all\n",
    "    else:\n",
    "        return \" AND (Trust IS NULL)\"\n",
    "\n",
    "def find_unique_key(cols_upper: set) -> str:\n",
    "    \"\"\"Find a unique identifier column if it exists\"\"\"\n",
    "    possible_keys = [\"ROW_ID\", \"RECORD_ID\", \"ID\", \"UNIQUE_ID\", \"PK_ID\", \"CLINICAL_EVENT_ID\", \"ENCNTR_ALIAS_ID\"]\n",
    "    for key in possible_keys:\n",
    "        if key in cols_upper:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "def update_encntr_id_safe(fqn: str, col_types: dict):\n",
    "    \"\"\"Safely update ENCNTR_ID using aggregated subquery\"\"\"\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE {fqn}\n",
    "        SET ENCNTR_ID = (\n",
    "            SELECT MAX(m.ENCNTR_ID)\n",
    "            FROM v_event_to_encntr_dedup AS m\n",
    "            WHERE m.EVENT_ID = {fqn}.EVENT_ID\n",
    "        )\n",
    "        WHERE EVENT_ID IS NOT NULL\n",
    "          AND ENCNTR_ID IS NULL\n",
    "          {adc_cond(fqn, col_types)}\n",
    "    \"\"\")\n",
    "\n",
    "def update_organization_id_safe(fqn: str, col_types: dict):\n",
    "    \"\"\"Safely update ORGANIZATION_ID using aggregated subquery\"\"\"\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE {fqn}\n",
    "        SET ORGANIZATION_ID = (\n",
    "            SELECT MAX(m.ORGANIZATION_ID)\n",
    "            FROM v_encntr_to_org_dedup AS m\n",
    "            WHERE m.ENCNTR_ID = {fqn}.ENCNTR_ID\n",
    "        )\n",
    "        WHERE ENCNTR_ID IS NOT NULL\n",
    "          AND ORGANIZATION_ID IS NULL\n",
    "          {adc_cond(fqn, col_types)}\n",
    "    \"\"\")\n",
    "\n",
    "def log_trust_records(fqn: str, table_name: str, col_types: dict, cols_upper: set):\n",
    "    \"\"\"Log rows with BHRUT trust or NULL trust that are at least 3 days old (if eligible for lookup)\"\"\"\n",
    "    # Build the select columns based on what exists in the table\n",
    "    select_cols = []\n",
    "    if \"EVENT_ID\" in cols_upper:\n",
    "        select_cols.append(\"EVENT_ID\")\n",
    "    else:\n",
    "        select_cols.append(\"NULL AS EVENT_ID\")\n",
    "    \n",
    "    if \"ENCNTR_ID\" in cols_upper:\n",
    "        select_cols.append(\"ENCNTR_ID\")\n",
    "    else:\n",
    "        select_cols.append(\"NULL AS ENCNTR_ID\")\n",
    "    \n",
    "    if \"ORGANIZATION_ID\" in cols_upper:\n",
    "        select_cols.append(\"ORGANIZATION_ID\")\n",
    "    else:\n",
    "        select_cols.append(\"NULL AS ORGANIZATION_ID\")\n",
    "    \n",
    "    # Build date condition for NULL trust rows (3+ days old)\n",
    "    date_cond = \"\"\n",
    "    if \"ADC_UPDT\" in col_types:\n",
    "        # ADC_UPDT should always be a timestamp - check records at least 3 days old\n",
    "        date_cond = \"AND ADC_UPDT <= current_timestamp() - INTERVAL 3 DAYS\"    \n",
    "        \n",
    "    # Build condition for NULL trust - only log if eligible for lookup\n",
    "    # (has non-null ENCNTR_ID or EVENT_ID)\n",
    "    null_trust_cond = \"Trust IS NULL\"\n",
    "    if \"ENCNTR_ID\" in cols_upper or \"EVENT_ID\" in cols_upper:\n",
    "        # Table is eligible for lookup, check if the row has the necessary IDs\n",
    "        eligibility_parts = []\n",
    "        if \"ENCNTR_ID\" in cols_upper:\n",
    "            eligibility_parts.append(\"ENCNTR_ID IS NOT NULL\")\n",
    "        if \"EVENT_ID\" in cols_upper:\n",
    "            eligibility_parts.append(\"EVENT_ID IS NOT NULL\")\n",
    "        eligibility_cond = \" OR \".join(eligibility_parts)\n",
    "        null_trust_cond = f\"(Trust IS NULL AND ({eligibility_cond}) {date_cond})\"\n",
    "    else:\n",
    "        # Table not eligible for lookup, don't log NULL trust rows\n",
    "        null_trust_cond = \"FALSE\"\n",
    "    \n",
    "    # Insert rows that are either BHRUT or NULL trust (if eligible)\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO `6_mgmt`.`logs`.`bhrt_updates`\n",
    "        SELECT \n",
    "            '{table_name}' AS table_name,\n",
    "            {', '.join(select_cols)},\n",
    "            Trust AS trust,\n",
    "            current_timestamp() AS processed_timestamp,\n",
    "            timestamp('{RUN_TIMESTAMP.isoformat()}') AS run_timestamp\n",
    "        FROM {fqn}\n",
    "        WHERE (\n",
    "            Trust = 'BHRUT' \n",
    "            OR {null_trust_cond}\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "# Target tables  \n",
    "tables = [r.table_name for r in spark.sql(f\"\"\"  \n",
    "SELECT table_name  \n",
    "FROM system.information_schema.tables  \n",
    "WHERE table_catalog = '{CATALOG}'  \n",
    "  AND table_schema = '{SCHEMA}'\n",
    "\"\"\").collect()]  \n",
    "  \n",
    "for t in tables:  \n",
    "    fqn = f\"`{CATALOG}`.`{SCHEMA}`.`{t}`\"  \n",
    "    try:  \n",
    "        col_types = get_columns_and_types(t)  \n",
    "        cols_upper = set(col_types.keys())  \n",
    "        unique_key = find_unique_key(cols_upper)\n",
    "  \n",
    "        has_event = \"EVENT_ID\" in cols_upper  \n",
    "        has_enc = \"ENCNTR_ID\" in cols_upper  \n",
    "  \n",
    "        # 1) ENCNTR_ID: ensure column and backfill via UPDATE with aggregated subquery\n",
    "        if has_event:  \n",
    "            add_column_if_missing(fqn, \"ENCNTR_ID\", \"BIGINT\", cols_upper)  \n",
    "            \n",
    "            # Try MERGE first with a better construction, fall back to UPDATE if it fails\n",
    "            try:\n",
    "                if unique_key:\n",
    "                    # Use MERGE with unique key and row_number to avoid duplicates\n",
    "                    spark.sql(f\"\"\"  \n",
    "                        MERGE INTO {fqn} AS tgt  \n",
    "                        USING (  \n",
    "                            SELECT {unique_key}, EVENT_ID, ENCNTR_ID\n",
    "                            FROM (\n",
    "                                SELECT p.{unique_key}, p.EVENT_ID, m.ENCNTR_ID,\n",
    "                                       ROW_NUMBER() OVER (PARTITION BY p.{unique_key} ORDER BY m.ENCNTR_ID) as rn\n",
    "                                FROM {fqn} AS p  \n",
    "                                JOIN v_event_to_encntr_dedup AS m ON p.EVENT_ID = m.EVENT_ID  \n",
    "                                WHERE p.EVENT_ID IS NOT NULL  \n",
    "                                  AND p.ENCNTR_ID IS NULL  \n",
    "                                  {adc_cond('p', col_types)}\n",
    "                            )\n",
    "                            WHERE rn = 1\n",
    "                        ) src  \n",
    "                        ON tgt.{unique_key} = src.{unique_key}\n",
    "                        WHEN MATCHED THEN UPDATE SET tgt.ENCNTR_ID = src.ENCNTR_ID  \n",
    "                    \"\"\")\n",
    "                else:\n",
    "                    # No unique key, use UPDATE with aggregated subquery\n",
    "                    update_encntr_id_safe(fqn, col_types)\n",
    "            except:\n",
    "                # If MERGE fails, fall back to UPDATE with aggregated subquery\n",
    "                update_encntr_id_safe(fqn, col_types)\n",
    "            \n",
    "            has_enc = True  # now present  \n",
    "  \n",
    "        # 2) ORGANIZATION_ID: ensure column and backfill  \n",
    "        if has_enc:  \n",
    "            add_column_if_missing(fqn, \"ORGANIZATION_ID\", \"BIGINT\", cols_upper)  \n",
    "            \n",
    "            # Try MERGE first with a better construction, fall back to UPDATE if it fails\n",
    "            try:\n",
    "                if unique_key:\n",
    "                    # Use MERGE with unique key and row_number to avoid duplicates\n",
    "                    spark.sql(f\"\"\"  \n",
    "                        MERGE INTO {fqn} AS tgt  \n",
    "                        USING (  \n",
    "                            SELECT {unique_key}, ENCNTR_ID, ORGANIZATION_ID\n",
    "                            FROM (\n",
    "                                SELECT q.{unique_key}, q.ENCNTR_ID, m.ORGANIZATION_ID,\n",
    "                                       ROW_NUMBER() OVER (PARTITION BY q.{unique_key} ORDER BY m.ORGANIZATION_ID) as rn\n",
    "                                FROM {fqn} AS q  \n",
    "                                JOIN v_encntr_to_org_dedup AS m ON q.ENCNTR_ID = m.ENCNTR_ID  \n",
    "                                WHERE q.ENCNTR_ID IS NOT NULL  \n",
    "                                  AND q.ORGANIZATION_ID IS NULL  \n",
    "                                  {adc_cond('q', col_types)}\n",
    "                            )\n",
    "                            WHERE rn = 1\n",
    "                        ) src  \n",
    "                        ON tgt.{unique_key} = src.{unique_key}\n",
    "                        WHEN MATCHED THEN UPDATE SET tgt.ORGANIZATION_ID = src.ORGANIZATION_ID  \n",
    "                    \"\"\")\n",
    "                else:\n",
    "                    # No unique key, use UPDATE with aggregated subquery\n",
    "                    update_organization_id_safe(fqn, col_types)\n",
    "            except:\n",
    "                # If MERGE fails, fall back to UPDATE with aggregated subquery\n",
    "                update_organization_id_safe(fqn, col_types)\n",
    "  \n",
    "        # 3) Trust: always ensure the column exists  \n",
    "        add_column_if_missing(fqn, \"Trust\", \"STRING\", cols_upper)  \n",
    "  \n",
    "        # Only update Trust if ORGANIZATION_ID exists on the table  \n",
    "        has_org = \"ORGANIZATION_ID\" in cols_upper  \n",
    "        if has_org:  \n",
    "            # Update Trust='BHRUT' for BHRUT organizations\n",
    "            # In FULL_REPAIR mode, overwrite existing trust assignments\n",
    "            spark.sql(f\"\"\"\n",
    "                UPDATE {fqn}\n",
    "                SET Trust = 'BHRUT'\n",
    "                WHERE ORGANIZATION_ID IN (SELECT ORGANIZATION_ID FROM v_bhrut_orgs)\n",
    "                  {trust_cond()}\n",
    "                  {adc_cond(fqn, col_types)}\n",
    "            \"\"\")\n",
    "            \n",
    "            # Update Trust='Barts' for Barts organizations\n",
    "            # In FULL_REPAIR mode, overwrite existing trust assignments (except BHRUT which has priority)\n",
    "            # In normal mode, only update if Trust is NULL\n",
    "            if FULL_REPAIR:\n",
    "                # In full repair, update Barts but don't overwrite BHRUT\n",
    "                spark.sql(f\"\"\"\n",
    "                    UPDATE {fqn}\n",
    "                    SET Trust = 'Barts'\n",
    "                    WHERE ORGANIZATION_ID IN (SELECT ORGANIZATION_ID FROM v_barts_orgs)\n",
    "                      {adc_cond(fqn, col_types)}\n",
    "                \"\"\")\n",
    "            else:\n",
    "                # In normal mode, only update NULL trust values\n",
    "                spark.sql(f\"\"\"\n",
    "                    UPDATE {fqn}\n",
    "                    SET Trust = 'Barts'\n",
    "                    WHERE ORGANIZATION_ID IN (SELECT ORGANIZATION_ID FROM v_barts_orgs)\n",
    "                      AND (Trust IS NULL)\n",
    "                      {adc_cond(fqn, col_types)}\n",
    "                \"\"\")\n",
    "            \n",
    "            # Log trust records AFTER the assignments\n",
    "            try:\n",
    "                log_trust_records(fqn, t, col_types, cols_upper)\n",
    "            except Exception as log_e:\n",
    "                print(f\"Warning: Failed to log trust records for {fqn}: {log_e}\")\n",
    "        else:  \n",
    "            # No ORGANIZATION_ID column on this table; still log NULL trust records if Trust column exists\n",
    "            if \"TRUST\" in cols_upper:\n",
    "                try:\n",
    "                    log_trust_records(fqn, t, col_types, cols_upper)\n",
    "                except Exception as log_e:\n",
    "                    print(f\"Warning: Failed to log trust records for {fqn}: {log_e}\")\n",
    "  \n",
    "        print(f\"Processed {fqn}\")  \n",
    "  \n",
    "    except Exception as e:  \n",
    "        print(f\"Error processing {fqn}: {e}\")\n",
    "\n",
    "# Print summary of logged records\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        trust,\n",
    "        COUNT(*) as record_count,\n",
    "        COUNT(DISTINCT table_name) as tables_affected\n",
    "    FROM `6_mgmt`.`logs`.`bhrt_updates`\n",
    "    WHERE run_timestamp = timestamp('{RUN_TIMESTAMP.isoformat()}')\n",
    "    GROUP BY trust\n",
    "    ORDER BY trust\n",
    "\"\"\").show()\n",
    "\n",
    "# Also show total summary\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_logged,\n",
    "        COUNT(DISTINCT table_name) as total_tables_affected,\n",
    "        SUM(CASE WHEN trust = 'BHRUT' THEN 1 ELSE 0 END) as bhrut_records,\n",
    "        SUM(CASE WHEN trust IS NULL THEN 1 ELSE 0 END) as null_trust_records\n",
    "    FROM `6_mgmt`.`logs`.`bhrt_updates`\n",
    "    WHERE run_timestamp = timestamp('{RUN_TIMESTAMP.isoformat()}')\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Trust Tagging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
