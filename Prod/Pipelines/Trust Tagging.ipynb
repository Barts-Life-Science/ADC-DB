{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42f9302c-090b-4ee4-9481-36b9cab8c428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "import time\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "CATALOG = \"4_prod\"\n",
    "RAW_SCHEMA = \"raw\"\n",
    "TMP_SCHEMA = \"tmp\"\n",
    "\n",
    "# Log locations\n",
    "LOG_CATALOG = \"6_mgmt\"\n",
    "LOG_SCHEMA = \"logs\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "RUN_TS = datetime.now()\n",
    "IS_WEEKLY_RUN = (RUN_TS.weekday() == 6)  # Sunday = Deep Clean\n",
    "\n",
    "# Retention & Settings\n",
    "LOOKUP_RETENTION_DAYS = 14\n",
    "METADATA_FRESHNESS_DAYS = 30\n",
    "INITIAL_LOOKBACK_VERSIONS = 25\n",
    "FILTER_BHRUT = True\n",
    "BACKWINDOW_DAYS = 7\n",
    "FALLBACK_LOOKBACK_HOURS = 48\n",
    "\n",
    "# Tables\n",
    "TRUST_MAP_TBL = f\"{CATALOG}.{TMP_SCHEMA}.org_to_trust_map\"\n",
    "ORG_HIST_TBL = f\"{CATALOG}.{TMP_SCHEMA}.org_mapping_history\"\n",
    "HUB_TBL = f\"{CATALOG}.{TMP_SCHEMA}.sw_mapping_hub\"\n",
    "ENC_ORG_TBL = f\"{CATALOG}.{TMP_SCHEMA}.sw_enc_org_map\"\n",
    "CHANGED_ENC_TBL = f\"{CATALOG}.{TMP_SCHEMA}.sw_changed_encounters\"\n",
    "CONTROL_TBL = f\"{CATALOG}.{TMP_SCHEMA}.incr_updt_trust_control\"\n",
    "\n",
    "# Logging tables\n",
    "FLAG_TBL = f\"{LOG_CATALOG}.{LOG_SCHEMA}.organization_flags\"\n",
    "AUDIT_TBL = f\"{LOG_CATALOG}.{LOG_SCHEMA}.bhrt_updates\"\n",
    "# NEW: Aggregated audit summary table\n",
    "AUDIT_SUMMARY_TBL = f\"{LOG_CATALOG}.{LOG_SCHEMA}.trust_audit_summary\"\n",
    "# NEW: Phase 2c version tracking for skip optimization\n",
    "PHASE2C_TRACKER_TBL = f\"{CATALOG}.{TMP_SCHEMA}.phase2c_version_tracker\"\n",
    "\n",
    "# Org Config\n",
    "BARTS_ORGS = [\n",
    "    873843, 8367658, 669849, 9073614, 2681833, 4401825, 3203824, 2681830,\n",
    "    8061679, 669848, 8467812, 2681824, 2619824, 2681827, 3203825, 691988,\n",
    "    3125827, 8061682, 8061694, 2641824, 2641827, 669847, 8056759, 8061685,\n",
    "    2641830, 3201824, 691989, 669845, 669843, 8061691, 669846, 3199824,\n",
    "    669850, 6333825, 669844, 8397458, 8152502, 671843, 613843\n",
    "]\n",
    "BHRUT_ORGS = [9161976, 9163579, 9161983, 723896, 9161987, 9161988, 9163583, 9161989]\n",
    "\n",
    "# Key Lookups Configuration\n",
    "KEY_LOOKUPS = [\n",
    "    (\"SURG_CASE_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_surgical_case\", \"ENCNTR_ID\"),\n",
    "    (\"SURG_CASE_PROC_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_surg_case_procedure\", \"NULL\"),\n",
    "    (\"DCP_FORMS_ACTIVITY_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_dcp_forms_activity\", \"ENCNTR_ID\"),\n",
    "    (\"EPISODE_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_episode_encntr_reltn\", \"ENCNTR_ID\"),\n",
    "    (\"ORDER_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_orders\", \"ENCNTR_ID\"),\n",
    "    (\"PROBLEM_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_problem\", \"COALESCE(ORIGINATING_ENCNTR_ID, UPDATE_ENCNTR_ID)\"),\n",
    "    (\"SCH_EVENT_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_sch_event_patient\", \"ENCNTR_ID\"),\n",
    "    (\"SCHEDULE_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_sch_schedule\", \"ENCNTR_ID\"),\n",
    "    (\"IM_STUDY_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_im_study\", \"ENCNTR_ID\"),\n",
    "    (\"IM_ACQUIRED_STUDY_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_im_acquired_study\", \"NULL\"),\n",
    "    (\"CV_PROC_ID\", f\"{CATALOG}.{RAW_SCHEMA}.mill_cv_proc\", \"ENCNTR_ID\")\n",
    "]\n",
    "\n",
    "# Dependency Map for Chained Updates\n",
    "CHAINED_DEPENDENCIES = {\n",
    "    \"IM_STUDY_ID\": (\"mill_im_acquired_study\", \"MATCHED_STUDY_ID\", \"IM_ACQUIRED_STUDY_ID\"),\n",
    "    \"SURG_CASE_ID\": (\"mill_surg_case_procedure\", \"SURG_CASE_ID\", \"SURG_CASE_PROC_ID\")\n",
    "}\n",
    "\n",
    "# Tables protected from deletion\n",
    "TABLES_SKIP_NULL_TRUST_LOGGING = {\n",
    "    \"MILL_PERSON_ORG_RELTN\", \"MILL_ORG_ORG_RELTN\", \"MILL_ORGANIZATION_ALIAS\",\n",
    "    \"MILL_ORGANIZATION\", \"MILL_PRSNL_ORG_RELTN\", \"MILL_ORG_TYPE_RELTN\",\n",
    "    \"MILL_SCH_LOCATION\", \"MILL_SCH_EVENT\", \"MILL_SCH_APPT\",\n",
    "    \"MILL_SCH_EVENT_ALIAS\", \"MILL_SCH_SCHEDULE\"\n",
    "}\n",
    "\n",
    "# Source of Truth Tables - don't overwrite ENCNTR_ID on these\n",
    "SOURCE_OF_TRUTH_TABLES = {\n",
    "    \"MILL_CLINICAL_EVENT\", \"MILL_ENCOUNTER\", \"MILL_ORDERS\",\n",
    "    \"MILL_SURGICAL_CASE\", \"MILL_DCP_FORMS_ACTIVITY\",\n",
    "    \"MILL_EPISODE_ENCNTR_RELTN\", \"MILL_SCH_EVENT_PATIENT\",\n",
    "    \"MILL_SCH_SCHEDULE\", \"MILL_IM_STUDY\", \"MILL_CV_PROC\",\n",
    "    \"MILL_PROBLEM\"\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. INFRASTRUCTURE & HELPERS\n",
    "# ==============================================================================\n",
    "def time_op(name, fn):\n",
    "    \"\"\"Time an operation and print duration\"\"\"\n",
    "    s = time.time()\n",
    "    try:\n",
    "        res = fn()\n",
    "        duration = time.time() - s\n",
    "        print(f\"  {name}: {duration:.2f}s\")\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        duration = time.time() - s\n",
    "        print(f\"  {name} FAILED ({duration:.2f}s): {str(e)[:500]}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def ensure_setup():\n",
    "    \"\"\"Create all required schemas and tables\"\"\"\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{TMP_SCHEMA}\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {FLAG_TBL} (\n",
    "            organization_id LONG, alert INT, event_time TIMESTAMP,\n",
    "            table_name STRING, first_seen_timestamp TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {CONTROL_TBL} (\n",
    "            table_name STRING, last_version LONG, last_timestamp TIMESTAMP, updated_at TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    # Original detailed audit table (keep for backward compatibility, but we won't insert to it by default)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {AUDIT_TBL} (\n",
    "            table_name STRING, event_id BIGINT, encntr_id BIGINT,\n",
    "            organization_id BIGINT, trust STRING, processed_timestamp TIMESTAMP,\n",
    "            run_timestamp TIMESTAMP, adc_timestamp TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    # NEW: Aggregated audit summary - much lower overhead\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {AUDIT_SUMMARY_TBL} (\n",
    "            run_timestamp TIMESTAMP,\n",
    "            table_name STRING,\n",
    "            trust STRING,\n",
    "            operation STRING,\n",
    "            record_count BIGINT,\n",
    "            min_pk BIGINT,\n",
    "            max_pk BIGINT\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {ORG_HIST_TBL} (\n",
    "            organization_id LONG, trust STRING, first_seen_timestamp TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {TRUST_MAP_TBL} (organization_id LONG, trust STRING) USING DELTA\")\n",
    "\n",
    "    values = \",\".join([f\"({o}, 'Barts')\" for o in BARTS_ORGS] + [f\"({o}, 'BHRUT')\" for o in BHRUT_ORGS])\n",
    "    spark.sql(f\"INSERT OVERWRITE {TRUST_MAP_TBL} SELECT * FROM VALUES {values}\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {ENC_ORG_TBL} (\n",
    "            ENCNTR_ID LONG, ORGANIZATION_ID LONG, last_updated TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {HUB_TBL} (\n",
    "            key_type STRING, key_id LONG, ENCNTR_ID LONG, last_updated TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {CHANGED_ENC_TBL} (\n",
    "            ENCNTR_ID LONG, ORGANIZATION_ID LONG, trust STRING,\n",
    "            change_timestamp TIMESTAMP, run_date DATE\n",
    "        ) USING DELTA PARTITIONED BY (run_date)\n",
    "    \"\"\")\n",
    "    \n",
    "    # NEW: Phase 2c version tracker - tracks which version we last updated each table to\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {PHASE2C_TRACKER_TBL} (\n",
    "            table_name STRING,\n",
    "            last_updated_version LONG,\n",
    "            hub_version LONG,\n",
    "            enc_org_version LONG,\n",
    "            updated_at TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def track_newly_mapped_orgs():\n",
    "    \"\"\"Identifies Orgs added to the config in the last BACKWINDOW_DAYS\"\"\"\n",
    "    all_org_ids = [(o, 'Barts') for o in BARTS_ORGS] + [(o, 'BHRUT') for o in BHRUT_ORGS]\n",
    "    df_config = spark.createDataFrame(all_org_ids, [\"organization_id\", \"trust\"])\n",
    "    df_hist = spark.table(ORG_HIST_TBL)\n",
    "\n",
    "    new_orgs = df_config.join(df_hist, \"organization_id\", \"left_anti\") \\\n",
    "        .withColumn(\"first_seen_timestamp\", F.current_timestamp())\n",
    "\n",
    "    if new_orgs.count() > 0:\n",
    "        new_orgs.write.format(\"delta\").mode(\"append\").saveAsTable(ORG_HIST_TBL)\n",
    "        print(f\"    Added {new_orgs.count()} new organizations to history\")\n",
    "\n",
    "    recent = spark.sql(f\"\"\"\n",
    "        SELECT organization_id FROM {ORG_HIST_TBL}\n",
    "        WHERE first_seen_timestamp >= current_timestamp() - INTERVAL {BACKWINDOW_DAYS} DAYS\n",
    "    \"\"\").collect()\n",
    "    return [row['organization_id'] for row in recent]\n",
    "\n",
    "\n",
    "def can_delete_bhrut(table_upper: str) -> bool:\n",
    "    return FILTER_BHRUT and (table_upper.upper() not in TABLES_SKIP_NULL_TRUST_LOGGING)\n",
    "\n",
    "\n",
    "def get_table_columns(fqn: str) -> set:\n",
    "    try:\n",
    "        return {c.upper() for c in spark.table(fqn).columns}\n",
    "    except:\n",
    "        return set()\n",
    "\n",
    "\n",
    "def pick_time_col(fqn: str):\n",
    "    \"\"\"Pick a suitable timestamp column for CDC fallback filters.\"\"\"\n",
    "    try:\n",
    "        cols = {c.upper() for c in spark.table(fqn).columns}\n",
    "    except:\n",
    "        return None\n",
    "    if \"ADC_UPDT\" in cols:\n",
    "        return \"ADC_UPDT\"\n",
    "    if \"UPDT_DT_TM\" in cols:\n",
    "        return \"UPDT_DT_TM\"\n",
    "    if \"CLINSIG_UPDT_DT_TM\" in cols:\n",
    "        return \"CLINSIG_UPDT_DT_TM\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def log_status(table_name, status, reason=\"\"):\n",
    "    \"\"\"Standardized logging format with ANSI colors.\"\"\"\n",
    "    c_blue = \"\\033[94m\"\n",
    "    c_green = \"\\033[92m\"\n",
    "    c_red = \"\\033[91m\"\n",
    "    c_yellow = \"\\033[93m\"\n",
    "    c_reset = \"\\033[0m\"\n",
    "\n",
    "    color = c_reset\n",
    "    if status == \"QUEUE\":\n",
    "        color = c_blue\n",
    "    if status == \"PROC\":\n",
    "        color = c_green\n",
    "    if status == \"ERR\":\n",
    "        color = c_red\n",
    "    if status == \"SKIP\":\n",
    "        color = c_yellow\n",
    "\n",
    "    print(f\"    {color}[{status}] {table_name:<35} : {reason}{c_reset}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1b. VERSION TRACKING HELPERS (NEW)\n",
    "# ==============================================================================\n",
    "def get_table_current_version(fqn: str) -> int:\n",
    "    \"\"\"Get the current version of a Delta table.\"\"\"\n",
    "    try:\n",
    "        return spark.sql(f\"DESCRIBE HISTORY {fqn} LIMIT 1\").collect()[0]['version']\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def get_hub_versions() -> tuple:\n",
    "    \"\"\"Get current versions of hub and enc_org tables.\"\"\"\n",
    "    hub_ver = get_table_current_version(HUB_TBL)\n",
    "    enc_org_ver = get_table_current_version(ENC_ORG_TBL)\n",
    "    return hub_ver, enc_org_ver\n",
    "\n",
    "\n",
    "def should_skip_phase2c(table_name: str, fqn: str, current_hub_ver: int, current_enc_org_ver: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Check if we can skip Phase 2c processing for this table.\n",
    "    \n",
    "    Returns (should_skip: bool, reason: str)\n",
    "    \n",
    "    Skip conditions:\n",
    "    1. Our pipeline was the last to update this table (table version unchanged)\n",
    "    2. AND the hub/enc_org mappings haven't changed since our last update\n",
    "    \n",
    "    This means: if no new data came into the table AND no new encounter mappings\n",
    "    were created, there's nothing new to propagate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        row = spark.sql(f\"\"\"\n",
    "            SELECT last_updated_version, hub_version, enc_org_version\n",
    "            FROM {PHASE2C_TRACKER_TBL}\n",
    "            WHERE table_name = '{table_name}'\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        if not row:\n",
    "            return False, \"No previous tracking record\"\n",
    "        \n",
    "        tracked = row[0]\n",
    "        last_table_ver = tracked['last_updated_version']\n",
    "        last_hub_ver = tracked['hub_version']\n",
    "        last_enc_org_ver = tracked['enc_org_version']\n",
    "        \n",
    "        current_table_ver = get_table_current_version(fqn)\n",
    "        \n",
    "        # Condition 1: Table version unchanged (we were the last updater)\n",
    "        if current_table_ver != last_table_ver:\n",
    "            return False, f\"Table changed (v{last_table_ver} -> v{current_table_ver})\"\n",
    "        \n",
    "        # Condition 2: Hub mappings unchanged (no new key->encounter mappings)\n",
    "        if current_hub_ver != last_hub_ver:\n",
    "            return False, f\"Hub changed (v{last_hub_ver} -> v{current_hub_ver})\"\n",
    "        \n",
    "        # Condition 3: Encounter->Org mappings unchanged\n",
    "        if current_enc_org_ver != last_enc_org_ver:\n",
    "            return False, f\"Enc-Org map changed (v{last_enc_org_ver} -> v{current_enc_org_ver})\"\n",
    "        \n",
    "        # All conditions met - safe to skip\n",
    "        return True, f\"No changes since last update (table v{current_table_ver}, hub v{current_hub_ver})\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Tracking check failed: {str(e)[:50]}\"\n",
    "\n",
    "\n",
    "def update_phase2c_tracker(table_name: str, fqn: str, hub_ver: int, enc_org_ver: int):\n",
    "    \"\"\"Record that we updated this table in Phase 2c with current hub versions.\"\"\"\n",
    "    table_ver = get_table_current_version(fqn)\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {PHASE2C_TRACKER_TBL} t\n",
    "        USING (SELECT \n",
    "            '{table_name}' as table_name, \n",
    "            {table_ver} as last_updated_version,\n",
    "            {hub_ver} as hub_version,\n",
    "            {enc_org_ver} as enc_org_version,\n",
    "            current_timestamp() as updated_at\n",
    "        ) s\n",
    "        ON t.table_name = s.table_name\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "            last_updated_version = s.last_updated_version,\n",
    "            hub_version = s.hub_version,\n",
    "            enc_org_version = s.enc_org_version,\n",
    "            updated_at = s.updated_at\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MATERIALIZED HUB VIEWS (NEW - Performance Optimization)\n",
    "# ==============================================================================\n",
    "def materialize_hub_views():\n",
    "    \"\"\"\n",
    "    Create temp views for the deduplicated hub and enc_org mappings.\n",
    "    These are reused throughout the pipeline instead of recalculating.\n",
    "    \"\"\"\n",
    "    print(\"  Materializing hub views...\")\n",
    "\n",
    "    # Deduplicated Encounter -> Org mapping\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW enc_org_deduped AS\n",
    "        SELECT ENCNTR_ID, ORGANIZATION_ID\n",
    "        FROM (\n",
    "            SELECT *, ROW_NUMBER() OVER (PARTITION BY ENCNTR_ID ORDER BY last_updated DESC) as rn\n",
    "            FROM {ENC_ORG_TBL}\n",
    "        )\n",
    "        WHERE rn = 1\n",
    "    \"\"\")\n",
    "\n",
    "    # Deduplicated Hub (all key types)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW hub_deduped AS\n",
    "        SELECT key_type, key_id, ENCNTR_ID\n",
    "        FROM (\n",
    "            SELECT *, ROW_NUMBER() OVER (PARTITION BY key_type, key_id ORDER BY last_updated DESC) as rn\n",
    "            FROM {HUB_TBL}\n",
    "        )\n",
    "        WHERE rn = 1\n",
    "    \"\"\")\n",
    "\n",
    "    # Trust map as temp view for easy joining\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW trust_map AS\n",
    "        SELECT organization_id, trust FROM {TRUST_MAP_TBL}\n",
    "    \"\"\")\n",
    "\n",
    "    # Pre-joined enc -> trust lookup (frequently used)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW enc_trust_lookup AS\n",
    "        SELECT eo.ENCNTR_ID, eo.ORGANIZATION_ID, tm.trust\n",
    "        FROM enc_org_deduped eo\n",
    "        JOIN trust_map tm ON eo.ORGANIZATION_ID = tm.organization_id\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DYNAMIC DISCOVERY (With Scopes)\n",
    "# ==============================================================================\n",
    "def get_table_scopes():\n",
    "    \"\"\"Scans schema and logs why tables are selected or rejected.\"\"\"\n",
    "    print(f\"  Scanning schema for candidate tables...\")\n",
    "\n",
    "    try:\n",
    "        all_tables_rows = spark.sql(f\"SHOW TABLES IN {CATALOG}.{RAW_SCHEMA}\").collect()\n",
    "        all_tables = sorted([row.tableName for row in all_tables_rows])\n",
    "    except Exception as e:\n",
    "        print(f\"    Error listing tables: {e}\")\n",
    "        return {}, {}\n",
    "\n",
    "    cdc_candidates = {}\n",
    "    propagation_candidates = {}\n",
    "\n",
    "    print(f\"  Found {len(all_tables)} total tables. Analyzing...\")\n",
    "\n",
    "    for t in all_tables:\n",
    "        fqn = f\"{CATALOG}.{RAW_SCHEMA}.{t}\"\n",
    "\n",
    "        if not t.startswith(\"mill_\") or t.lower() == \"mill_long_text\":\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            cols = get_table_columns(fqn)\n",
    "\n",
    "            if \"TRUST\" not in cols:\n",
    "                log_status(t, \"SKIP\", \"No 'TRUST' column found\")\n",
    "                continue\n",
    "\n",
    "            propagation_candidates[t] = cols\n",
    "\n",
    "            details = spark.sql(f\"DESCRIBE DETAIL {fqn}\").collect()[0]\n",
    "            delta = datetime.now() - details['lastModified']\n",
    "            hours_old = delta.total_seconds() / 3600\n",
    "            days_old = delta.days\n",
    "\n",
    "            is_forced = globals().get('FORCE_RUN', False)\n",
    "\n",
    "            if is_forced or (days_old <= METADATA_FRESHNESS_DAYS):\n",
    "                cdc_candidates[t] = cols\n",
    "                status_msg = f\"Queued for CDC (Last mod: {hours_old:.1f}h ago)\"\n",
    "                if is_forced:\n",
    "                    status_msg += \" [FORCED]\"\n",
    "                log_status(t, \"QUEUE\", status_msg)\n",
    "            else:\n",
    "                log_status(t, \"SKIP\", f\"Stale data (Last mod: {hours_old:.1f}h ago > {METADATA_FRESHNESS_DAYS}d limit)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_status(t, \"ERR\", f\"Metadata read failed: {str(e)[:50]}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"  Summary: {len(cdc_candidates)} CDC Candidates, {len(propagation_candidates)} Propagation Candidates.\")\n",
    "    return cdc_candidates, propagation_candidates\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CDC UTILITIES WITH FALLBACK\n",
    "# ==============================================================================\n",
    "def get_cdc_window(fqn, checkpoint_key=None):\n",
    "    \"\"\"\n",
    "    Returns: (start_version, end_version) or (\"FALLBACK\", last_timestamp) or (None, None)\n",
    "    \"\"\"\n",
    "    lookup_key = checkpoint_key if checkpoint_key else fqn\n",
    "\n",
    "    try:\n",
    "        hist_df = spark.sql(f\"DESCRIBE HISTORY {fqn}\")\n",
    "        hist_summary = hist_df.select(\n",
    "            F.min(\"version\").alias(\"min_ver\"),\n",
    "            F.max(\"version\").alias(\"max_ver\")\n",
    "        ).collect()[0]\n",
    "        min_avail_ver, curr_ver = hist_summary['min_ver'], hist_summary['max_ver']\n",
    "\n",
    "        row = spark.sql(f\"\"\"\n",
    "            SELECT last_version, last_timestamp FROM {CONTROL_TBL}\n",
    "            WHERE table_name = '{lookup_key}'\n",
    "        \"\"\").collect()\n",
    "        last_processed = row[0]['last_version'] if row else None\n",
    "        last_ts = row[0]['last_timestamp'] if row else None\n",
    "\n",
    "        if last_processed is None:\n",
    "            start_ver = max(curr_ver - INITIAL_LOOKBACK_VERSIONS, min_avail_ver, 0)\n",
    "        elif last_processed < min_avail_ver:\n",
    "            print(f\"      [FALLBACK] History truncated for {fqn.split('.')[-1]} (Key: {lookup_key})\")\n",
    "            return (\"FALLBACK\", last_ts)\n",
    "        else:\n",
    "            start_ver = last_processed + 1\n",
    "\n",
    "        if start_ver > curr_ver:\n",
    "            return (None, None)\n",
    "        return (start_ver, curr_ver)\n",
    "    except Exception as e:\n",
    "        print(f\"      [FALLBACK] CDF Check failed for {fqn.split('.')[-1]}: {str(e)[:80]}\")\n",
    "        try:\n",
    "            row = spark.sql(f\"SELECT last_timestamp FROM {CONTROL_TBL} WHERE table_name = '{lookup_key}'\").collect()\n",
    "            last_ts = row[0]['last_timestamp'] if row else None\n",
    "            return (\"FALLBACK\", last_ts)\n",
    "        except:\n",
    "            return (\"FALLBACK\", None)\n",
    "\n",
    "\n",
    "def update_checkpoint(fqn, version=None, timestamp=None, checkpoint_key=None):\n",
    "    \"\"\"Updates the control table.\"\"\"\n",
    "    if version is None and timestamp is None:\n",
    "        return\n",
    "\n",
    "    key_val = checkpoint_key if checkpoint_key else fqn\n",
    "    version_val = version if version is not None else \"NULL\"\n",
    "    ts_val = f\"timestamp('{timestamp}')\" if timestamp else \"current_timestamp()\"\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {CONTROL_TBL} t\n",
    "        USING (SELECT '{key_val}' as n, {version_val} as v, {ts_val} as ts) s\n",
    "        ON t.table_name = s.n\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "            last_version = COALESCE(s.v, t.last_version),\n",
    "            last_timestamp = s.ts,\n",
    "            updated_at = current_timestamp()\n",
    "        WHEN NOT MATCHED THEN INSERT (table_name, last_version, last_timestamp, updated_at)\n",
    "            VALUES (s.n, s.v, s.ts, current_timestamp())\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def get_fallback_filter(fqn: str, last_ts):\n",
    "    \"\"\"Get timestamp filter for fallback mode.\"\"\"\n",
    "    col = pick_time_col(fqn)\n",
    "    if col is None:\n",
    "        is_key_source = any(k[1] == fqn for k in KEY_LOOKUPS) or \\\n",
    "                        \"mill_clinical_event\" in fqn.lower() or \\\n",
    "                        \"mill_encounter\" in fqn.lower()\n",
    "        if is_key_source:\n",
    "            print(f\"      [WARNING] No time column for {fqn} but it is a Key Source. Defaulting to FULL SCAN.\")\n",
    "            return \"1=1\"\n",
    "\n",
    "        print(f\"      [WARNING] No time column for {fqn}. Skipping fallback scan for safety.\")\n",
    "        return \"1=0\"\n",
    "\n",
    "    if last_ts:\n",
    "        return f\"{col} > timestamp('{last_ts}')\"\n",
    "    else:\n",
    "        return f\"{col} >= current_timestamp() - INTERVAL {FALLBACK_LOOKBACK_HOURS} HOURS\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. PHASE 1: MAINTAIN THE HUB\n",
    "# ==============================================================================\n",
    "def refresh_hub():\n",
    "    \"\"\"Refresh mapping hub with Runtime Fallback logic and Checkpoint Isolation.\"\"\"\n",
    "    print(\"Phase 1: Refreshing Hub...\")\n",
    "    processed_keys = set()\n",
    "\n",
    "    # Clean up tracking table for current run\n",
    "    spark.sql(f\"DELETE FROM {CHANGED_ENC_TBL} WHERE run_date < current_date() - INTERVAL 1 DAY\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 1. SYNC ENCOUNTERS (Enc -> Org Map)\n",
    "    # ==========================================================================\n",
    "    enc_fqn = f\"{CATALOG}.{RAW_SCHEMA}.mill_encounter\"\n",
    "    hub_key = f\"{enc_fqn}_HUB\"\n",
    "\n",
    "    sv, ev = get_cdc_window(enc_fqn, checkpoint_key=hub_key)\n",
    "\n",
    "    if sv is not None:\n",
    "        print(f\"    [PROC] MILL_ENCOUNTER                : Syncing Hub (v{sv} to v{ev})\")\n",
    "\n",
    "        def run_enc_sync(is_fallback):\n",
    "            if is_fallback:\n",
    "                last_ts_row = spark.sql(f\"SELECT last_timestamp FROM {CONTROL_TBL} WHERE table_name = '{hub_key}'\").collect()\n",
    "                ts_val = last_ts_row[0]['last_timestamp'] if last_ts_row else None\n",
    "                time_filter = get_fallback_filter(enc_fqn, ts_val)\n",
    "                sql = f\"\"\"\n",
    "                    SELECT ENCNTR_ID, ORGANIZATION_ID, current_timestamp() as last_updated\n",
    "                    FROM {enc_fqn}\n",
    "                    WHERE {time_filter} AND ENCNTR_ID IS NOT NULL AND ORGANIZATION_ID IS NOT NULL\n",
    "                \"\"\"\n",
    "            else:\n",
    "                sql = f\"\"\"\n",
    "                    SELECT ENCNTR_ID, ORGANIZATION_ID, current_timestamp() as last_updated\n",
    "                    FROM table_changes('{enc_fqn}', {sv}, {ev})\n",
    "                    WHERE _change_type IN ('insert', 'update_postimage')\n",
    "                      AND ENCNTR_ID IS NOT NULL AND ORGANIZATION_ID IS NOT NULL\n",
    "                \"\"\"\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {ENC_ORG_TBL} tgt\n",
    "                USING (\n",
    "                    SELECT ENCNTR_ID, MAX(ORGANIZATION_ID) as ORGANIZATION_ID, MAX(last_updated) as last_updated\n",
    "                    FROM ({sql}) GROUP BY ENCNTR_ID\n",
    "                ) src\n",
    "                ON tgt.ENCNTR_ID = src.ENCNTR_ID\n",
    "                WHEN MATCHED THEN UPDATE SET\n",
    "                    ORGANIZATION_ID = src.ORGANIZATION_ID,\n",
    "                    last_updated = src.last_updated\n",
    "                WHEN NOT MATCHED THEN INSERT (ENCNTR_ID, ORGANIZATION_ID, last_updated)\n",
    "                    VALUES (src.ENCNTR_ID, src.ORGANIZATION_ID, src.last_updated)\n",
    "            \"\"\")\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                INSERT INTO {CHANGED_ENC_TBL}\n",
    "                SELECT DISTINCT c.ENCNTR_ID, c.ORGANIZATION_ID, tm.trust, current_timestamp(), current_date()\n",
    "                FROM ({sql}) c\n",
    "                LEFT JOIN {TRUST_MAP_TBL} tm ON c.ORGANIZATION_ID = tm.organization_id\n",
    "            \"\"\")\n",
    "\n",
    "        try:\n",
    "            if sv == \"FALLBACK\":\n",
    "                raise Exception(\"Force Fallback Mode\")\n",
    "            run_enc_sync(is_fallback=False)\n",
    "            update_checkpoint(enc_fqn, version=ev, checkpoint_key=hub_key)\n",
    "        except Exception as e:\n",
    "            print(f\"      [Runtime Fallback] Encounters Sync switched to Time-Window. Reason: {str(e)[:100]}\")\n",
    "            run_enc_sync(is_fallback=True)\n",
    "            update_checkpoint(enc_fqn, timestamp=datetime.now().isoformat(), checkpoint_key=hub_key)\n",
    "    else:\n",
    "        print(f\"    [SKIP] MILL_ENCOUNTER                : Hub up to date (Checked v{ev})\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. SYNC KEYS (Specific ID -> Enc Map)\n",
    "    # ==========================================================================\n",
    "    for key_col, source_fqn, enc_expr in KEY_LOOKUPS:\n",
    "        table_name = source_fqn.split('.')[-1].upper()\n",
    "        hub_key = f\"{source_fqn}_HUB\"\n",
    "\n",
    "        sv, ev = get_cdc_window(source_fqn, checkpoint_key=hub_key)\n",
    "\n",
    "        if sv is None:\n",
    "            print(f\"    [SKIP] {table_name:<29} : Hub up to date (Checked v{ev})\")\n",
    "            continue\n",
    "\n",
    "        processed_keys.add(key_col)\n",
    "        print(f\"    [PROC] {table_name:<29} : Syncing Key {key_col} (v{sv} to v{ev})\")\n",
    "\n",
    "        def run_key_sync(is_fallback):\n",
    "            if is_fallback:\n",
    "                last_ts_row = spark.sql(f\"SELECT last_timestamp FROM {CONTROL_TBL} WHERE table_name = '{hub_key}'\").collect()\n",
    "                ts_val = last_ts_row[0]['last_timestamp'] if last_ts_row else None\n",
    "                time_filter = get_fallback_filter(source_fqn, ts_val)\n",
    "                changes_view = f\"(SELECT * FROM {source_fqn} WHERE {time_filter})\"\n",
    "            else:\n",
    "                changes_view = f\"\"\"(SELECT * FROM table_changes('{source_fqn}', {sv}, {ev})\n",
    "                                   WHERE _change_type IN ('insert', 'update_postimage'))\"\"\"\n",
    "\n",
    "            if key_col == \"SURG_CASE_PROC_ID\":\n",
    "                key_enc_sql = f\"\"\"\n",
    "                    SELECT c.SURG_CASE_PROC_ID as key_id, sc.ENCNTR_ID\n",
    "                    FROM {changes_view} c\n",
    "                    JOIN {CATALOG}.{RAW_SCHEMA}.mill_surgical_case sc ON c.SURG_CASE_ID = sc.SURG_CASE_ID\n",
    "                    WHERE sc.ENCNTR_ID IS NOT NULL\n",
    "                \"\"\"\n",
    "            elif key_col == \"IM_STUDY_ID\":\n",
    "                im_cols = get_table_columns(source_fqn)\n",
    "                has_direct_enc = \"ENCNTR_ID\" in im_cols\n",
    "                enc_select = \"COALESCE(c.ENCNTR_ID, cv.ENCNTR_ID)\" if has_direct_enc else \"cv.ENCNTR_ID\"\n",
    "\n",
    "                key_enc_sql = f\"\"\"\n",
    "                    SELECT c.IM_STUDY_ID as key_id, {enc_select} as ENCNTR_ID\n",
    "                    FROM {changes_view} c\n",
    "                    LEFT JOIN {CATALOG}.{RAW_SCHEMA}.mill_cv_proc cv ON c.ORIG_ENTITY_ID = cv.CV_PROC_ID\n",
    "                    WHERE {enc_select} IS NOT NULL\n",
    "                \"\"\"\n",
    "            elif key_col == \"IM_ACQUIRED_STUDY_ID\":\n",
    "                im_fqn = f\"{CATALOG}.{RAW_SCHEMA}.mill_im_study\"\n",
    "                im_cols = get_table_columns(im_fqn)\n",
    "                has_direct_enc = \"ENCNTR_ID\" in im_cols\n",
    "                enc_select = \"COALESCE(st.ENCNTR_ID, cv.ENCNTR_ID)\" if has_direct_enc else \"cv.ENCNTR_ID\"\n",
    "\n",
    "                key_enc_sql = f\"\"\"\n",
    "                    SELECT acq.IM_ACQUIRED_STUDY_ID as key_id, {enc_select} as ENCNTR_ID\n",
    "                    FROM {changes_view} acq\n",
    "                    JOIN {im_fqn} st ON acq.MATCHED_STUDY_ID = st.IM_STUDY_ID\n",
    "                    LEFT JOIN {CATALOG}.{RAW_SCHEMA}.mill_cv_proc cv ON st.ORIG_ENTITY_ID = cv.CV_PROC_ID\n",
    "                    WHERE {enc_select} IS NOT NULL\n",
    "                \"\"\"\n",
    "            else:\n",
    "                key_enc_sql = f\"\"\"\n",
    "                    SELECT {key_col} as key_id, {enc_expr} as ENCNTR_ID\n",
    "                    FROM {changes_view}\n",
    "                    WHERE {enc_expr} IS NOT NULL\n",
    "                \"\"\"\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {HUB_TBL} tgt\n",
    "                USING (\n",
    "                    SELECT '{key_col}' as key_type, key_id, MAX(ENCNTR_ID) as ENCNTR_ID,\n",
    "                           current_timestamp() as last_updated\n",
    "                    FROM ({key_enc_sql}) GROUP BY key_id\n",
    "                ) src\n",
    "                ON tgt.key_type = src.key_type AND tgt.key_id = src.key_id\n",
    "                WHEN MATCHED THEN UPDATE SET\n",
    "                    ENCNTR_ID = src.ENCNTR_ID,\n",
    "                    last_updated = src.last_updated\n",
    "                WHEN NOT MATCHED THEN INSERT (key_type, key_id, ENCNTR_ID, last_updated)\n",
    "                    VALUES (src.key_type, src.key_id, src.ENCNTR_ID, src.last_updated)\n",
    "            \"\"\")\n",
    "\n",
    "        try:\n",
    "            if sv == \"FALLBACK\":\n",
    "                raise Exception(\"Force Fallback Mode\")\n",
    "            run_key_sync(is_fallback=False)\n",
    "            update_checkpoint(source_fqn, version=ev, checkpoint_key=hub_key)\n",
    "        except Exception as e:\n",
    "            print(f\"      [Runtime Fallback] Key Sync ({key_col}) switched to Time-Window. Reason: {str(e)[:100]}\")\n",
    "            run_key_sync(is_fallback=True)\n",
    "            update_checkpoint(source_fqn, timestamp=datetime.now().isoformat(), checkpoint_key=hub_key)\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. CLINICAL EVENTS (Event -> Enc Map)\n",
    "    # ==========================================================================\n",
    "    ce_fqn = f\"{CATALOG}.{RAW_SCHEMA}.mill_clinical_event\"\n",
    "    hub_key = f\"{ce_fqn}_HUB\"\n",
    "\n",
    "    sv, ev = get_cdc_window(ce_fqn, checkpoint_key=hub_key)\n",
    "\n",
    "    if sv is not None:\n",
    "        print(f\"    [PROC] MILL_CLINICAL_EVENT           : Syncing Events (v{sv} to v{ev})\")\n",
    "\n",
    "        def run_ce_sync(is_fallback):\n",
    "            if is_fallback:\n",
    "                last_ts_row = spark.sql(f\"SELECT last_timestamp FROM {CONTROL_TBL} WHERE table_name = '{hub_key}'\").collect()\n",
    "                ts_val = last_ts_row[0]['last_timestamp'] if last_ts_row else None\n",
    "                time_filter = get_fallback_filter(ce_fqn, ts_val)\n",
    "                sql = f\"\"\"\n",
    "                    SELECT EVENT_ID as key_id, ENCNTR_ID\n",
    "                    FROM {ce_fqn}\n",
    "                    WHERE {time_filter}\n",
    "                      AND EVENT_ID IS NOT NULL AND ENCNTR_ID IS NOT NULL\n",
    "                      AND (VALID_UNTIL_DT_TM IS NULL OR VALID_UNTIL_DT_TM > current_timestamp())\n",
    "                \"\"\"\n",
    "            else:\n",
    "                sql = f\"\"\"\n",
    "                    SELECT EVENT_ID as key_id, ENCNTR_ID\n",
    "                    FROM table_changes('{ce_fqn}', {sv}, {ev})\n",
    "                    WHERE _change_type IN ('insert', 'update_postimage')\n",
    "                      AND EVENT_ID IS NOT NULL AND ENCNTR_ID IS NOT NULL\n",
    "                      AND (VALID_UNTIL_DT_TM IS NULL OR VALID_UNTIL_DT_TM > current_timestamp())\n",
    "                \"\"\"\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {HUB_TBL} tgt\n",
    "                USING (\n",
    "                    SELECT 'EVENT_ID' as key_type, key_id, MAX(ENCNTR_ID) as ENCNTR_ID,\n",
    "                           current_timestamp() as last_updated\n",
    "                    FROM ({sql}) GROUP BY key_id\n",
    "                ) src\n",
    "                ON tgt.key_type = src.key_type AND tgt.key_id = src.key_id\n",
    "                WHEN MATCHED THEN UPDATE SET\n",
    "                    ENCNTR_ID = src.ENCNTR_ID,\n",
    "                    last_updated = src.last_updated\n",
    "                WHEN NOT MATCHED THEN INSERT (key_type, key_id, ENCNTR_ID, last_updated)\n",
    "                    VALUES (src.key_type, src.key_id, src.ENCNTR_ID, src.last_updated)\n",
    "            \"\"\")\n",
    "\n",
    "        try:\n",
    "            if sv == \"FALLBACK\":\n",
    "                raise Exception(\"Force Fallback Mode\")\n",
    "            run_ce_sync(is_fallback=False)\n",
    "            update_checkpoint(ce_fqn, version=ev, checkpoint_key=hub_key)\n",
    "            processed_keys.add('EVENT_ID')\n",
    "        except Exception as e:\n",
    "            print(f\"      [Runtime Fallback] Event Sync switched to Time-Window. Reason: {str(e)[:100]}\")\n",
    "            run_ce_sync(is_fallback=True)\n",
    "            update_checkpoint(ce_fqn, timestamp=datetime.now().isoformat(), checkpoint_key=hub_key)\n",
    "            processed_keys.add('EVENT_ID')\n",
    "    else:\n",
    "        print(f\"    [SKIP] MILL_CLINICAL_EVENT           : Hub up to date (Checked v{ev})\")\n",
    "\n",
    "    # Cleanup Old Cache\n",
    "    spark.sql(f\"DELETE FROM {HUB_TBL} WHERE last_updated < current_timestamp() - INTERVAL {LOOKUP_RETENTION_DAYS} DAYS\")\n",
    "    spark.sql(f\"DELETE FROM {ENC_ORG_TBL} WHERE last_updated < current_timestamp() - INTERVAL {LOOKUP_RETENTION_DAYS} DAYS\")\n",
    "\n",
    "    return processed_keys\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. PHASE 2: UPDATE TARGETS WITH JOIN-BASED RESOLUTION (OPTIMIZED)\n",
    "# ==============================================================================\n",
    "def build_trust_resolution_sql_v2(table_name: str, cols: set, pk_col: str, changes_sql: str):\n",
    "    \"\"\"\n",
    "    Build trust resolution SQL using explicit JOINs instead of correlated subqueries.\n",
    "    This is significantly more efficient for tables with multiple key columns.\n",
    "    \"\"\"\n",
    "    # Identify which keys are present in this table\n",
    "    key_cols_present = []\n",
    "    if \"ENCNTR_ID\" in cols:\n",
    "        key_cols_present.append((\"ENCNTR_ID\", \"direct\"))\n",
    "    if \"EVENT_ID\" in cols:\n",
    "        key_cols_present.append((\"EVENT_ID\", \"hub\"))\n",
    "    for key_col, _, _ in KEY_LOOKUPS:\n",
    "        if key_col in cols:\n",
    "            key_cols_present.append((key_col, \"hub\"))\n",
    "\n",
    "    has_org = \"ORGANIZATION_ID\" in cols\n",
    "\n",
    "    # If no way to link to encounter and no Org ID, skip\n",
    "    if not key_cols_present and not has_org:\n",
    "        return None\n",
    "\n",
    "    # Build JOIN clauses and COALESCE parts\n",
    "    join_clauses = []\n",
    "    enc_coalesce_parts = []\n",
    "    join_idx = 0\n",
    "\n",
    "    for key_col, key_type in key_cols_present:\n",
    "        if key_type == \"direct\":\n",
    "            enc_coalesce_parts.append(\"src.ENCNTR_ID\")\n",
    "        else:\n",
    "            alias = f\"h{join_idx}\"\n",
    "            join_clauses.append(f\"\"\"\n",
    "                LEFT JOIN hub_deduped {alias}\n",
    "                    ON {alias}.key_type = '{key_col}' AND {alias}.key_id = src.{key_col}\n",
    "            \"\"\")\n",
    "            enc_coalesce_parts.append(f\"{alias}.ENCNTR_ID\")\n",
    "            join_idx += 1\n",
    "\n",
    "    enc_coalesce = f\"COALESCE({', '.join(enc_coalesce_parts)})\" if enc_coalesce_parts else \"CAST(NULL AS BIGINT)\"\n",
    "    joins_sql = \"\\n\".join(join_clauses)\n",
    "\n",
    "    # Build org resolution for the final CTE\n",
    "    org_parts = []\n",
    "    if has_org:\n",
    "        # CORRECTION HERE: \n",
    "        # Refer to the column aliased in the 'resolved' CTE ('r'), not the 'changes' CTE ('src')\n",
    "        org_parts.append(\"r.direct_org\") \n",
    "    if enc_coalesce_parts:\n",
    "        org_parts.append(\"etl.ORGANIZATION_ID\")\n",
    "\n",
    "    org_coalesce = f\"COALESCE({', '.join(org_parts)})\" if org_parts else \"CAST(NULL AS BIGINT)\"\n",
    "\n",
    "    return f\"\"\"\n",
    "        WITH changes AS ({changes_sql}),\n",
    "        resolved AS (\n",
    "            SELECT\n",
    "                src.{pk_col},\n",
    "                {enc_coalesce} as resolved_enc,\n",
    "                {\"src.ORGANIZATION_ID\" if has_org else \"CAST(NULL AS BIGINT)\"} as direct_org\n",
    "            FROM changes src\n",
    "            {joins_sql}\n",
    "        ),\n",
    "        with_trust AS (\n",
    "            SELECT\n",
    "                r.{pk_col},\n",
    "                COALESCE(r.resolved_enc, etl.ENCNTR_ID) as new_enc_id,\n",
    "                {org_coalesce} as new_org_id,\n",
    "                COALESCE(tm_direct.trust, etl.trust) as new_trust\n",
    "            FROM resolved r\n",
    "            LEFT JOIN enc_trust_lookup etl ON r.resolved_enc = etl.ENCNTR_ID\n",
    "            LEFT JOIN trust_map tm_direct ON r.direct_org = tm_direct.organization_id\n",
    "            WHERE COALESCE(tm_direct.trust, etl.trust) IS NOT NULL\n",
    "        )\n",
    "        SELECT {pk_col}, new_enc_id, new_org_id, new_trust FROM with_trust\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def audit_changes_aggregated(table_name: str, pk_col: str, trust_counts: dict):\n",
    "    \"\"\"\n",
    "    Log aggregated audit summary instead of individual rows.\n",
    "    This dramatically reduces audit table overhead.\n",
    "    \"\"\"\n",
    "    return\n",
    "    try:\n",
    "        for trust, stats in trust_counts.items():\n",
    "            # FIX: Convert Python None to SQL NULL string to prevent \"UNRESOLVED_COLUMN\" error\n",
    "            min_pk_val = stats['min_pk'] if stats['min_pk'] is not None else \"NULL\"\n",
    "            max_pk_val = stats['max_pk'] if stats['max_pk'] is not None else \"NULL\"\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                INSERT INTO {AUDIT_SUMMARY_TBL}\n",
    "                VALUES (\n",
    "                    timestamp('{RUN_TS.isoformat()}'),\n",
    "                    '{table_name}',\n",
    "                    '{trust}',\n",
    "                    'UPDATE',\n",
    "                    {stats['count']},\n",
    "                    {min_pk_val},\n",
    "                    {max_pk_val}\n",
    "                )\n",
    "            \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"      [Audit Warning] Could not log summary for {table_name}: {e}\")\n",
    "\n",
    "\n",
    "def execute_merge_deduped_v2(fqn: str, source_sql: str, pk_col: str, can_delete: bool, cols: set, processed_pks: set = None):\n",
    "    \"\"\"\n",
    "    Execute MERGE with pre-deduplication and aggregated audit logging.\n",
    "    Returns set of processed PKs for deduplication in Phase 2c.\n",
    "    \"\"\"\n",
    "    table_upper = fqn.split('.')[-1].upper()\n",
    "    has_enc = \"ENCNTR_ID\" in cols\n",
    "    has_org = \"ORGANIZATION_ID\" in cols\n",
    "    has_adc = \"ADC_UPDT\" in cols\n",
    "\n",
    "    # LINEAGE GUARD\n",
    "    safe_to_update_enc = has_enc and table_upper not in SOURCE_OF_TRUTH_TABLES\n",
    "    safe_to_update_org = has_org and table_upper != \"MILL_ENCOUNTER\"\n",
    "\n",
    "    set_parts = [\"tgt.Trust = src.new_trust\"]\n",
    "    if safe_to_update_enc:\n",
    "        set_parts.append(\"tgt.ENCNTR_ID = COALESCE(tgt.ENCNTR_ID, src.new_enc_id)\")\n",
    "    if safe_to_update_org:\n",
    "        set_parts.append(\"tgt.ORGANIZATION_ID = COALESCE(tgt.ORGANIZATION_ID, src.new_org_id)\")\n",
    "    if has_adc:\n",
    "        set_parts.append(\"tgt.ADC_UPDT = current_timestamp()\")\n",
    "\n",
    "    delete_clause = \"\"\n",
    "    if FILTER_BHRUT and can_delete:\n",
    "        delete_clause = \"WHEN MATCHED AND src.new_trust = 'BHRUT' THEN DELETE\"\n",
    "\n",
    "    deduped_source = f\"\"\"\n",
    "        SELECT {pk_col}, MAX(new_enc_id) as new_enc_id, MAX(new_org_id) as new_org_id,\n",
    "               MAX(new_trust) as new_trust\n",
    "        FROM ({source_sql}) GROUP BY {pk_col}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the merge\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {fqn} tgt\n",
    "        USING ({deduped_source}) src\n",
    "        ON tgt.{pk_col} = src.{pk_col}\n",
    "        {delete_clause}\n",
    "        WHEN MATCHED AND (tgt.Trust IS NULL OR tgt.Trust = '' OR tgt.Trust != src.new_trust)\n",
    "        THEN UPDATE SET {\", \".join(set_parts)}\n",
    "    \"\"\")\n",
    "\n",
    "    return set()\n",
    "\n",
    "\n",
    "def determine_pk_column(table_name: str, cols: set) -> str:\n",
    "    table_upper = table_name.upper()\n",
    "    pk_patterns = {\n",
    "        \"MILL_ENCOUNTER\": \"ENCNTR_ID\",\n",
    "        \"MILL_CLINICAL_EVENT\": \"EVENT_ID\",\n",
    "        \"MILL_ORDERS\": \"ORDER_ID\",\n",
    "        \"MILL_PROBLEM\": \"PROBLEM_ID\",\n",
    "        \"MILL_SURGICAL_CASE\": \"SURG_CASE_ID\",\n",
    "        \"MILL_SURG_CASE_PROCEDURE\": \"SURG_CASE_PROC_ID\",\n",
    "        \"MILL_DCP_FORMS_ACTIVITY\": \"DCP_FORMS_ACTIVITY_ID\",\n",
    "        \"MILL_EPISODE_ENCNTR_RELTN\": \"EPISODE_ID\",\n",
    "        \"MILL_SCH_EVENT_PATIENT\": \"SCH_EVENT_ID\",\n",
    "        \"MILL_SCH_SCHEDULE\": \"SCHEDULE_ID\",\n",
    "        \"MILL_IM_STUDY\": \"IM_STUDY_ID\",\n",
    "        \"MILL_IM_ACQUIRED_STUDY\": \"IM_ACQUIRED_STUDY_ID\",\n",
    "        \"MILL_CV_PROC\": \"CV_PROC_ID\",\n",
    "    }\n",
    "    if table_upper in pk_patterns and pk_patterns[table_upper] in cols:\n",
    "        return pk_patterns[table_upper]\n",
    "    for pk_candidate in [\"EVENT_ID\", \"ENCNTR_ID\", \"ORDER_ID\", \"PROBLEM_ID\"]:\n",
    "        if pk_candidate in cols:\n",
    "            return pk_candidate\n",
    "    for key_col, _, _ in KEY_LOOKUPS:\n",
    "        if key_col in cols:\n",
    "            return key_col\n",
    "    return None\n",
    "\n",
    "\n",
    "def force_refresh_new_orgs(new_orgs):\n",
    "    if not new_orgs:\n",
    "        return\n",
    "    print(f\"    Forcing refresh for {len(new_orgs)} newly mapped organizations...\")\n",
    "    org_list = \",\".join(str(x) for x in new_orgs)\n",
    "    enc_fqn = f\"{CATALOG}.{RAW_SCHEMA}.mill_encounter\"\n",
    "    cols = get_table_columns(enc_fqn)\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT e.ENCNTR_ID, e.ENCNTR_ID as new_enc_id, e.ORGANIZATION_ID as new_org_id, tm.trust as new_trust\n",
    "        FROM {enc_fqn} e\n",
    "        JOIN {TRUST_MAP_TBL} tm ON e.ORGANIZATION_ID = tm.organization_id\n",
    "        WHERE e.ORGANIZATION_ID IN ({org_list})\n",
    "          AND (e.Trust IS NULL OR e.Trust = '' OR e.Trust != tm.trust)\n",
    "    \"\"\"\n",
    "    can_delete = can_delete_bhrut(\"MILL_ENCOUNTER\")\n",
    "    execute_merge_deduped_v2(enc_fqn, sql, \"ENCNTR_ID\", can_delete, cols)\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {CHANGED_ENC_TBL}\n",
    "        SELECT DISTINCT e.ENCNTR_ID, e.ORGANIZATION_ID, tm.trust, current_timestamp(), current_date()\n",
    "        FROM {enc_fqn} e\n",
    "        JOIN {TRUST_MAP_TBL} tm ON e.ORGANIZATION_ID = tm.organization_id\n",
    "        WHERE e.ORGANIZATION_ID IN ({org_list})\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def update_targets(cdc_tables, new_orgs):\n",
    "    \"\"\"\n",
    "    Update target tables based on their own CDC changes.\n",
    "    Returns dict of {table_name: set(processed_pks)} for Phase 2c deduplication.\n",
    "    \"\"\"\n",
    "    print(\"Phase 2: Updating Target Tables (Direct CDC)...\")\n",
    "    force_refresh_new_orgs(new_orgs)\n",
    "\n",
    "    processed_tables = {}\n",
    "\n",
    "    for table_name, cols in cdc_tables.items():\n",
    "        fqn = f\"{CATALOG}.{RAW_SCHEMA}.{table_name}\"\n",
    "\n",
    "        sv, ev = get_cdc_window(fqn)\n",
    "\n",
    "        if sv is None:\n",
    "            log_status(table_name, \"SKIP\", f\"Up to date (Checked v{ev})\")\n",
    "            continue\n",
    "\n",
    "        pk_col = determine_pk_column(table_name, cols)\n",
    "        if not pk_col:\n",
    "            log_status(table_name, \"SKIP\", \"Could not determine Primary Key\")\n",
    "            continue\n",
    "\n",
    "        log_status(table_name, \"PROC\", f\"Processing updates (v{sv} to v{ev})\")\n",
    "\n",
    "        def run_update(is_fallback):\n",
    "            if is_fallback:\n",
    "                last_ts = spark.sql(f\"SELECT last_timestamp FROM {CONTROL_TBL} WHERE table_name = '{fqn}'\").collect()\n",
    "                ts_val = last_ts[0]['last_timestamp'] if last_ts else None\n",
    "                time_filter = get_fallback_filter(fqn, ts_val)\n",
    "                changes_sql = f\"SELECT * FROM {fqn} WHERE {time_filter}\"\n",
    "            else:\n",
    "                changes_sql = f\"\"\"\n",
    "                    SELECT * FROM table_changes('{fqn}', {sv}, {ev})\n",
    "                    WHERE _change_type IN ('insert', 'update_postimage')\n",
    "                \"\"\"\n",
    "\n",
    "            resolution_sql = build_trust_resolution_sql_v2(table_name, cols, pk_col, changes_sql)\n",
    "            if resolution_sql:\n",
    "                can_delete = can_delete_bhrut(table_name.upper())\n",
    "                pks = execute_merge_deduped_v2(fqn, resolution_sql, pk_col, can_delete, cols, processed_pks=set())\n",
    "                return pks\n",
    "            else:\n",
    "                print(f\"      [Info] No resolution logic available (Missing link columns)\")\n",
    "                return set()\n",
    "\n",
    "        try:\n",
    "            if sv == \"FALLBACK\":\n",
    "                raise Exception(\"Force Fallback Mode\")\n",
    "            pks = run_update(is_fallback=False)\n",
    "            processed_tables[table_name] = pks\n",
    "            update_checkpoint(fqn, version=ev)\n",
    "        except Exception as e:\n",
    "            print(f\"      [Runtime Fallback] {table_name} Update switched to Time-Window. Reason: {str(e)[:200]}\")\n",
    "            pks = run_update(is_fallback=True)\n",
    "            processed_tables[table_name] = pks\n",
    "            update_checkpoint(fqn, timestamp=datetime.now().isoformat())\n",
    "\n",
    "    return processed_tables\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. PHASE 2b: CHAINED DEPENDENCIES\n",
    "# ==============================================================================\n",
    "def propagate_chained_updates(processed_keys, all_targets):\n",
    "    \"\"\"\n",
    "    Propagate updates through chained dependencies.\n",
    "    Note: Updated to use all_targets since child tables may not have TRUST column\n",
    "    but still need updates propagated.\n",
    "    \"\"\"\n",
    "    print(\"Phase 2b: Propagating Chained Updates...\")\n",
    "\n",
    "    for parent_key, (child_table, join_col, child_pk) in CHAINED_DEPENDENCIES.items():\n",
    "        if parent_key not in processed_keys:\n",
    "            continue\n",
    "\n",
    "        # Check if child table exists in raw schema (it may not have TRUST column)\n",
    "        child_fqn = f\"{CATALOG}.{RAW_SCHEMA}.{child_table}\"\n",
    "        cols = get_table_columns(child_fqn)\n",
    "\n",
    "        if \"TRUST\" not in cols:\n",
    "            print(f\"    Skipping {child_table} - no TRUST column\")\n",
    "            continue\n",
    "\n",
    "        print(f\"    Triggering updates for {child_table} via {parent_key}\")\n",
    "\n",
    "        source_sql = f\"\"\"\n",
    "            SELECT child.{child_pk}, h.ENCNTR_ID as new_enc_id,\n",
    "                   eo.ORGANIZATION_ID as new_org_id, tm.trust as new_trust\n",
    "            FROM {child_fqn} child\n",
    "            JOIN hub_deduped h ON h.key_type = '{parent_key}' AND child.{join_col} = h.key_id\n",
    "            JOIN enc_org_deduped eo ON h.ENCNTR_ID = eo.ENCNTR_ID\n",
    "            JOIN trust_map tm ON eo.ORGANIZATION_ID = tm.organization_id\n",
    "            WHERE child.Trust IS NULL OR child.Trust = '' OR child.Trust != tm.trust\n",
    "        \"\"\"\n",
    "        try:\n",
    "            can_delete = can_delete_bhrut(child_table.upper())\n",
    "            execute_merge_deduped_v2(child_fqn, source_sql, child_pk, can_delete, cols)\n",
    "        except Exception as e:\n",
    "            print(f\"      Error: {str(e)[:150]}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. PHASE 2c: REVERSE PROPAGATION WITH VERSION-BASED SKIP OPTIMIZATION\n",
    "# ==============================================================================\n",
    "def propagate_encounter_changes(all_targets, processed_tables):\n",
    "    \"\"\"\n",
    "    Propagate encounter changes to downstream tables.\n",
    "    \n",
    "    NEW: Uses version tracking to skip tables where:\n",
    "    1. Our pipeline was the last to update the table (version unchanged)\n",
    "    2. The hub/enc_org mappings haven't changed since our last update\n",
    "    \"\"\"\n",
    "    print(\"Phase 2c: Propagating Encounter Changes to Downstream Tables...\")\n",
    "\n",
    "    try:\n",
    "        changed_count = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(DISTINCT ENCNTR_ID) as cnt\n",
    "            FROM {CHANGED_ENC_TBL}\n",
    "            WHERE run_date = current_date()\n",
    "        \"\"\").collect()[0]['cnt']\n",
    "    except:\n",
    "        changed_count = 0\n",
    "\n",
    "    if changed_count == 0:\n",
    "        print(\"    No encounter changes to propagate\")\n",
    "        return\n",
    "\n",
    "    print(f\"    Found {changed_count} changed encounters. Propagating to {len(all_targets)} potential tables.\")\n",
    "\n",
    "    # Get current hub versions for skip optimization\n",
    "    current_hub_ver, current_enc_org_ver = get_hub_versions()\n",
    "    print(f\"    Current versions - Hub: v{current_hub_ver}, Enc-Org: v{current_enc_org_ver}\")\n",
    "\n",
    "    # Track statistics for skip optimization\n",
    "    skipped_count = 0\n",
    "    processed_count = 0\n",
    "\n",
    "    # Create a BROADCASTED view of changed encounters for efficient joins\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW changed_enc_broadcast AS\n",
    "        SELECT /*+ BROADCAST(ce) */\n",
    "            ce.ENCNTR_ID, ce.ORGANIZATION_ID, ce.trust as new_trust\n",
    "        FROM {CHANGED_ENC_TBL} ce\n",
    "        WHERE ce.run_date = current_date() AND ce.trust IS NOT NULL\n",
    "    \"\"\")\n",
    "\n",
    "    # Alias for consistency in loop\n",
    "    spark.sql(\"CREATE OR REPLACE TEMP VIEW changed_enc_cached AS SELECT DISTINCT ENCNTR_ID, ORGANIZATION_ID, new_trust FROM changed_enc_broadcast\")\n",
    "\n",
    "    for table_name, cols in all_targets.items():\n",
    "        if table_name == \"mill_encounter\":\n",
    "            continue\n",
    "\n",
    "        fqn = f\"{CATALOG}.{RAW_SCHEMA}.{table_name}\"\n",
    "        pk_col = determine_pk_column(table_name, cols)\n",
    "        if not pk_col:\n",
    "            continue\n",
    "\n",
    "        # =====================================================================\n",
    "        # NEW: Version-based skip optimization\n",
    "        # =====================================================================\n",
    "        should_skip, skip_reason = should_skip_phase2c(\n",
    "            table_name, fqn, current_hub_ver, current_enc_org_ver\n",
    "        )\n",
    "        \n",
    "        if should_skip:\n",
    "            log_status(table_name, \"SKIP\", f\"Version check: {skip_reason}\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        # =====================================================================\n",
    "\n",
    "        # Get PKs already processed in Phase 2 for this table\n",
    "        already_processed = processed_tables.get(table_name, set())\n",
    "\n",
    "        # Track if we actually updated anything (for version tracking)\n",
    "        table_was_updated = False\n",
    "\n",
    "        # Direct Link via ENCNTR_ID\n",
    "        if \"ENCNTR_ID\" in cols:\n",
    "            print(f\"      Propagating to {table_name} (via ENCNTR_ID) - {skip_reason if not should_skip else ''}\")\n",
    "            try:\n",
    "                # Build exclusion clause if we have processed PKs\n",
    "                exclusion_clause = \"\"\n",
    "                if already_processed and len(already_processed) < 10000:\n",
    "                    # Only use IN clause for reasonable sizes\n",
    "                    pk_list = \",\".join(str(pk) for pk in already_processed)\n",
    "                    exclusion_clause = f\"AND t.{pk_col} NOT IN ({pk_list})\"\n",
    "                elif already_processed:\n",
    "                    # For large sets, create a temp table\n",
    "                    spark.createDataFrame(\n",
    "                        [(pk,) for pk in already_processed],\n",
    "                        [pk_col]\n",
    "                    ).createOrReplaceTempView(f\"processed_{table_name}\")\n",
    "                    exclusion_clause = f\"AND t.{pk_col} NOT IN (SELECT {pk_col} FROM processed_{table_name})\"\n",
    "\n",
    "                source_sql = f\"\"\"\n",
    "                    SELECT /*+ BROADCAST(ce) */\n",
    "                        t.{pk_col}, ce.ENCNTR_ID as new_enc_id,\n",
    "                        ce.ORGANIZATION_ID as new_org_id, ce.new_trust\n",
    "                    FROM {fqn} t\n",
    "                    JOIN changed_enc_cached ce ON t.ENCNTR_ID = ce.ENCNTR_ID\n",
    "                    WHERE (t.Trust IS NULL OR t.Trust = '' OR t.Trust != ce.new_trust)\n",
    "                    {exclusion_clause}\n",
    "                \"\"\"\n",
    "                can_delete = can_delete_bhrut(table_name.upper())\n",
    "                execute_merge_deduped_v2(fqn, source_sql, pk_col, can_delete, cols)\n",
    "                table_was_updated = True\n",
    "            except Exception as e:\n",
    "                print(f\"        Error: {str(e)[:150]}\")\n",
    "\n",
    "        # Key Link (if not direct ENCNTR_ID)\n",
    "        else:\n",
    "            key_cols_present = [k for k, _, _ in KEY_LOOKUPS if k in cols]\n",
    "            if \"EVENT_ID\" in cols:\n",
    "                key_cols_present.append(\"EVENT_ID\")\n",
    "\n",
    "            if not key_cols_present:\n",
    "                continue\n",
    "\n",
    "            print(f\"      Propagating to {table_name} (via {', '.join(key_cols_present)})\")\n",
    "            try:\n",
    "                # Build JOIN-based encounter resolution\n",
    "                join_clauses = []\n",
    "                enc_parts = []\n",
    "                for idx, k in enumerate(key_cols_present):\n",
    "                    alias = f\"hk{idx}\"\n",
    "                    join_clauses.append(f\"\"\"\n",
    "                        LEFT JOIN hub_deduped {alias}\n",
    "                            ON {alias}.key_type = '{k}' AND {alias}.key_id = t.{k}\n",
    "                    \"\"\")\n",
    "                    enc_parts.append(f\"{alias}.ENCNTR_ID\")\n",
    "\n",
    "                enc_coalesce = f\"COALESCE({', '.join(enc_parts)})\"\n",
    "                joins_sql = \"\\n\".join(join_clauses)\n",
    "\n",
    "                # Build exclusion for already processed\n",
    "                exclusion_clause = \"\"\n",
    "                if already_processed and len(already_processed) < 10000:\n",
    "                    pk_list = \",\".join(str(pk) for pk in already_processed)\n",
    "                    exclusion_clause = f\"AND t.{pk_col} NOT IN ({pk_list})\"\n",
    "\n",
    "                source_sql = f\"\"\"\n",
    "                    WITH resolved AS (\n",
    "                        SELECT t.{pk_col}, {enc_coalesce} as resolved_enc\n",
    "                        FROM {fqn} t\n",
    "                        {joins_sql}\n",
    "                        WHERE (t.Trust IS NULL OR t.Trust = '' OR t.Trust NOT IN ('Barts', 'BHRUT'))\n",
    "                        {exclusion_clause}\n",
    "                    )\n",
    "                    SELECT /*+ BROADCAST(ce) */\n",
    "                        r.{pk_col}, ce.ENCNTR_ID as new_enc_id,\n",
    "                        ce.ORGANIZATION_ID as new_org_id, ce.new_trust\n",
    "                    FROM resolved r\n",
    "                    JOIN changed_enc_cached ce ON r.resolved_enc = ce.ENCNTR_ID\n",
    "                \"\"\"\n",
    "                can_delete = can_delete_bhrut(table_name.upper())\n",
    "                execute_merge_deduped_v2(fqn, source_sql, pk_col, can_delete, cols)\n",
    "                table_was_updated = True\n",
    "            except Exception as e:\n",
    "                print(f\"        Error: {str(e)[:150]}\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # NEW: Update version tracker after processing\n",
    "        # =====================================================================\n",
    "        if table_was_updated:\n",
    "            try:\n",
    "                update_phase2c_tracker(table_name, fqn, current_hub_ver, current_enc_org_ver)\n",
    "            except Exception as e:\n",
    "                print(f\"        [Tracker Warning] Could not update version tracker: {str(e)[:50]}\")\n",
    "        \n",
    "        processed_count += 1\n",
    "\n",
    "    print(f\"    Phase 2c Summary: {processed_count} tables processed, {skipped_count} tables skipped (version unchanged)\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. LOGGING & ALERTS\n",
    "# ==============================================================================\n",
    "def flag_unknown_organizations():\n",
    "    print(\"Phase 3: Flagging Unknown Organizations...\")\n",
    "    try:\n",
    "        enc_fqn = f\"{CATALOG}.{RAW_SCHEMA}.mill_encounter\"\n",
    "        time_filter = get_fallback_filter(enc_fqn, None)\n",
    "        if time_filter == \"1=0\":\n",
    "            time_filter = \"1=1\"\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO {FLAG_TBL}\n",
    "            SELECT DISTINCT\n",
    "                e.ORGANIZATION_ID,\n",
    "                1 AS alert,\n",
    "                current_timestamp() AS event_time,\n",
    "                'MILL_ENCOUNTER' AS table_name,\n",
    "                current_timestamp() AS first_seen_timestamp\n",
    "            FROM {enc_fqn} e\n",
    "            WHERE {time_filter}\n",
    "              AND e.ORGANIZATION_ID IS NOT NULL\n",
    "              AND NOT EXISTS (SELECT 1 FROM {TRUST_MAP_TBL} m WHERE m.organization_id = e.ORGANIZATION_ID)\n",
    "              AND NOT EXISTS (\n",
    "                  SELECT 1 FROM {FLAG_TBL} f\n",
    "                  WHERE f.organization_id = e.ORGANIZATION_ID\n",
    "                    AND f.event_time >= current_timestamp() - INTERVAL 7 DAYS\n",
    "              )\n",
    "        \"\"\")\n",
    "        flagged = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(DISTINCT organization_id) as cnt\n",
    "            FROM {FLAG_TBL}\n",
    "            WHERE event_time >= current_timestamp() - INTERVAL 1 DAY\n",
    "        \"\"\").collect()[0]['cnt']\n",
    "        if flagged > 0:\n",
    "            print(f\"    Flagged {flagged} unknown organization IDs\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Org Flagging failed: {str(e)[:150]}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. WEEKLY DEEP CLEAN (OPTIMIZED)\n",
    "# ==============================================================================\n",
    "def run_weekly_clean(all_targets):\n",
    "    \"\"\"Run comprehensive cleanup using JOIN-based trust resolution.\"\"\"\n",
    "    print(\"Phase 4: Running Weekly Deep Clean...\")\n",
    "\n",
    "    def build_join_clauses(cols, table_alias):\n",
    "        \"\"\"Build JOIN clauses for a given table alias.\"\"\"\n",
    "        join_clauses = []\n",
    "        trust_parts = []\n",
    "\n",
    "        if \"ORGANIZATION_ID\" in cols:\n",
    "            join_clauses.append(f\"LEFT JOIN trust_map tm_direct ON {table_alias}.ORGANIZATION_ID = tm_direct.organization_id\")\n",
    "            trust_parts.append(\"tm_direct.trust\")\n",
    "\n",
    "        if \"ENCNTR_ID\" in cols:\n",
    "            join_clauses.append(f\"LEFT JOIN enc_trust_lookup etl_direct ON {table_alias}.ENCNTR_ID = etl_direct.ENCNTR_ID\")\n",
    "            trust_parts.append(\"etl_direct.trust\")\n",
    "\n",
    "        if \"EVENT_ID\" in cols:\n",
    "            join_clauses.append(f\"\"\"\n",
    "                LEFT JOIN hub_deduped h_event ON h_event.key_type = 'EVENT_ID' AND h_event.key_id = {table_alias}.EVENT_ID\n",
    "                LEFT JOIN enc_trust_lookup etl_event ON h_event.ENCNTR_ID = etl_event.ENCNTR_ID\n",
    "            \"\"\")\n",
    "            trust_parts.append(\"etl_event.trust\")\n",
    "\n",
    "        for k, _, _ in KEY_LOOKUPS:\n",
    "            if k in cols:\n",
    "                safe_k = k.lower()\n",
    "                join_clauses.append(f\"\"\"\n",
    "                    LEFT JOIN hub_deduped h_{safe_k} ON h_{safe_k}.key_type = '{k}' AND h_{safe_k}.key_id = {table_alias}.{k}\n",
    "                    LEFT JOIN enc_trust_lookup etl_{safe_k} ON h_{safe_k}.ENCNTR_ID = etl_{safe_k}.ENCNTR_ID\n",
    "                \"\"\")\n",
    "                trust_parts.append(f\"etl_{safe_k}.trust\")\n",
    "\n",
    "        return join_clauses, trust_parts\n",
    "\n",
    "    for table_name, cols in all_targets.items():\n",
    "        fqn = f\"{CATALOG}.{RAW_SCHEMA}.{table_name}\"\n",
    "        pk_col = determine_pk_column(table_name, cols)\n",
    "        \n",
    "        if not pk_col:\n",
    "            print(f\"    Skipping {table_name} - no PK column found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"    Deep cleaning {table_name}...\")\n",
    "\n",
    "        try:\n",
    "            # Build JOIN clauses using 't' as alias for main queries\n",
    "            join_clauses, trust_parts = build_join_clauses(cols, \"t\")\n",
    "\n",
    "            if not trust_parts:\n",
    "                continue\n",
    "\n",
    "            trust_coalesce = f\"COALESCE({', '.join(trust_parts)})\"\n",
    "            joins_sql = \"\\n\".join(join_clauses)\n",
    "\n",
    "            has_adc = \"ADC_UPDT\" in cols\n",
    "            time_filter = \"t.ADC_UPDT >= current_date() - INTERVAL 90 DAYS\" if has_adc else \"1=1\"\n",
    "            adc_update = \", tgt.ADC_UPDT = current_timestamp()\" if has_adc else \"\"\n",
    "\n",
    "            # 1. BHRUT Deletion Logic - use a temp view + IN clause approach\n",
    "            #    This avoids the problematic string replacement entirely\n",
    "            if FILTER_BHRUT and can_delete_bhrut(table_name.upper()):\n",
    "                # Create a temp view of PKs to delete\n",
    "                temp_view_name = f\"bhrut_delete_pks_{table_name.replace('.', '_')}\"\n",
    "                spark.sql(f\"\"\"\n",
    "                    CREATE OR REPLACE TEMP VIEW {temp_view_name} AS\n",
    "                    SELECT t.{pk_col}\n",
    "                    FROM {fqn} t\n",
    "                    {joins_sql}\n",
    "                    WHERE t.Trust = 'BHRUT'\n",
    "                      AND {trust_coalesce} = 'BHRUT'\n",
    "                \"\"\")\n",
    "                \n",
    "                spark.sql(f\"\"\"\n",
    "                    DELETE FROM {fqn}\n",
    "                    WHERE {pk_col} IN (SELECT {pk_col} FROM {temp_view_name})\n",
    "                \"\"\")\n",
    "\n",
    "            # 2. Update Missing Trust using MERGE with proper PK matching\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {fqn} tgt\n",
    "                USING (\n",
    "                    SELECT t.{pk_col}, {trust_coalesce} as resolved_trust\n",
    "                    FROM {fqn} t\n",
    "                    {joins_sql}\n",
    "                    WHERE (t.Trust IS NULL OR t.Trust = '')\n",
    "                      AND {time_filter}\n",
    "                      AND {trust_coalesce} IS NOT NULL\n",
    "                ) src\n",
    "                ON tgt.{pk_col} = src.{pk_col}\n",
    "                WHEN MATCHED THEN UPDATE SET tgt.Trust = src.resolved_trust {adc_update}\n",
    "            \"\"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      Error: {str(e)[:150]}\")\n",
    "\n",
    "    print(\"    Optimizing lookup tables...\")\n",
    "    try:\n",
    "        spark.sql(f\"OPTIMIZE {HUB_TBL} ZORDER BY (key_type, key_id)\")\n",
    "        spark.sql(f\"OPTIMIZE {ENC_ORG_TBL} ZORDER BY (ENCNTR_ID)\")\n",
    "        spark.sql(f\"VACUUM {HUB_TBL} RETAIN 168 HOURS\")\n",
    "        spark.sql(f\"VACUUM {ENC_ORG_TBL} RETAIN 168 HOURS\")\n",
    "    except Exception as e:\n",
    "        print(f\"      Optimization warning: {str(e)[:100]}\")\n",
    "    \n",
    "    # Clear version tracker on weekly clean to force full re-evaluation next run\n",
    "    try:\n",
    "        spark.sql(f\"DELETE FROM {PHASE2C_TRACKER_TBL}\")\n",
    "        print(\"    Cleared Phase 2c version tracker for fresh start\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ==============================================================================\n",
    "# 11. SUMMARY REPORTING\n",
    "# ==============================================================================\n",
    "def print_summary(start_time):\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\nRUN SUMMARY\\n\" + \"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        enc_stats = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(DISTINCT ENCNTR_ID) as encounters_changed,\n",
    "                   COUNT(DISTINCT ORGANIZATION_ID) as orgs_involved\n",
    "            FROM {CHANGED_ENC_TBL}\n",
    "            WHERE run_date = current_date()\n",
    "        \"\"\").collect()[0]\n",
    "        print(f\"Encounters Changed: {enc_stats['encounters_changed']}\")\n",
    "        print(f\"Organizations Involved: {enc_stats['orgs_involved']}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Report from aggregated audit summary\n",
    "    try:\n",
    "        audit_stats = spark.sql(f\"\"\"\n",
    "            SELECT trust, SUM(record_count) as total_records, COUNT(DISTINCT table_name) as tables_affected\n",
    "            FROM {AUDIT_SUMMARY_TBL}\n",
    "            WHERE run_timestamp >= timestamp('{RUN_TS.isoformat()}')\n",
    "            GROUP BY trust\n",
    "        \"\"\").collect()\n",
    "        print(\"\\nAudit Summary (Aggregated):\")\n",
    "        for r in audit_stats:\n",
    "            print(f\"  {r['trust']}: {r['total_records']} records across {r['tables_affected']} tables\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Report Phase 2c skip statistics\n",
    "    try:\n",
    "        tracker_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {PHASE2C_TRACKER_TBL}\").collect()[0]['cnt']\n",
    "        print(f\"\\nPhase 2c Tracker: {tracker_count} tables tracked for version-based skipping\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"\\nMode: {'WEEKLY DEEP CLEAN' if IS_WEEKLY_RUN else 'INCREMENTAL'}\")\n",
    "    print(f\"Total Duration: {duration:.2f}s\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    print(\"=\" * 60 + f\"\\nTrust Assignment Pipeline v3 - {RUN_TS.strftime('%Y-%m-%d %H:%M:%S')}\\n\" + \"=\" * 60)\n",
    "\n",
    "    time_op(\"Setup Infrastructure\", ensure_setup)\n",
    "\n",
    "    # 1. Discover tables\n",
    "    cdc_tables, all_targets = time_op(\"Discover Table Scopes\", get_table_scopes)\n",
    "    if not all_targets:\n",
    "        print(\"No eligible tables found.\")\n",
    "        raise SystemExit(0)\n",
    "\n",
    "    # 2. Track new orgs\n",
    "    newly_mapped_orgs = time_op(\"Track Newly Mapped Orgs\", track_newly_mapped_orgs)\n",
    "\n",
    "    # 3. Refresh Hub (Phase 1)\n",
    "    mod_keys = time_op(\"Refresh Mapping Hub\", refresh_hub)\n",
    "\n",
    "    # 4. Materialize hub views for efficient joins (NEW)\n",
    "    time_op(\"Materialize Hub Views\", materialize_hub_views)\n",
    "\n",
    "    # 5. Update Targets - Direct CDC (Phase 2) - now returns processed PKs\n",
    "    processed_tables = time_op(\"Update Targets (Direct CDC)\", lambda: update_targets(cdc_tables, newly_mapped_orgs))\n",
    "\n",
    "    # 6. Chained Dependencies (Phase 2b) - uses all_targets now\n",
    "    time_op(\"Propagate Chained Updates\", lambda: propagate_chained_updates(mod_keys or set(), all_targets))\n",
    "\n",
    "    # 7. Reverse Propagation (Phase 2c) - with version-based skip optimization\n",
    "    time_op(\"Propagate Encounter Changes\", lambda: propagate_encounter_changes(all_targets, processed_tables or {}))\n",
    "\n",
    "    # 8. Flag Unknown Orgs (Phase 3)\n",
    "    time_op(\"Flag Unknown Organizations\", flag_unknown_organizations)\n",
    "\n",
    "    # 9. Weekly Deep Clean (Phase 4)\n",
    "    if IS_WEEKLY_RUN:\n",
    "        time_op(\"Weekly Deep Clean\", lambda: run_weekly_clean(all_targets))\n",
    "\n",
    "    print_summary(start_time)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Trust Tagging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
