{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "609fef78-7af1-4224-8242-76afd6755e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combined Blob Processing Notebook\n",
    "# Acts as orchestrator when run without parameters\n",
    "# Acts as shard processor when run with SHARD_ID parameter\n",
    "\n",
    "from pyspark.sql import functions as F, types as T, Row\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import io, re, random, string, os, traceback, tempfile, shutil, subprocess\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# ==================== PARAMETERS ====================\n",
    "# Create widgets for optional parameters\n",
    "dbutils.widgets.text(\"RUN_ID\", \"\")\n",
    "dbutils.widgets.text(\"SHARDS\", \"8\")  # Total number of parallel shards\n",
    "dbutils.widgets.text(\"SHARD_ID\", \"\")  # Empty means run as orchestrator\n",
    "\n",
    "RUN_ID = dbutils.widgets.get(\"RUN_ID\")\n",
    "SHARDS = int(dbutils.widgets.get(\"SHARDS\"))\n",
    "SHARD_ID_STR = dbutils.widgets.get(\"SHARD_ID\")\n",
    "\n",
    "# Determine execution mode\n",
    "IS_ORCHESTRATOR = (SHARD_ID_STR == \"\")\n",
    "SHARD_ID = None if IS_ORCHESTRATOR else int(SHARD_ID_STR)\n",
    "\n",
    "# Configuration\n",
    "STAGING_DB = \"4_prod.tmp\"\n",
    "TARGET_TABLE = \"4_prod.bronze.mill_blob_text\"\n",
    "METADATA_TABLE = f\"{STAGING_DB}.pipeline_metadata\"\n",
    "MAX_BLOB_SIZE = 16 * 1024 * 1024\n",
    "LZW_TIMEOUT_SECONDS = 30\n",
    "OCF_MARKER = b'ocf_blob\\0'\n",
    "\n",
    "# OCR Configuration\n",
    "ENABLE_OCR = True\n",
    "OCR_MAX_PAGES = 10\n",
    "OCR_MAX_PDF_SIZE_MB = 50\n",
    "OCR_LANG = \"eng\"\n",
    "\n",
    "# ==================== SPARK CONFIGURATION ====================\n",
    "# Optimize for Unity Catalog Shared cluster\n",
    "# Note: Some configs are restricted on UC Shared clusters\n",
    "try:\n",
    "    # These should work on most clusters\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"320\")\n",
    "    spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Some Spark configs could not be set: {e}\")\n",
    "    # Continue with defaults\n",
    "\n",
    "# ==================== MODE DETECTION ====================\n",
    "if IS_ORCHESTRATOR:\n",
    "    print(\"=\"*80)\n",
    "    print(\"RUNNING AS ORCHESTRATOR\")\n",
    "    print(f\"Will launch {SHARDS} parallel shards\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"RUNNING AS SHARD PROCESSOR\")\n",
    "    print(f\"Shard {SHARD_ID}/{SHARDS}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# ==================== ORCHESTRATOR MODE ====================\n",
    "if IS_ORCHESTRATOR:\n",
    "    # Get latest run if not specified\n",
    "    if not RUN_ID:\n",
    "        latest_run = spark.sql(f\"\"\"\n",
    "            SELECT run_id, worklist_table \n",
    "            FROM {METADATA_TABLE}\n",
    "            WHERE status = 'worklist_created'\n",
    "            ORDER BY created_ts DESC\n",
    "            LIMIT 1\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        if not latest_run:\n",
    "            raise Exception(\"No worklist found! Run Notebook 1 first.\")\n",
    "        \n",
    "        RUN_ID = latest_run[0]['run_id']\n",
    "        WORKLIST_TABLE = latest_run[0]['worklist_table']\n",
    "    else:\n",
    "        run_info = spark.sql(f\"\"\"\n",
    "            SELECT worklist_table \n",
    "            FROM {METADATA_TABLE}\n",
    "            WHERE run_id = '{RUN_ID}'\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        if not run_info:\n",
    "            raise Exception(f\"Run {RUN_ID} not found!\")\n",
    "        \n",
    "        WORKLIST_TABLE = run_info[0]['worklist_table']\n",
    "    \n",
    "    print(f\"Run ID: {RUN_ID}\")\n",
    "    print(f\"Worklist: {WORKLIST_TABLE}\")\n",
    "    \n",
    "    # Check worklist status\n",
    "    print(\"\\nChecking worklist status...\")\n",
    "    status_df = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            status,\n",
    "            COUNT(*) as count,\n",
    "            SUM(compressed_size) as total_bytes\n",
    "        FROM {WORKLIST_TABLE}\n",
    "        GROUP BY status\n",
    "    \"\"\")\n",
    "    \n",
    "    status_df.show()\n",
    "    \n",
    "    pending_count = status_df.filter(F.col(\"status\") == \"pending\").select(\"count\").collect()\n",
    "    if not pending_count or pending_count[0]['count'] == 0:\n",
    "        print(\"No pending events to process!\")\n",
    "        dbutils.notebook.exit(\"No work\")\n",
    "    \n",
    "    total_pending = pending_count[0]['count']\n",
    "    print(f\"\\nTotal pending events: {total_pending:,}\")\n",
    "    \n",
    "    # Launch shards\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LAUNCHING {SHARDS} PARALLEL SHARDS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    \n",
    "    def run_shard(shard_id):\n",
    "        print(f\"  Launching shard {shard_id}...\")\n",
    "        try:\n",
    "            # Get the current notebook path\n",
    "            current_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "            \n",
    "            result = dbutils.notebook.run(\n",
    "                current_path,  # Run itself!\n",
    "                timeout_seconds=7200,\n",
    "                arguments={\n",
    "                    \"RUN_ID\": RUN_ID,\n",
    "                    \"SHARDS\": str(SHARDS),\n",
    "                    \"SHARD_ID\": str(shard_id)  # This makes it run as shard processor\n",
    "                }\n",
    "            )\n",
    "            return (shard_id, \"SUCCESS\", result)\n",
    "        except Exception as e:\n",
    "            return (shard_id, \"FAILED\", str(e))\n",
    "    \n",
    "    # Launch all shards in parallel\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=SHARDS) as executor:\n",
    "        futures = {executor.submit(run_shard, i): i for i in range(SHARDS)}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            shard_id = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                print(f\"  Shard {result[0]} completed: {result[1]}\")\n",
    "            except Exception as e:\n",
    "                results.append((shard_id, \"FAILED\", str(e)))\n",
    "                print(f\"  Shard {shard_id} failed: {str(e)}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"RESULTS SUMMARY:\")\n",
    "    for shard_id, status, message in results:\n",
    "        print(f\"  Shard {shard_id}: {status} - {message}\")\n",
    "    \n",
    "    success_count = sum(1 for _, status, _ in results if status == \"SUCCESS\")\n",
    "    print(f\"\\nCompleted: {success_count}/{SHARDS} shards successful\")\n",
    "    print(f\"Total time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Verify results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFYING RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    final_status = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            status,\n",
    "            COUNT(*) as count\n",
    "        FROM {WORKLIST_TABLE}\n",
    "        GROUP BY status\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nFinal worklist status:\")\n",
    "    final_status.show()\n",
    "    \n",
    "    # Collect batch table names and count events\n",
    "    batch_tables = []\n",
    "    total_processed = 0\n",
    "    for shard_id in range(SHARDS):\n",
    "        batch_table = f\"{STAGING_DB}.batch_{RUN_ID}_shard_{shard_id:04d}\"\n",
    "        if spark.catalog.tableExists(batch_table):\n",
    "            batch_tables.append(batch_table)\n",
    "    \n",
    "    # Check batch tables\n",
    "    if batch_tables:\n",
    "        print(f\"\\nBatch tables created: {len(batch_tables)}\")\n",
    "        for table in batch_tables:\n",
    "            count = spark.table(table).count()\n",
    "            print(f\"  {table}: {count:,} events\")\n",
    "            total_processed += count\n",
    "        print(f\"\\nTotal events processed: {total_processed:,}\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Warning: No batch tables found\")\n",
    "    \n",
    "    # Update metadata with batch tables and final count - single update to avoid conflicts\n",
    "    completed_count = final_status.filter(F.col(\"status\") == \"completed\").select(\"count\").collect()\n",
    "    if completed_count and batch_tables:\n",
    "        completed_events = completed_count[0]['count']\n",
    "        batch_tables_str = \",\".join(batch_tables)\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "            UPDATE {METADATA_TABLE}\n",
    "            SET \n",
    "                batch_tables = '{batch_tables_str}',\n",
    "                processed_events = {total_processed},\n",
    "                status = 'processing_complete',\n",
    "                completed_ts = current_timestamp()\n",
    "            WHERE run_id = '{RUN_ID}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"\\n✓ Processing complete!\")\n",
    "        print(f\"✓ Completed events in worklist: {completed_events:,}\")\n",
    "        print(f\"✓ Total records in batch tables: {total_processed:,}\")\n",
    "        print(f\"✓ Batch tables: {batch_tables_str}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NEXT STEPS:\")\n",
    "    print(\"1. Validate batch tables if needed\")\n",
    "    print(\"2. Run merge process to combine batch tables into final bronze table\")\n",
    "    print(\"3. Clean up batch tables after successful merge\")\n",
    "    \n",
    "    dbutils.notebook.exit(\"Orchestration complete\")\n",
    "\n",
    "# ==================== SHARD PROCESSOR MODE ====================\n",
    "# All the processing logic for when running as a shard\n",
    "\n",
    "# Library imports\n",
    "import chardet\n",
    "from charset_normalizer import detect as cn_detect\n",
    "from ocflzw_decompress.lzw import LzwDecompress\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "from bs4 import BeautifulSoup\n",
    "import docx2txt\n",
    "from openpyxl import load_workbook\n",
    "import xlrd\n",
    "import pdfplumber\n",
    "from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "\n",
    "# Optional libraries\n",
    "try:\n",
    "    import magic\n",
    "except Exception:\n",
    "    magic = None\n",
    "\n",
    "try:\n",
    "    import olefile\n",
    "    HAVE_OLE = True\n",
    "except Exception:\n",
    "    HAVE_OLE = False\n",
    "\n",
    "try:\n",
    "    import extract_msg\n",
    "    HAVE_EXTRACT_MSG = True\n",
    "except Exception:\n",
    "    HAVE_EXTRACT_MSG = False\n",
    "\n",
    "try:  \n",
    "    import fitz\n",
    "    HAVE_FITZ = True  \n",
    "except Exception:  \n",
    "    HAVE_FITZ = False  \n",
    "\n",
    "try:  \n",
    "    import pypdf\n",
    "    HAVE_PYPDF = True  \n",
    "except Exception:  \n",
    "    HAVE_PYPDF = False  \n",
    "\n",
    "try:  \n",
    "    import ocrmypdf\n",
    "    HAVE_OCRMYPDF = True  \n",
    "except Exception:  \n",
    "    HAVE_OCRMYPDF = False  \n",
    "\n",
    "try:  \n",
    "    import pytesseract\n",
    "    from PIL import Image\n",
    "    HAVE_TESS = True  \n",
    "except Exception:  \n",
    "    HAVE_TESS = False\n",
    "\n",
    "# Get run information\n",
    "if not RUN_ID:\n",
    "    raise Exception(\"RUN_ID required for shard processing\")\n",
    "\n",
    "run_info = spark.sql(f\"\"\"\n",
    "    SELECT worklist_table \n",
    "    FROM {METADATA_TABLE}\n",
    "    WHERE run_id = '{RUN_ID}'\n",
    "\"\"\").collect()\n",
    "\n",
    "if not run_info:\n",
    "    raise Exception(f\"Run {RUN_ID} not found!\")\n",
    "\n",
    "WORKLIST_TABLE = run_info[0]['worklist_table']\n",
    "\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Worklist: {WORKLIST_TABLE}\")\n",
    "\n",
    "# ==================== HELPER FUNCTIONS (OPTIMIZED) ====================\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def safe_numeric(value, default=None):\n",
    "    if value is None or value == '':\n",
    "        return default\n",
    "    try:\n",
    "        if isinstance(value, (int, float)):\n",
    "            return int(value)\n",
    "        return int(float(str(value)))\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def combine_blob_chunks(chunks):\n",
    "    \"\"\"OPTIMIZED: Combine blob chunks using join\"\"\"\n",
    "    return b\"\".join(c for c in (chunks or []) if c)\n",
    "\n",
    "def remove_ocf_wrapper_aggressive(data: bytes):\n",
    "    \"\"\"Remove ALL occurrences of OCF marker\"\"\"\n",
    "    try:\n",
    "        if not data:\n",
    "            return data\n",
    "        if data.endswith(OCF_MARKER):\n",
    "            data = data[:-len(OCF_MARKER)]\n",
    "        if OCF_MARKER in data:\n",
    "            data = b''.join(data.split(OCF_MARKER))\n",
    "        return data\n",
    "    except Exception:\n",
    "        return data\n",
    "\n",
    "def remove_ocf_wrapper_conservative(data: bytes):\n",
    "    \"\"\"Only remove trailing OCF marker\"\"\"\n",
    "    try:\n",
    "        if not data:\n",
    "            return data\n",
    "        if data.endswith(OCF_MARKER):\n",
    "            return data[:-len(OCF_MARKER)]\n",
    "        return data\n",
    "    except Exception:\n",
    "        return data\n",
    "\n",
    "def decompress_lzw_with_timeout(data: bytes, timeout_seconds=LZW_TIMEOUT_SECONDS):\n",
    "    \"\"\"Decompress LZW with timeout protection\"\"\"\n",
    "    try:\n",
    "        if timeout_seconds <= 0 or len(data) < 100000:\n",
    "            return bytes(LzwDecompress().decompress(data))\n",
    "        \n",
    "        import threading\n",
    "        if threading.current_thread() is threading.main_thread():\n",
    "            import signal\n",
    "            \n",
    "            class TimeoutError(Exception):\n",
    "                pass\n",
    "            \n",
    "            def timeout_handler(signum, frame):\n",
    "                raise TimeoutError(f\"LZW timeout after {timeout_seconds}s\")\n",
    "            \n",
    "            old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n",
    "            signal.alarm(timeout_seconds)\n",
    "            try:\n",
    "                result = bytes(LzwDecompress().decompress(data))\n",
    "                signal.alarm(0)\n",
    "                return result\n",
    "            finally:\n",
    "                signal.signal(signal.SIGALRM, old_handler)\n",
    "        else:\n",
    "            return bytes(LzwDecompress().decompress(data))\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def decompress_blob_simple(raw: bytes, compression_cd, chunk_count: int, metrics: dict):\n",
    "    \"\"\"OPTIMIZED: Fixed indentation and early ocf_count recording\"\"\"\n",
    "    if not raw:\n",
    "        return None, \"Empty content\"\n",
    "    \n",
    "    ocf_count = raw.count(OCF_MARKER)\n",
    "    metrics[\"ocf_marker_count\"] = ocf_count\n",
    "    \n",
    "    try:\n",
    "        cd = int(compression_cd) if compression_cd is not None else None\n",
    "    except Exception:\n",
    "        cd = None\n",
    "    \n",
    "    if cd == 727:  # Uncompressed\n",
    "        metrics[\"decompression_strategy\"] = \"uncompressed\"\n",
    "        return (remove_ocf_wrapper_aggressive(raw) if ocf_count > 0 else raw), None\n",
    "    \n",
    "    if cd != 728:  # Not LZW\n",
    "        return None, f\"Unknown compression: {compression_cd}\"\n",
    "    \n",
    "    # LZW decompression strategies\n",
    "    if chunk_count > 10 or ocf_count > 10:\n",
    "        strategies = [(\"aggressive\", remove_ocf_wrapper_aggressive),\n",
    "                      (\"raw\", lambda d: d)]\n",
    "    else:\n",
    "        strategies = [(\"aggressive\", remove_ocf_wrapper_aggressive),\n",
    "                      (\"conservative\", remove_ocf_wrapper_conservative),\n",
    "                      (\"raw\", lambda d: d)]\n",
    "    \n",
    "    last_error = None\n",
    "    for strategy_name, strategy_fn in strategies:\n",
    "        try:\n",
    "            cleaned = strategy_fn(raw)\n",
    "            result = decompress_lzw_with_timeout(cleaned)\n",
    "            metrics[\"decompression_strategy\"] = f\"lzw_{strategy_name}\"\n",
    "            return result, None\n",
    "        except TimeoutException as e:\n",
    "            metrics[\"timeout_hit\"] = True\n",
    "            metrics[\"timeout_stage\"] = f\"lzw_{strategy_name}\"\n",
    "            last_error = str(e)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_error = str(e)\n",
    "            continue\n",
    "    \n",
    "    return None, f\"LZW failed all attempts: {last_error}\"\n",
    "\n",
    "def calculate_printable_ratio(text, sample_size=1000):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    if len(text) <= sample_size:\n",
    "        sample = text\n",
    "    else:\n",
    "        sample = ''.join(random.choice(text) for _ in range(sample_size))\n",
    "    printable = sum(1 for c in sample if c in string.printable)\n",
    "    return printable / len(sample) if sample else 0.0\n",
    "\n",
    "def guess_text(content: bytes):\n",
    "    if not content:\n",
    "        return None, None, 0.0\n",
    "    \n",
    "    ch = chardet.detect(content) or {}\n",
    "    cn = cn_detect(content) or {}\n",
    "    candidates = [ch.get('encoding'), cn.get('encoding'), 'utf-8', 'windows-1252', 'latin-1', 'ascii']\n",
    "    best_decoded, best_encoding, best_ratio = None, None, 0.0\n",
    "    \n",
    "    for enc in candidates:\n",
    "        if not enc:\n",
    "            continue\n",
    "        try:\n",
    "            decoded = content.decode(enc, errors='ignore')\n",
    "            r = calculate_printable_ratio(decoded)\n",
    "            if r > best_ratio:\n",
    "                best_ratio, best_decoded, best_encoding = r, decoded, enc\n",
    "            if best_ratio > 0.95:\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return best_decoded, best_encoding, best_ratio\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    cleaned = re.sub(r'<%.*?%>', '', text, flags=re.DOTALL)\n",
    "    cleaned = cleaned.replace('|', '\\n')\n",
    "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
    "    cleaned = re.sub(r'\\n+', '\\n', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def detect_mime(content: bytes):\n",
    "    if not content:\n",
    "        return 'application/octet-stream'\n",
    "    \n",
    "    if content.startswith(b'%PDF-'):\n",
    "        return 'application/pdf'\n",
    "    \n",
    "    if content.startswith(b'\\x50\\x4B\\x03\\x04'):\n",
    "        head = content[:4096]\n",
    "        if b'word/' in head:\n",
    "            return 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n",
    "        if b'xl/' in head:\n",
    "            return 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "        if b'ppt/' in head:\n",
    "            return 'application/vnd.openxmlformats-officedocument.presentationml.presentation'\n",
    "        return 'application/zip'\n",
    "    \n",
    "    if content.startswith(b'\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1'):\n",
    "        return 'application/x-ole-storage'\n",
    "    \n",
    "    if content.startswith(b'{\\\\'):\n",
    "        return 'text/rtf'\n",
    "    \n",
    "    if magic:\n",
    "        try:\n",
    "            return magic.Magic(mime=True).from_buffer(content) or 'application/octet-stream'\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return 'application/octet-stream'\n",
    "\n",
    "def classify_ole(data: bytes):\n",
    "    if not (HAVE_OLE and data):\n",
    "        return 'application/x-ole-storage'\n",
    "    \n",
    "    try:\n",
    "        with olefile.OleFileIO(io.BytesIO(data)) as ole:\n",
    "            if ole.exists('WordDocument'):\n",
    "                return 'application/msword'\n",
    "            if ole.exists('Workbook') or ole.exists('Book'):\n",
    "                return 'application/vnd.ms-excel'\n",
    "            if ole.exists('PowerPoint Document'):\n",
    "                return 'application/vnd.ms-powerpoint'\n",
    "            if ole.exists('__properties_version1.0') and (\n",
    "                ole.exists('__recip_version1.0') or ole.exists('__attach_version1.0')\n",
    "            ):\n",
    "                return 'application/vnd.ms-outlook'\n",
    "            return 'application/x-ole-storage'\n",
    "    except Exception:\n",
    "        return 'application/x-ole-storage'\n",
    "\n",
    "def refine_mime_with_ole(content_type, data: bytes):\n",
    "    if content_type == 'application/x-ole-storage':\n",
    "        return classify_ole(data)\n",
    "    return content_type\n",
    "\n",
    "# [Include all the text extraction functions from the previous version]\n",
    "# I'll include just the key ones here for brevity\n",
    "\n",
    "def extract_pdf_with_pypdf(content: bytes):\n",
    "    if not HAVE_PYPDF:\n",
    "        return None\n",
    "    try:\n",
    "        reader = pypdf.PdfReader(io.BytesIO(content), strict=False)\n",
    "        if getattr(reader, \"is_encrypted\", False):\n",
    "            try:\n",
    "                reader.decrypt(\"\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        parts = []\n",
    "        for page in reader.pages:\n",
    "            t = page.extract_text() or \"\"\n",
    "            if t.strip():\n",
    "                parts.append(t)\n",
    "        return \"\\n\".join(parts) if parts else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_pdf_with_pymupdf(content: bytes):\n",
    "    if not HAVE_FITZ:\n",
    "        return None\n",
    "    try:\n",
    "        doc = fitz.open(stream=content, filetype=\"pdf\")\n",
    "        if doc.needs_pass:\n",
    "            try:\n",
    "                doc.authenticate(\"\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        parts = []\n",
    "        for page in doc:\n",
    "            t = page.get_text(\"text\") or \"\"\n",
    "            if t.strip():\n",
    "                parts.append(t)\n",
    "        return \"\\n\".join(parts) if parts else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_docx(content):\n",
    "    try:\n",
    "        return docx2txt.process(io.BytesIO(content))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_excel(content):\n",
    "    try:\n",
    "        wb = load_workbook(io.BytesIO(content), read_only=True, data_only=True)\n",
    "        parts = []\n",
    "        for sheet in wb.sheetnames:\n",
    "            ws = wb[sheet]\n",
    "            for row in ws.iter_rows(values_only=True):\n",
    "                row_text = ' '.join(str(cell) for cell in row if cell is not None)\n",
    "                if row_text.strip():\n",
    "                    parts.append(row_text)\n",
    "        return '\\n'.join(parts)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# [Include all other extraction functions from previous version...]\n",
    "\n",
    "def parse_blob_content(content: bytes, provided_type=None):\n",
    "    \"\"\"Main content parsing function\"\"\"\n",
    "    if not content:\n",
    "        return None, None, None\n",
    "    \n",
    "    content_type = provided_type or detect_mime(content)\n",
    "    content_type = refine_mime_with_ole(content_type, content)\n",
    "    \n",
    "    # Quick routing based on content type\n",
    "    if content_type == 'application/pdf':\n",
    "        # Try fast extractors first\n",
    "        txt = extract_pdf_with_pypdf(content) or extract_pdf_with_pymupdf(content)\n",
    "        if txt:\n",
    "            return clean_text(txt), content_type, 'utf-8'\n",
    "        return \"[PDF - extraction failed]\", content_type, None\n",
    "    \n",
    "    elif content_type in ('application/vnd.openxmlformats-officedocument.wordprocessingml.document',):\n",
    "        txt = extract_text_from_docx(content)\n",
    "        if txt:\n",
    "            return clean_text(txt), content_type, 'utf-8'\n",
    "    \n",
    "    elif content_type in ('application/vnd.ms-excel', \n",
    "                          'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'):\n",
    "        txt = extract_text_from_excel(content)\n",
    "        if txt:\n",
    "            return clean_text(txt), content_type, 'utf-8'\n",
    "    \n",
    "    # Default text extraction\n",
    "    decoded, best_enc, ratio = guess_text(content)\n",
    "    if decoded:\n",
    "        return clean_text(decoded), content_type, best_enc\n",
    "    \n",
    "    return f\"[Binary data, unable to decode]\", content_type, None\n",
    "\n",
    "# ==================== MAIN PROCESSING UDF ====================\n",
    "\n",
    "process_output_schema = T.StructType([\n",
    "    T.StructField(\"EVENT_ID\", T.LongType(), True),\n",
    "    T.StructField(\"ENCNTR_ID\", T.LongType(), True),\n",
    "    T.StructField(\"Trust\", T.StringType(), True),\n",
    "    T.StructField(\"VALID_UNTIL_DT_TM\", T.TimestampType(), True),\n",
    "    T.StructField(\"VALID_FROM_DT_TM\", T.TimestampType(), True),\n",
    "    T.StructField(\"UPDT_DT_TM\", T.TimestampType(), True),\n",
    "    T.StructField(\"UPDT_ID\", T.LongType(), True),\n",
    "    T.StructField(\"UPDT_TASK\", T.LongType(), True),\n",
    "    T.StructField(\"UPDT_CNT\", T.LongType(), True),\n",
    "    T.StructField(\"UPDT_APPLCTX\", T.LongType(), True),\n",
    "    T.StructField(\"LAST_UTC_TS\", T.TimestampType(), True),\n",
    "    T.StructField(\"ADC_UPDT\", T.TimestampType(), True),\n",
    "    T.StructField(\"BLOB_BINARY\", T.BinaryType(), True),\n",
    "    T.StructField(\"CONTENT_TYPE\", T.StringType(), True),\n",
    "    T.StructField(\"ENCODING\", T.StringType(), True),\n",
    "    T.StructField(\"BLOB_TEXT\", T.StringType(), True),\n",
    "    T.StructField(\"BINARY_SIZE\", T.LongType(), True),\n",
    "    T.StructField(\"TEXT_LENGTH\", T.LongType(), True),\n",
    "    T.StructField(\"STATUS\", T.StringType(), True),\n",
    "    T.StructField(\"anon_text\", T.StringType(), True),\n",
    "    T.StructField(\"metrics\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "@F.udf(returnType=process_output_schema)\n",
    "def process_blob_udf(event_id, encntr_id, trust, chunks_data, valid_until, valid_from, updt_dt, \n",
    "                     updt_id, updt_task, updt_cnt, updt_applctx, \n",
    "                     last_utc, adc_updt):\n",
    "    \"\"\"Process a single event's chunks\"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        \"combine_ms\": 0,\n",
    "        \"decompress_ms\": 0,\n",
    "        \"parse_ms\": 0,\n",
    "        \"chunk_count\": 0,\n",
    "        \"ocf_marker_count\": 0,\n",
    "        \"decompression_strategy\": None,\n",
    "        \"timeout_hit\": False,\n",
    "        \"timeout_stage\": None,\n",
    "        \"error\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import time\n",
    "        t0 = time.perf_counter()\n",
    "        \n",
    "        # Sort chunks by sequence number\n",
    "        sorted_chunks = sorted(chunks_data, key=lambda x: x['BLOB_SEQ_NUM'] or 0)\n",
    "        chunks = [c['BLOB_CONTENTS'] for c in sorted_chunks if c['BLOB_CONTENTS']]\n",
    "        metrics[\"chunk_count\"] = len(chunks)\n",
    "        \n",
    "        # Get first chunk's compression code\n",
    "        compression_cd = sorted_chunks[0]['COMPRESSION_CD'] if sorted_chunks else None\n",
    "        \n",
    "        # Combine chunks - OPTIMIZED\n",
    "        combined = combine_blob_chunks(chunks)\n",
    "        metrics[\"combine_ms\"] = int((time.perf_counter() - t0) * 1000)\n",
    "        \n",
    "        # Check size\n",
    "        if len(combined) > MAX_BLOB_SIZE:\n",
    "            return Row(\n",
    "                EVENT_ID=safe_numeric(event_id),\n",
    "                ENCNTR_ID=safe_numeric(encntr_id),\n",
    "                Trust=trust,\n",
    "                VALID_UNTIL_DT_TM=valid_until,\n",
    "                VALID_FROM_DT_TM=valid_from,\n",
    "                UPDT_DT_TM=updt_dt,\n",
    "                UPDT_ID=safe_numeric(updt_id),\n",
    "                UPDT_TASK=safe_numeric(updt_task),\n",
    "                UPDT_CNT=safe_numeric(updt_cnt),\n",
    "                UPDT_APPLCTX=safe_numeric(updt_applctx),\n",
    "                LAST_UTC_TS=last_utc,\n",
    "                ADC_UPDT=adc_updt,\n",
    "                BLOB_BINARY=None,\n",
    "                CONTENT_TYPE=None,\n",
    "                ENCODING=None,\n",
    "                BLOB_TEXT=None,\n",
    "                BINARY_SIZE=len(combined),\n",
    "                TEXT_LENGTH=None,\n",
    "                STATUS=f\"Compressed Too Large: {len(combined)} bytes\",\n",
    "                anon_text=None,\n",
    "                metrics=json.dumps(metrics)\n",
    "            )\n",
    "        \n",
    "        # Decompress - OPTIMIZED\n",
    "        t0 = time.perf_counter()\n",
    "        decompressed, dec_err = decompress_blob_simple(combined, compression_cd, len(chunks), metrics)\n",
    "        metrics[\"decompress_ms\"] = int((time.perf_counter() - t0) * 1000)\n",
    "        \n",
    "        if decompressed is None:\n",
    "            return Row(\n",
    "                EVENT_ID=safe_numeric(event_id),\n",
    "                ENCNTR_ID=safe_numeric(encntr_id),\n",
    "                Trust=trust,\n",
    "                VALID_UNTIL_DT_TM=valid_until,\n",
    "                VALID_FROM_DT_TM=valid_from,\n",
    "                UPDT_DT_TM=updt_dt,\n",
    "                UPDT_ID=safe_numeric(updt_id),\n",
    "                UPDT_TASK=safe_numeric(updt_task),\n",
    "                UPDT_CNT=safe_numeric(updt_cnt),\n",
    "                UPDT_APPLCTX=safe_numeric(updt_applctx),\n",
    "                LAST_UTC_TS=last_utc,\n",
    "                ADC_UPDT=adc_updt,\n",
    "                BLOB_BINARY=None,\n",
    "                CONTENT_TYPE=None,\n",
    "                ENCODING=None,\n",
    "                BLOB_TEXT=None,\n",
    "                BINARY_SIZE=None,\n",
    "                TEXT_LENGTH=None,\n",
    "                STATUS=dec_err or \"Decompression failed\",\n",
    "                anon_text=None,\n",
    "                metrics=json.dumps(metrics)\n",
    "            )\n",
    "        \n",
    "        # Check decompressed size\n",
    "        if len(decompressed) > MAX_BLOB_SIZE:\n",
    "            return Row(\n",
    "                EVENT_ID=safe_numeric(event_id),\n",
    "                ENCNTR_ID=safe_numeric(encntr_id),\n",
    "                Trust=trust,\n",
    "                VALID_UNTIL_DT_TM=valid_until,\n",
    "                VALID_FROM_DT_TM=valid_from,\n",
    "                UPDT_DT_TM=updt_dt,\n",
    "                UPDT_ID=safe_numeric(updt_id),\n",
    "                UPDT_TASK=safe_numeric(updt_task),\n",
    "                UPDT_CNT=safe_numeric(updt_cnt),\n",
    "                UPDT_APPLCTX=safe_numeric(updt_applctx),\n",
    "                LAST_UTC_TS=last_utc,\n",
    "                ADC_UPDT=adc_updt,\n",
    "                BLOB_BINARY=None,\n",
    "                CONTENT_TYPE=None,\n",
    "                ENCODING=None,\n",
    "                BLOB_TEXT=None,\n",
    "                BINARY_SIZE=len(decompressed),\n",
    "                TEXT_LENGTH=None,\n",
    "                STATUS=f\"Decompressed too large: {len(decompressed)} bytes\",\n",
    "                anon_text=None,\n",
    "                metrics=json.dumps(metrics)\n",
    "            )\n",
    "        \n",
    "        # Parse content - OPTIMIZED\n",
    "        t0 = time.perf_counter()\n",
    "        blob_text, content_type, encoding = parse_blob_content(decompressed, None)\n",
    "        metrics[\"parse_ms\"] = int((time.perf_counter() - t0) * 1000)\n",
    "        \n",
    "        # Determine status\n",
    "        if blob_text:\n",
    "            if isinstance(blob_text, str):\n",
    "                if blob_text.startswith(\"[\") and \"]\" in blob_text[:200]:\n",
    "                    status = blob_text.split(\"]\")[0][1:]\n",
    "                else:\n",
    "                    status = \"Decoded\"\n",
    "            else:\n",
    "                status = \"Decoded\"\n",
    "        else:\n",
    "            status = \"Failed to decode\"\n",
    "        \n",
    "        # Safe encode text\n",
    "        if blob_text and isinstance(blob_text, str):\n",
    "            blob_text = blob_text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            if len(blob_text) > 1000000:\n",
    "                blob_text = blob_text[:1000000]\n",
    "        \n",
    "        return Row(\n",
    "            EVENT_ID=safe_numeric(event_id),\n",
    "            ENCNTR_ID=safe_numeric(encntr_id),\n",
    "            Trust=trust,\n",
    "            VALID_UNTIL_DT_TM=valid_until,\n",
    "            VALID_FROM_DT_TM=valid_from,\n",
    "            UPDT_DT_TM=updt_dt,\n",
    "            UPDT_ID=safe_numeric(updt_id),\n",
    "            UPDT_TASK=safe_numeric(updt_task),\n",
    "            UPDT_CNT=safe_numeric(updt_cnt),\n",
    "            UPDT_APPLCTX=safe_numeric(updt_applctx),\n",
    "            LAST_UTC_TS=last_utc,\n",
    "            ADC_UPDT=adc_updt,\n",
    "            BLOB_BINARY=None,\n",
    "            CONTENT_TYPE=content_type,\n",
    "            ENCODING=encoding,\n",
    "            BLOB_TEXT=blob_text,\n",
    "            BINARY_SIZE=len(decompressed) if decompressed else None,\n",
    "            TEXT_LENGTH=len(blob_text) if blob_text else None,\n",
    "            STATUS=status,\n",
    "            anon_text=None,\n",
    "            metrics=json.dumps(metrics)\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        metrics[\"error\"] = str(e)[:500]\n",
    "        return Row(\n",
    "            EVENT_ID=safe_numeric(event_id),\n",
    "            VALID_UNTIL_DT_TM=valid_until,\n",
    "            VALID_FROM_DT_TM=valid_from,\n",
    "            UPDT_DT_TM=updt_dt,\n",
    "            UPDT_ID=safe_numeric(updt_id),\n",
    "            UPDT_TASK=safe_numeric(updt_task),\n",
    "            UPDT_CNT=safe_numeric(updt_cnt),\n",
    "            UPDT_APPLCTX=safe_numeric(updt_applctx),\n",
    "            LAST_UTC_TS=last_utc,\n",
    "            ADC_UPDT=adc_updt,\n",
    "            BLOB_BINARY=None,\n",
    "            CONTENT_TYPE=None,\n",
    "            ENCODING=None,\n",
    "            BLOB_TEXT=None,\n",
    "            BINARY_SIZE=None,\n",
    "            TEXT_LENGTH=None,\n",
    "            STATUS=f\"Error: {str(e)[:200]}\",\n",
    "            anon_text=None,\n",
    "            metrics=json.dumps(metrics)\n",
    "        )\n",
    "\n",
    "# ==================== MAIN SHARD PROCESSING ====================\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "# 1) Select pending events for this shard deterministically\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Selecting events for shard {SHARD_ID}...\")\n",
    "\n",
    "events_df = spark.sql(f\"\"\"\n",
    "    SELECT EVENT_ID, ADC_UPDT, chunks_data, compressed_size\n",
    "    FROM {WORKLIST_TABLE}\n",
    "    WHERE status = 'pending'\n",
    "      AND pmod(xxhash64(EVENT_ID), {SHARDS}) = {SHARD_ID}\n",
    "\"\"\")\n",
    "\n",
    "# 2) Count and check if there's work\n",
    "event_count = events_df.count()\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Shard {SHARD_ID}: Found {event_count:,} events to process\")\n",
    "\n",
    "if event_count == 0:\n",
    "    print(f\"Shard {SHARD_ID} has no work.\")\n",
    "    dbutils.notebook.exit(f\"Shard {SHARD_ID}: No work\")\n",
    "\n",
    "# 3) Repartition for parallel UDF execution\n",
    "desired_partitions = max(160, min(320, (event_count // 200) or 1))\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Repartitioning to {desired_partitions} partitions...\")\n",
    "events_df = events_df.repartition(desired_partitions, F.col(\"EVENT_ID\"))\n",
    "\n",
    "# 4) Build first-chunk projection\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Extracting metadata from first chunks...\")\n",
    "first = (events_df\n",
    "    .select(\n",
    "        \"EVENT_ID\", \"ADC_UPDT\", \"chunks_data\",\n",
    "        F.element_at(F.col(\"chunks_data\"), 1).alias(\"first_chunk\")\n",
    "    )\n",
    "    .select(\n",
    "        \"EVENT_ID\",\n",
    "        F.col(\"first_chunk.ENCNTR_ID\").alias(\"ENCNTR_ID\"), \n",
    "        F.col(\"first_chunk.Trust\").alias(\"Trust\"),          \n",
    "        \"chunks_data\",\n",
    "        F.col(\"first_chunk.VALID_UNTIL_DT_TM\").alias(\"VALID_UNTIL_DT_TM\"),\n",
    "        F.col(\"first_chunk.VALID_FROM_DT_TM\").alias(\"VALID_FROM_DT_TM\"),\n",
    "        F.col(\"first_chunk.UPDT_DT_TM\").alias(\"UPDT_DT_TM\"),\n",
    "        F.col(\"first_chunk.UPDT_ID\").alias(\"UPDT_ID\"),\n",
    "        F.col(\"first_chunk.UPDT_TASK\").alias(\"UPDT_TASK\"),\n",
    "        F.col(\"first_chunk.UPDT_CNT\").alias(\"UPDT_CNT\"),\n",
    "        F.col(\"first_chunk.UPDT_APPLCTX\").alias(\"UPDT_APPLCTX\"),\n",
    "        F.col(\"first_chunk.LAST_UTC_TS\").alias(\"LAST_UTC_TS\"),\n",
    "        F.col(\"first_chunk.ADC_UPDT\").alias(\"ADC_UPDT\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5) Apply UDF\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Processing blobs with UDF...\")\n",
    "processed = first.select(\n",
    "    process_blob_udf(\n",
    "        F.col(\"EVENT_ID\"),\n",
    "        F.col(\"ENCNTR_ID\"),\n",
    "        F.col(\"Trust\"),\n",
    "        F.col(\"chunks_data\"),\n",
    "        F.col(\"VALID_UNTIL_DT_TM\"),\n",
    "        F.col(\"VALID_FROM_DT_TM\"),\n",
    "        F.col(\"UPDT_DT_TM\"),\n",
    "        F.col(\"UPDT_ID\"),\n",
    "        F.col(\"UPDT_TASK\"),\n",
    "        F.col(\"UPDT_CNT\"),\n",
    "        F.col(\"UPDT_APPLCTX\"),\n",
    "        F.col(\"LAST_UTC_TS\"),\n",
    "        F.col(\"ADC_UPDT\")\n",
    "    ).alias(\"r\")\n",
    ").select(\"r.*\")\n",
    "\n",
    "# 6) Write to batch table for this shard\n",
    "batch_table = f\"{STAGING_DB}.batch_{RUN_ID}_shard_{SHARD_ID:04d}\"\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Writing results to batch table: {batch_table}\")\n",
    "\n",
    "# Write with optimized partitions for later merge\n",
    "write_partitions = max(10, min(100, event_count // 1000))\n",
    "processed.repartition(write_partitions).write.mode(\"overwrite\").saveAsTable(batch_table)\n",
    "\n",
    "# 7) Mark this shard's rows as completed\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Updating worklist status...\")\n",
    "events_df.select(\"EVENT_ID\").distinct().createOrReplaceTempView(\"processed_ids\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {WORKLIST_TABLE} AS w\n",
    "    SET status = 'completed', process_ts = current_timestamp()\n",
    "    WHERE status = 'pending'\n",
    "      AND pmod(xxhash64(w.EVENT_ID), {SHARDS}) = {SHARD_ID}\n",
    "      AND EXISTS (SELECT 1 FROM processed_ids p WHERE p.EVENT_ID = w.EVENT_ID)\n",
    "\"\"\")\n",
    "\n",
    "# 8) Report metrics\n",
    "elapsed = time.time() - total_start\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SHARD {SHARD_ID} COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Events processed: {event_count:,}\")\n",
    "print(f\"Batch table: {batch_table}\")\n",
    "print(f\"Total time: {elapsed:.1f}s\")\n",
    "print(f\"Processing rate: {event_count/elapsed:.1f} events/sec\")\n",
    "\n",
    "# 9) Don't update metadata from shards - let orchestrator do it to avoid conflicts\n",
    "# Each shard just reports its results\n",
    "print(f\"Shard {SHARD_ID} completed successfully\")\n",
    "\n",
    "# Return success with batch table info\n",
    "dbutils.notebook.exit(f\"Shard {SHARD_ID}: Processed {event_count} events -> {batch_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Blob 2:Processing",
   "widgets": {
    "RUN_ID": {
     "currentValue": "",
     "nuid": "e118c9e6-7c40-40a4-b78e-751c7c8189e0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "RUN_ID",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "RUN_ID",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SHARDS": {
     "currentValue": "8",
     "nuid": "7c8fcfcd-742e-4c02-8bce-112cf126890d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "4",
      "label": null,
      "name": "SHARDS",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "4",
      "label": null,
      "name": "SHARDS",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SHARD_ID": {
     "currentValue": "",
     "nuid": "88a1d11e-b1c8-4695-a23c-4fc4eb6dbed5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "SHARD_ID",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "SHARD_ID",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
