{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1835f642-e1b8-407f-9c2b-781605d10610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "from ocflzw_decompress.lzw import LzwDecompress\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import chardet\n",
    "import traceback\n",
    "from charset_normalizer import detect\n",
    "import magic\n",
    "import random\n",
    "import string\n",
    "import io\n",
    "from datetime import datetime\n",
    "import docx2txt\n",
    "import xlrd\n",
    "from openpyxl import load_workbook\n",
    "import sys\n",
    "import signal\n",
    "import time\n",
    "import pdfplumber\n",
    "import tempfile\n",
    "import subprocess\n",
    "import os\n",
    "import threading\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# At the beginning of your script\n",
    "def error_handler(exctype, value, tb):\n",
    "    print(\"Uncaught exception:\", file=sys.stderr)\n",
    "    print(\"Type:\", exctype, file=sys.stderr)\n",
    "    print(\"Value:\", value, file=sys.stderr)\n",
    "    traceback.print_tb(tb)\n",
    "\n",
    "sys.excepthook = error_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d25f338-1034-4b4a-a021-463b987634a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_max_adc_updt(table_name):\n",
    "    default_date = datetime(1980, 1, 1)  # Python datetime object\n",
    "    try:\n",
    "        result = spark.sql(f\"SELECT MAX(ADC_UPDT) AS max_date FROM {table_name}\")\n",
    "        max_date = result.select(F.max(\"max_date\").alias(\"max_date\")).first()[\"max_date\"]\n",
    "        return max_date if max_date is not None else default_date\n",
    "    except:\n",
    "        return default_date  \n",
    "\n",
    "def table_exists(table_name):\n",
    "    try:\n",
    "        result = spark.sql(f\"SELECT 1 FROM {table_name} LIMIT 1\")\n",
    "        return result.first() is not None\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d851d8e4-3c46-49e2-a979-4ac4817d10f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "\n",
    "def format_size(size_bytes):\n",
    "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_bytes < 1024.0:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024.0\n",
    "    return f\"{size_bytes:.2f} PB\"\n",
    "\n",
    "\n",
    "\n",
    "def combine_blob_chunks(chunks):\n",
    "    combined = bytearray()\n",
    "    for chunk in chunks:\n",
    "        combined.extend(chunk)\n",
    "    return bytes(combined)\n",
    "\n",
    "def decompress_blob(blob_contents, compression_cd):\n",
    "    if blob_contents and isinstance(blob_contents, (bytes, bytearray)):\n",
    "        try:\n",
    "            if compression_cd == 728:  # LZW compression\n",
    "                lzw = LzwDecompress()\n",
    "                return bytes(lzw.decompress(blob_contents))\n",
    "            elif compression_cd == 727:  # No compression\n",
    "                return bytes(blob_contents)\n",
    "            else:\n",
    "                return f\"Unknown compression type: {compression_cd}\"\n",
    "        except Exception as e:\n",
    "            return f\"Decompression error: {str(e)}\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def enhanced_content_type_detection(content):\n",
    "    \"\"\"Enhanced content type detection with additional checks\"\"\"\n",
    "    try:\n",
    "        mime = magic.Magic(mime=True)\n",
    "        libmagic_type = mime.from_buffer(content)\n",
    "        \n",
    "        # Check for specific file signatures\n",
    "        if content.startswith(b'%PDF-'):\n",
    "            return 'application/pdf'\n",
    "        elif content.startswith(b'\\x50\\x4B\\x03\\x04'):\n",
    "            # Check for specific Office formats\n",
    "            if b'word/' in content[:200]:\n",
    "                return 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n",
    "            elif b'xl/' in content[:200]:\n",
    "                return 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "            else:\n",
    "                return 'application/zip'\n",
    "        elif content.startswith(b'\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1'):\n",
    "            # Old Office format\n",
    "            return 'application/msword'\n",
    "        elif content.startswith(b'{\\\\'): \n",
    "            # RTF\n",
    "            return 'text/rtf'\n",
    "        \n",
    "        return libmagic_type\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_binary(content, content_type):\n",
    "    \"\"\"Modified to return more specific error information\"\"\"\n",
    "    if len(content) < 250:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        if content_type == 'application/pdf':\n",
    "            text = parse_pdf(content)\n",
    "            if not text or text.startswith('[PDF Content'):\n",
    "                # Try alternate PDF parsing method\n",
    "                with io.BytesIO(content) as pdf_file:\n",
    "                    try:\n",
    "                        with pdfplumber.open(pdf_file) as pdf:\n",
    "                            text = '\\n'.join(\n",
    "                                page.extract_text(x_tolerance=3, y_tolerance=3) \n",
    "                                for page in pdf.pages \n",
    "                                if page.extract_text(x_tolerance=3, y_tolerance=3)\n",
    "                            )\n",
    "                        if text.strip():\n",
    "                            return text\n",
    "                    except Exception as e:\n",
    "                        return f\"[PDF Content - pdfplumber Error: {str(e)}]\"\n",
    "            return text\n",
    "        elif content_type in ['application/msword', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document']:\n",
    "            return extract_text_from_doc(content)\n",
    "        elif content_type in ['application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:\n",
    "            return extract_text_from_excel(content)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return f\"[Binary Content - Extraction Error: {str(e)}]\"\n",
    "\n",
    "\n",
    "def extract_text_from_doc(content):\n",
    "    try:\n",
    "        return docx2txt.process(io.BytesIO(content))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_excel(content):\n",
    "    try:\n",
    "        workbook = load_workbook(io.BytesIO(content))\n",
    "        text = []\n",
    "        for sheet in workbook.sheetnames:\n",
    "            for row in workbook[sheet].iter_rows(values_only=True):\n",
    "                text.append(' '.join(str(cell) for cell in row if cell))\n",
    "        return '\\n'.join(text)\n",
    "    except:\n",
    "        try:\n",
    "            workbook = xlrd.open_workbook(file_contents=content)\n",
    "            text = []\n",
    "            for sheet in workbook.sheets():\n",
    "                for row in range(sheet.nrows):\n",
    "                    text.append(' '.join(str(cell.value) for cell in sheet.row(row)))\n",
    "            return '\\n'.join(text)\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "\n",
    "def parse_blob_content(content, provided_content_type=None):\n",
    "    try:\n",
    "        if not content:\n",
    "            return None, None, None\n",
    "        \n",
    "        # Enhanced content type detection\n",
    "        content_type = provided_content_type or enhanced_content_type_detection(content)\n",
    "\n",
    "        if content_type.startswith('image/') or content_type == 'application/zip':\n",
    "            return f\"[{content_type} Content]\", content_type, None\n",
    "\n",
    "        # Try to extract text based on content type\n",
    "        extracted_text = extract_text_from_binary(content, content_type)\n",
    "        if extracted_text:\n",
    "            return clean_text(extracted_text), content_type, 'utf-8'\n",
    "\n",
    "        # If text extraction failed, proceed with encoding detection and decoding\n",
    "        chardet_result = chardet.detect(content)\n",
    "        charset_normalizer_result = detect(content)\n",
    "        \n",
    "        encodings = [\n",
    "            chardet_result['encoding'],\n",
    "            charset_normalizer_result.get('encoding'),\n",
    "            'utf-8',\n",
    "            'iso-8859-1',\n",
    "            'windows-1252',\n",
    "            'ascii'\n",
    "        ]\n",
    "        \n",
    "        best_decoded = None\n",
    "        best_encoding = None\n",
    "        max_printable_ratio = 0.6\n",
    "\n",
    "        for encoding in encodings:\n",
    "            if encoding:\n",
    "                try:\n",
    "                    decoded = content.decode(encoding)\n",
    "                    printable_ratio = calculate_printable_ratio(decoded)\n",
    "                    if printable_ratio > max_printable_ratio:\n",
    "                        best_decoded = decoded\n",
    "                        best_encoding = encoding\n",
    "                        max_printable_ratio = printable_ratio\n",
    "                    if max_printable_ratio > 0.9:  # If we find a good enough encoding, stop searching\n",
    "                        break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "\n",
    "        # If we still haven't found a good decoding, try binary file processing\n",
    "        if max_printable_ratio <= 0.9 and len(content) > 250:\n",
    "            binary_content_types = [\n",
    "                'application/pdf',\n",
    "                'application/msword',\n",
    "                'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
    "                'application/vnd.ms-excel',\n",
    "                'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "                'application/rtf'\n",
    "            ]\n",
    "            \n",
    "            for binary_type in binary_content_types:\n",
    "                extracted_text = extract_text_from_binary(content, binary_type)\n",
    "                if extracted_text:\n",
    "                    printable_ratio = calculate_printable_ratio(extracted_text)\n",
    "                    if printable_ratio > max_printable_ratio:\n",
    "                        best_decoded = extracted_text\n",
    "                        best_encoding = 'utf-8'  # Assume UTF-8 for extracted text\n",
    "                        max_printable_ratio = printable_ratio\n",
    "                        content_type = binary_type\n",
    "                    if max_printable_ratio > 0.8:  # If we find a good enough extraction, stop searching\n",
    "                        break\n",
    "\n",
    "        if best_decoded is None:\n",
    "            return f\"[Binary data, unable to decode. Best printable ratio: {max_printable_ratio:.2f}]\", content_type, None\n",
    "\n",
    "        if content_type == \"text/rtf\":\n",
    "            return rtf_to_text(best_decoded), content_type, best_encoding\n",
    "        elif content_type in [\"text/html\", \"text/xml\", \"application/xhtml+xml\"]:\n",
    "            soup = BeautifulSoup(best_decoded, 'html.parser')\n",
    "            return soup.get_text(separator='\\n', strip=True), content_type, best_encoding\n",
    "        else:\n",
    "            return clean_text(best_decoded), content_type, best_encoding\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in parse_blob_content: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)  # This will print to the Spark logs\n",
    "        return error_msg, None, None\n",
    "\n",
    "def calculate_printable_ratio(text, sample_size=1000):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    if text.startswith(\"[Binary data, unable to decode.\"):\n",
    "        return 0.0\n",
    "    elif text == \"[PDF Content - Error extracting text]\":\n",
    "        return 0.0\n",
    "    \n",
    "    total_length = len(text)\n",
    "    if total_length <= sample_size:\n",
    "        # If the text is shorter than the sample size, use the entire text\n",
    "        sample = text\n",
    "    else:\n",
    "        # Take a random sample of characters\n",
    "        sample = ''.join(random.choice(text) for _ in range(sample_size))\n",
    "    \n",
    "    printable_count = 0\n",
    "    total_count = 0\n",
    "    for c in sample:\n",
    "        total_count += 1\n",
    "        if c in string.printable:\n",
    "            printable_count += 1\n",
    "    \n",
    "    return printable_count / total_count if total_count > 0 else 0.0\n",
    "\n",
    "def parse_pdf(content):\n",
    "    \"\"\"\n",
    "    Enhanced PDF parsing using pdfplumber with thorough error handling and detailed logging\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Track processing steps for debugging\n",
    "        processing_log = []\n",
    "        processing_log.append(\"Starting PDF parsing\")\n",
    "\n",
    "        # First try to repair the PDF if needed\n",
    "        repaired_content = repair_pdf(content)\n",
    "        if repaired_content:\n",
    "            content = repaired_content\n",
    "            processing_log.append(\"PDF repair successful\")\n",
    "        else:\n",
    "            processing_log.append(\"PDF repair not needed or failed\")\n",
    "\n",
    "        with io.BytesIO(content) as pdf_file:\n",
    "            try:\n",
    "                with pdfplumber.open(pdf_file) as pdf:\n",
    "                    extracted_text = []\n",
    "                    processing_log.append(f\"Successfully opened PDF with {len(pdf.pages)} pages\")\n",
    "                    \n",
    "                    for page_num, page in enumerate(pdf.pages, 1):\n",
    "                        processing_log.append(f\"Processing page {page_num}\")\n",
    "                        \n",
    "                        try:\n",
    "                            # Extract text with position information\n",
    "                            words = page.extract_words(\n",
    "                                x_tolerance=3,\n",
    "                                y_tolerance=3,\n",
    "                                keep_blank_chars=False,\n",
    "                                use_text_flow=False\n",
    "                            )\n",
    "                            \n",
    "                            if not words:\n",
    "                                processing_log.append(f\"No words found on page {page_num}\")\n",
    "                                continue\n",
    "                                \n",
    "                            # Sort words by vertical position first, then horizontal\n",
    "                            lines = []\n",
    "                            current_line_y = None\n",
    "                            line_words = []\n",
    "                            \n",
    "                            for word in words:\n",
    "                                if current_line_y is None or abs(float(word['top']) - float(current_line_y)) > 3:\n",
    "                                    if line_words:\n",
    "                                        line_words.sort(key=lambda w: float(w['x0']))\n",
    "                                        lines.append(line_words)\n",
    "                                    line_words = [word]\n",
    "                                    current_line_y = float(word['top'])\n",
    "                                else:\n",
    "                                    line_words.append(word)\n",
    "                            \n",
    "                            if line_words:\n",
    "                                line_words.sort(key=lambda w: float(w['x0']))\n",
    "                                lines.append(line_words)\n",
    "                            \n",
    "                            # Process each line\n",
    "                            for line in lines:\n",
    "                                line_text = ' '.join(word['text'] for word in line)\n",
    "                                if line_text.strip():\n",
    "                                    extracted_text.append(line_text)\n",
    "                            \n",
    "                            # Add page break if not the last page\n",
    "                            if page_num < len(pdf.pages):\n",
    "                                extracted_text.append(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "                                \n",
    "                        except Exception as page_error:\n",
    "                            processing_log.append(f\"Error processing page {page_num}: {str(page_error)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    final_text = '\\n'.join(extracted_text)\n",
    "                    if final_text.strip():\n",
    "                        processing_log.append(\"Successfully extracted text\")\n",
    "                        return final_text\n",
    "                    else:\n",
    "                        processing_log.append(\"No text content found in PDF\")\n",
    "                        return \"[PDF Content - Empty Document]\"\n",
    "                        \n",
    "            except Exception as pdf_error:\n",
    "                processing_log.append(f\"Error opening PDF: {str(pdf_error)}\")\n",
    "                # If the first attempt fails, try with the original content\n",
    "                if repaired_content:\n",
    "                    processing_log.append(\"Retrying with original content\")\n",
    "                    with pdfplumber.open(io.BytesIO(content)) as pdf:\n",
    "                        text = '\\n'.join(page.extract_text() for page in pdf.pages)\n",
    "                        if text.strip():\n",
    "                            return text\n",
    "                raise pdf_error\n",
    "\n",
    "    except Exception as e:\n",
    "        processing_log.append(f\"Fatal error: {str(e)}\")\n",
    "        print(\"PDF Processing Log:\")\n",
    "        for log in processing_log:\n",
    "            print(f\"  - {log}\")\n",
    "        return \"[PDF Content - Error extracting text]\"\n",
    "    \n",
    "        \n",
    "def repair_pdf(content):\n",
    "    \"\"\"\n",
    "    Attempt to repair corrupted PDF files using pdftk\n",
    "    \"\"\"\n",
    "    input_path = None\n",
    "    output_path = None\n",
    "    \n",
    "    try:\n",
    "        # Create temporary files\n",
    "        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as input_pdf:\n",
    "            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as output_pdf:\n",
    "                input_path = input_pdf.name\n",
    "                output_path = output_pdf.name\n",
    "\n",
    "        # Write content to temp file\n",
    "        with open(input_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        try:\n",
    "            # Run pdftk repair\n",
    "            result = subprocess.run(\n",
    "                ['pdftk', input_path, 'output', output_path],\n",
    "                capture_output=True,\n",
    "                timeout=300\n",
    "            )\n",
    "            \n",
    "            # Check process result\n",
    "            if result.returncode != 0:\n",
    "                print(f\"pdftk repair failed with return code {result.returncode}\")\n",
    "                print(f\"stderr: {result.stderr.decode('utf-8', errors='ignore')}\")\n",
    "                return None\n",
    "            \n",
    "            # Read repaired content\n",
    "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "                with open(output_path, 'rb') as f:\n",
    "                    return f.read()\n",
    "            else:\n",
    "                print(\"Repair failed: Output file is empty or missing\")\n",
    "                return None\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"pdftk repair timed out after 300 seconds\")\n",
    "            return None\n",
    "        except subprocess.SubprocessError as e:\n",
    "            print(f\"subprocess error running pdftk: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in repair_pdf: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup temp files\n",
    "        for path in [input_path, output_path]:\n",
    "            if path and os.path.exists(path):\n",
    "                try:\n",
    "                    os.unlink(path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cleaning up temporary file {path}: {str(e)}\")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r'<%.*?%>', '', text)\n",
    "    cleaned = cleaned.replace('|', '\\n')\n",
    "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
    "    cleaned = re.sub(r'\\n+', '\\n', cleaned)  # Remove multiple consecutive newlines\n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "def clean_non_printable(text):\n",
    "    # Remove two or more consecutive non-printable characters\n",
    "    cleaned = re.sub(r'[^\\x20-\\x7E]{2,}', '\\n', text)\n",
    "    # Replace single non-printable characters with a space\n",
    "    cleaned = ''.join(char if char in string.printable else ' ' for char in cleaned)\n",
    "    # Remove extra spaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd3c1adf-8563-422d-a9d0-f1303c5c72af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "output_schema = StructType([\n",
    "    StructField(\"EVENT_ID\", LongType(), True),\n",
    "    StructField(\"VALID_UNTIL_DT_TM\", TimestampType(), True),\n",
    "    StructField(\"VALID_FROM_DT_TM\", TimestampType(), True),\n",
    "    StructField(\"UPDT_DT_TM\", TimestampType(), True),\n",
    "    StructField(\"UPDT_ID\", LongType(), True),\n",
    "    StructField(\"UPDT_TASK\", LongType(), True),\n",
    "    StructField(\"UPDT_CNT\", LongType(), True),\n",
    "    StructField(\"UPDT_APPLCTX\", LongType(), True),\n",
    "    StructField(\"LAST_UTC_TS\", TimestampType(), True),\n",
    "    StructField(\"ADC_UPDT\", TimestampType(), True),\n",
    "    StructField(\"BLOB_BINARY\", BinaryType(), True),\n",
    "    StructField(\"CONTENT_TYPE\", StringType(), True),\n",
    "    StructField(\"ENCODING\", StringType(), True),\n",
    "    StructField(\"BLOB_TEXT\", StringType(), True),\n",
    "    StructField(\"BINARY_SIZE\", LongType(), True),\n",
    "    StructField(\"TEXT_LENGTH\", LongType(), True),\n",
    "    StructField(\"STATUS\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Global set to store failed EVENT_IDs\n",
    "failed_event_ids = set()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_ocf_wrapper(blob_contents):\n",
    "      \"\"\"Remove OCF wrapper from any content type\"\"\"\n",
    "      try:\n",
    "        ocf_marker = b'ocf_blob\\0'\n",
    "        \n",
    "        # Handle case where marker occurs at end\n",
    "        if blob_contents.endswith(ocf_marker):\n",
    "            blob_contents = blob_contents[:-len(ocf_marker)]\n",
    "            \n",
    "        # Handle case where marker might occur multiple times\n",
    "        # Split on marker and rejoin, effectively removing all instances\n",
    "        if ocf_marker in blob_contents:\n",
    "            parts = blob_contents.split(ocf_marker)\n",
    "            blob_contents = b''.join(parts)\n",
    "            \n",
    "        return blob_contents\n",
    "      except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def process_single_row(row, row_timeout=120):\n",
    "    try:\n",
    "        total_blob_length = row['TOTAL_BLOB_LENGTH']\n",
    "        if total_blob_length > 8 * 1024 * 1024:  # 8 MB\n",
    "            status = f\"File Too Large: {format_size(total_blob_length)}\"\n",
    "            return create_result_dict(row, status=status)\n",
    "        \n",
    "        # Combine chunks\n",
    "        blob_contents = combine_blob_chunks(row['BLOB_CONTENTS_LIST'])\n",
    "        \n",
    "        # Decompress if needed\n",
    "        decompressed = decompress_blob(blob_contents, row['COMPRESSION_CD'])\n",
    "        if isinstance(decompressed, str):\n",
    "            return create_result_dict(row, status=decompressed)\n",
    "            \n",
    "        if decompressed is not None:\n",
    "            # Remove OCF wrapper if present\n",
    "            cleaned_content = remove_ocf_wrapper(decompressed)\n",
    "            if cleaned_content is not None:\n",
    "                decompressed = cleaned_content\n",
    "            \n",
    "            # Detect content type efficiently\n",
    "            content_type = enhanced_content_type_detection(decompressed)\n",
    "            \n",
    "            # For RTF files, use optimized handling with timeout protection\n",
    "            if content_type == \"text/rtf\":\n",
    "                try:\n",
    "                    with timeout(60):  # 60-second timeout for RTF processing\n",
    "                        # Decode with a simpler method first\n",
    "                        content_str = decompressed.decode('latin-1', errors='ignore')\n",
    "                        \n",
    "                        # Try simplified extraction first (much faster)\n",
    "                        blob_text = None\n",
    "                        try:\n",
    "                            # Simple non-regex text extraction\n",
    "                            text = ''\n",
    "                            in_control_word = False\n",
    "                            in_brace = 0\n",
    "                            for c in content_str[:1000000]:  # Process first 1MB max\n",
    "                                if c == '\\\\':\n",
    "                                    in_control_word = True\n",
    "                                elif in_control_word and c.isspace():\n",
    "                                    in_control_word = False\n",
    "                                    text += ' '\n",
    "                                elif not in_control_word and c == '{':\n",
    "                                    in_brace += 1\n",
    "                                elif not in_control_word and c == '}':\n",
    "                                    in_brace -= 1\n",
    "                                elif not in_control_word and not c.isspace() and in_brace >= 0:\n",
    "                                    text += c\n",
    "                            \n",
    "                            blob_text = text.strip()\n",
    "                            if len(blob_text) < 100:  # If we got very little text, try the standard method\n",
    "                                blob_text = rtf_to_text(content_str)\n",
    "                        except:\n",
    "                            # Fall back to standard method\n",
    "                            blob_text = rtf_to_text(content_str)\n",
    "                            \n",
    "                        return create_result_dict(row, decompressed, content_type, 'latin-1', \n",
    "                                                blob_text, status='Decoded')\n",
    "                except ThreadTimeoutError:\n",
    "                    # If RTF processing times out, use a very simple extraction\n",
    "                    try:\n",
    "                        # Emergency fallback - just grab anything that looks like text\n",
    "                        import re\n",
    "                        content_str = decompressed.decode('latin-1', errors='ignore')\n",
    "                        text = re.sub(r'[^\\x20-\\x7E\\n]', ' ', content_str)\n",
    "                        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                        return create_result_dict(row, decompressed, content_type, 'latin-1',\n",
    "                                                text, status='Decoded (fallback method)')\n",
    "                    except:\n",
    "                        return create_result_dict(row, decompressed, content_type, None, \n",
    "                                                \"[RTF processing timed out]\", status='Timeout')\n",
    "            \n",
    "            # Handle other content types\n",
    "            blob_text, detected_type, encoding = parse_blob_content(decompressed, content_type)\n",
    "            \n",
    "            # Use detected content type if available\n",
    "            if detected_type:\n",
    "                content_type = detected_type\n",
    "                \n",
    "            if blob_text:\n",
    "                if isinstance(blob_text, str) and blob_text.startswith(\"[\"):\n",
    "                    status = blob_text\n",
    "                else:\n",
    "                    status = 'Decoded'\n",
    "            else:\n",
    "                status = 'Failed to decode'\n",
    "                \n",
    "            return create_result_dict(row, decompressed, content_type, encoding, blob_text, status)\n",
    "        else:\n",
    "            status = \"Decompression returned None\"\n",
    "            return create_result_dict(row, None, None, None, None, status)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return create_result_dict(row, status=f\"Error: {str(e)}\")\n",
    "    \n",
    "    \n",
    "def create_result_dict(row, decompressed=None, content_type=None, encoding=None, blob_text=None, status=\"Error\"):\n",
    "    \"\"\"Fixed version to handle all type edge cases\"\"\"\n",
    "    # Ensure status is always a string\n",
    "    if not isinstance(status, str):\n",
    "        status = str(status)\n",
    "        \n",
    "    # Safe string encoding with better type checking\n",
    "    def safe_encode(text):\n",
    "        if isinstance(text, str):\n",
    "            return text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "        elif text is not None:\n",
    "            try:\n",
    "                return str(text)\n",
    "            except:\n",
    "                return \"[Non-string content]\"\n",
    "        return text\n",
    "    \n",
    "    # Handle edge cases for numeric fields\n",
    "    event_id = row['EVENT_ID']\n",
    "    updt_id = row['UPDT_ID']\n",
    "    updt_task = row['UPDT_TASK'] \n",
    "    updt_cnt = row['UPDT_CNT']\n",
    "    updt_applctx = row['UPDT_APPLCTX']\n",
    "    \n",
    "    # Calculate sizes safely\n",
    "    binary_size = None\n",
    "    if decompressed is not None:\n",
    "        try:\n",
    "            binary_size = len(decompressed)\n",
    "        except:\n",
    "            binary_size = -1  # Use a sentinel value for error\n",
    "    \n",
    "    text_length = None\n",
    "    if blob_text is not None:\n",
    "        try:\n",
    "            text_length = len(blob_text)\n",
    "        except:\n",
    "            text_length = -1  # Use a sentinel value for error\n",
    "    \n",
    "    return {\n",
    "        \"EVENT_ID\": event_id,\n",
    "        \"VALID_UNTIL_DT_TM\": row['VALID_UNTIL_DT_TM'],\n",
    "        \"VALID_FROM_DT_TM\": row['VALID_FROM_DT_TM'],\n",
    "        \"UPDT_DT_TM\": row['UPDT_DT_TM'],\n",
    "        \"UPDT_ID\": updt_id,\n",
    "        \"UPDT_TASK\": updt_task,\n",
    "        \"UPDT_CNT\": updt_cnt,\n",
    "        \"UPDT_APPLCTX\": updt_applctx,\n",
    "        \"LAST_UTC_TS\": row['LAST_UTC_TS'],\n",
    "        \"ADC_UPDT\": row['ADC_UPDT'],\n",
    "        \"BLOB_BINARY\": decompressed,\n",
    "        \"CONTENT_TYPE\": content_type,\n",
    "        \"ENCODING\": encoding,\n",
    "        \"BLOB_TEXT\": safe_encode(blob_text),\n",
    "        \"BINARY_SIZE\": binary_size,\n",
    "        \"TEXT_LENGTH\": text_length,\n",
    "        \"STATUS\": status\n",
    "    }\n",
    "\n",
    "\n",
    "# Define a pandas UDF to process rows\n",
    "@pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "def process_rows(pdf):\n",
    "    results = []\n",
    "    global failed_event_ids\n",
    "    \n",
    "    for _, row in pdf.iterrows():\n",
    "        try:\n",
    "            result = process_single_row(row)\n",
    "            status = result['STATUS']\n",
    "            if isinstance(status, str) and (status.startswith('Error:') or status.startswith('[')):\n",
    "                failed_event_ids.add(row['EVENT_ID'])\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            failed_event_ids.add(row['EVENT_ID'])\n",
    "            results.append(create_result_dict(row, status=f\"Error: Unexpected - {str(e)}\"))\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a51743f-d932-49db-a7da-61fb8f9886ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ThreadTimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "@contextmanager\n",
    "def timeout(seconds):\n",
    "    \"\"\"Thread-based timeout context manager that works in PySpark workers\"\"\"\n",
    "    timer = None\n",
    "    \n",
    "    def raise_timeout():\n",
    "        thread_id = threading.current_thread().ident\n",
    "        for thread in threading.enumerate():\n",
    "            if thread.ident == thread_id:\n",
    "                raise ThreadTimeoutError(f\"Timed out after {seconds} seconds\")\n",
    "    \n",
    "    try:\n",
    "        timer = threading.Timer(seconds, raise_timeout)\n",
    "        timer.start()\n",
    "        yield\n",
    "    finally:\n",
    "        if timer:\n",
    "            timer.cancel()\n",
    "\n",
    "def process_and_write_data(data, mode=\"append\", retry_count=0, batch_timeout=3600):  # 60 minutes timeout\n",
    "    import gc\n",
    "\n",
    "    gc.collect()\n",
    "    global failed_event_ids\n",
    "    try:\n",
    "        # Remove previously failed EVENT_IDs\n",
    "        if failed_event_ids:\n",
    "            data = data.filter(~F.col(\"EVENT_ID\").isin(list(failed_event_ids)))\n",
    "        \n",
    "        if data.count() == 0:\n",
    "            print(\"No data to process after removing failed EVENT_IDs.\")\n",
    "            return True\n",
    "\n",
    "        with timeout(batch_timeout):\n",
    "            # Process the data\n",
    "            processed_df = data.groupby(\"EVENT_ID\").apply(process_rows)\n",
    "            \n",
    "            # Write the processed data\n",
    "            processed_df.write.mode(mode).insertInto(target_table_name)\n",
    "        gc.collect()\n",
    "        return True\n",
    "    except ThreadTimeoutError:\n",
    "        print(f\"Batch processing timed out after {batch_timeout} seconds\")\n",
    "        # Here you might want to add logic to handle the timeout,\n",
    "        # such as splitting the batch or marking it for later processing\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {str(e)}\")\n",
    "        if retry_count < 3 and len(failed_event_ids) > 0:\n",
    "            print(f\"Retrying without {len(failed_event_ids)} failed EVENT_IDs. Retry count: {retry_count + 1}\")\n",
    "            return process_and_write_data(data, mode=\"append\", retry_count=retry_count + 1)\n",
    "        else:\n",
    "            print(\"Max retries reached. Some data could not be processed.\")\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "063008f7-c157-4a62-be18-a7fa4b6d164d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the target table exists and get the maximum ADC_UPDT\n",
    "target_table_name = \"4_prod.bronze.mill_blob_text\"\n",
    "max_adc_updt = get_max_adc_updt(target_table_name)\n",
    "\n",
    "# Define batch sizes and their corresponding timeouts\n",
    "batch_configs = [\n",
    "    {\"size\": 2048 * 1024 * 1024, \"timeout\": 3600},  # 2GB, 60 minutes\n",
    "    {\"size\": 64 * 1024 * 1024, \"timeout\": 600},     # 64 MB, 10 minutes\n",
    "    {\"size\": 1 * 1024 * 1024, \"timeout\": 300},      # 1 MB, 5 minutes\n",
    "    {\"size\": 1, \"timeout\": 120}                     # Single record, 2 minutes\n",
    "]\n",
    "\n",
    "# Get new data that's newer than the latest ADC_UPDT in the target table\n",
    "print(f\"\\nProcessing data newer than ADC_UPDT: {max_adc_updt}\")\n",
    "df = spark.table(\"4_prod.raw.mill_ce_blob\").filter(\n",
    "    F.col(\"ADC_UPDT\") > F.lit(max_adc_updt)\n",
    ")\n",
    "\n",
    "# If no new data, exit\n",
    "if df.count() == 0:\n",
    "    print(\"No new data to process\")\n",
    "    exit(0)\n",
    "\n",
    "# Group and prepare the data\n",
    "grouped_df = df.groupBy(\"EVENT_ID\", \"COMPRESSION_CD\").agg(\n",
    "    F.sort_array(\n",
    "        F.collect_list(\n",
    "            F.struct(\"BLOB_SEQ_NUM\", \"BLOB_CONTENTS\")\n",
    "        )\n",
    "    ).getField(\"BLOB_CONTENTS\").alias(\"BLOB_CONTENTS_LIST\"),\n",
    "    F.sum(F.col(\"BLOB_LENGTH\").cast(\"bigint\")).alias(\"TOTAL_BLOB_LENGTH\"),\n",
    "    F.count(\"*\").alias(\"ROW_COUNT\"),\n",
    "    F.first(\"VALID_UNTIL_DT_TM\").alias(\"VALID_UNTIL_DT_TM\"),\n",
    "    F.first(\"VALID_FROM_DT_TM\").alias(\"VALID_FROM_DT_TM\"),\n",
    "    F.first(\"UPDT_DT_TM\").alias(\"UPDT_DT_TM\"),\n",
    "    F.first(\"UPDT_ID\").alias(\"UPDT_ID\"),\n",
    "    F.first(\"UPDT_TASK\").alias(\"UPDT_TASK\"),\n",
    "    F.first(\"UPDT_CNT\").alias(\"UPDT_CNT\"),\n",
    "    F.first(\"UPDT_APPLCTX\").alias(\"UPDT_APPLCTX\"),\n",
    "    F.first(\"LAST_UTC_TS\").alias(\"LAST_UTC_TS\"),\n",
    "    F.first(\"ADC_UPDT\").alias(\"ADC_UPDT\")\n",
    ")\n",
    "\n",
    "# Collect all EVENT_IDs that need processing, ordered by ADC_UPDT\n",
    "size_df = grouped_df.select(\n",
    "    \"EVENT_ID\", \n",
    "    \"TOTAL_BLOB_LENGTH\", \n",
    "    \"ROW_COUNT\",\n",
    "    \"ADC_UPDT\"\n",
    ").orderBy(\"ADC_UPDT\", \"EVENT_ID\").collect()\n",
    "\n",
    "remaining_events = {row['EVENT_ID'] for row in size_df}\n",
    "\n",
    "# Process the data in batches\n",
    "for batch_config in batch_configs:\n",
    "    target_batch_size = batch_config[\"size\"]\n",
    "    batch_timeout = batch_config[\"timeout\"]\n",
    "\n",
    "    if not remaining_events:\n",
    "        break\n",
    "    \n",
    "    print(f\"\\nProcessing with batch size: {target_batch_size/1024/1024:.2f} MB (timeout: {batch_timeout} seconds)\")\n",
    "\n",
    "    # Convert remaining_events set to list for current batch size processing\n",
    "    current_size_df = [row for row in size_df if row['EVENT_ID'] in remaining_events]\n",
    "\n",
    "    # Process all remaining events with current batch configuration\n",
    "    current_batch = []\n",
    "    current_batch_size = 0\n",
    "    current_batch_row_count = 0\n",
    "    batch_number = 1\n",
    "\n",
    "    for row in current_size_df:\n",
    "        event_id = row['EVENT_ID']\n",
    "        blob_length = row['TOTAL_BLOB_LENGTH']\n",
    "        row_count = row['ROW_COUNT']\n",
    "        \n",
    "        # Cap large blobs at 8MB for batch size calculation\n",
    "        if blob_length > 8 * 1024 * 1024:\n",
    "            effective_blob_length = 1\n",
    "        else:\n",
    "            effective_blob_length = blob_length\n",
    "\n",
    "        current_batch.append(event_id)\n",
    "        current_batch_size += effective_blob_length\n",
    "        current_batch_row_count += row_count\n",
    "\n",
    "        # Process batch when size limit is reached\n",
    "        if (len(current_batch) > 1 and current_batch_size + effective_blob_length > target_batch_size) or len(current_batch) >= 100000:\n",
    "            print(f\"Processing batch {batch_number} with {len(current_batch)} EVENT_IDs\")\n",
    "            print(f\"Batch size: {current_batch_size/1048576:.2f} MB, {current_batch_row_count} rows\")\n",
    "        \n",
    "            start_time_batch = time.time()\n",
    "            batch_df = grouped_df.filter(F.col(\"EVENT_ID\").isin(current_batch))\n",
    "        \n",
    "            if process_and_write_data(batch_df, mode=\"append\", batch_timeout=batch_timeout):\n",
    "                remaining_events -= set(current_batch)\n",
    "            else:\n",
    "                print(f\"Batch {batch_number} failed or timed out. Will try with smaller batch size.\")\n",
    "        \n",
    "            end_time_batch = time.time()\n",
    "            print(f\"Batch duration: {end_time_batch - start_time_batch:.2f} seconds\")\n",
    "        \n",
    "            batch_number += 1\n",
    "            current_batch = []\n",
    "            current_batch_size = 0\n",
    "            current_batch_row_count = 0\n",
    "\n",
    "    # Process final batch for current batch size if needed\n",
    "    if current_batch:\n",
    "        print(f\"Processing final batch {batch_number} with {len(current_batch)} EVENT_IDs\")\n",
    "        print(f\"Batch size: {current_batch_size/1048576:.2f} MB, {current_batch_row_count} rows\")\n",
    "    \n",
    "        batch_df = grouped_df.filter(F.col(\"EVENT_ID\").isin(current_batch))\n",
    "        if process_and_write_data(batch_df, mode=\"append\", batch_timeout=batch_timeout):\n",
    "            remaining_events -= set(current_batch)\n",
    "\n",
    "print(f\"\\nProcessing complete.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Blob Decompression",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
