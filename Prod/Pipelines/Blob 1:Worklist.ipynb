{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d362e10f-8e7a-47e4-9f0d-1fe2c542c0c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook 1: Create Worklist with Data (CORRECTED v2)\n",
    "# Creates a worklist table with actual blob data for efficient processing\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "SOURCE_TABLE = \"4_prod.raw.mill_ce_blob\"\n",
    "TARGET_TABLE = \"4_prod.bronze.mill_blob_text\"\n",
    "STAGING_DB = \"4_prod.tmp\"\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "WORKLIST_TABLE = f\"{STAGING_DB}.blob_worklist_{RUN_ID}\"\n",
    "MAX_BLOB_SIZE = 16 * 1024 * 1024  # 16 MB\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"CREATING WORKLIST: {WORKLIST_TABLE}\")\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Get events not in target\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Identifying new events...\")\n",
    "new_events = (spark.table(SOURCE_TABLE)\n",
    "              .select(\"EVENT_ID\", \"ADC_UPDT\")\n",
    "              .distinct()\n",
    "              .join(spark.table(TARGET_TABLE).select(\"EVENT_ID\").distinct(), \n",
    "                    on=\"EVENT_ID\", how=\"left_anti\")).limit(250000)\n",
    "\n",
    "new_event_count = new_events.count()\n",
    "print(f\"Found {new_event_count:,} events to process\")\n",
    "\n",
    "if new_event_count == 0:\n",
    "    print(\"No new events to process!\")\n",
    "    dbutils.notebook.exit(\"NO_WORK\")\n",
    "\n",
    "# Step 2: Create worklist with actual data and metadata\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Building worklist with blob data...\")\n",
    "\n",
    "# Define columns we need\n",
    "META_AND_BLOB_COLS = [\n",
    "    \"EVENT_ID\", \"BLOB_SEQ_NUM\",\n",
    "    \"VALID_UNTIL_DT_TM\", \"VALID_FROM_DT_TM\",\n",
    "    \"UPDT_DT_TM\", \"UPDT_ID\", \"UPDT_TASK\", \"UPDT_CNT\", \"UPDT_APPLCTX\",\n",
    "    \"LAST_UTC_TS\", \"ADC_UPDT\", \"COMPRESSION_CD\", \"BLOB_CONTENTS\", \"BLOB_LENGTH\"\n",
    "]\n",
    "\n",
    "# Load source data for new events\n",
    "source_data = (spark.table(SOURCE_TABLE)\n",
    "               .join(F.broadcast(new_events), on=[\"EVENT_ID\", \"ADC_UPDT\"], how=\"inner\")\n",
    "               .select(*META_AND_BLOB_COLS))\n",
    "\n",
    "# Deduplicate using window function\n",
    "w_temporal = Window.partitionBy(\"EVENT_ID\", \"BLOB_SEQ_NUM\").orderBy(\n",
    "    F.col(\"VALID_UNTIL_DT_TM\").desc(),\n",
    "    F.col(\"UPDT_DT_TM\").desc(),\n",
    "    F.col(\"LAST_UTC_TS\").desc()\n",
    ")\n",
    "\n",
    "deduped_data = (source_data\n",
    "                .withColumn(\"version_rank\", F.row_number().over(w_temporal))\n",
    "                .filter(F.col(\"version_rank\") == 1)\n",
    "                .drop(\"version_rank\"))\n",
    "\n",
    "# Add metadata columns - Note: we're including ADC_UPDT in the struct now\n",
    "worklist_with_meta = (deduped_data\n",
    "    .withColumn(\"chunk_size\", F.coalesce(F.col(\"BLOB_LENGTH\").cast(\"long\"), F.lit(0)))\n",
    "    .groupBy(\"EVENT_ID\", \"ADC_UPDT\")\n",
    "    .agg(\n",
    "        F.sum(\"chunk_size\").alias(\"compressed_size\"),\n",
    "        F.count(\"*\").alias(\"chunk_count\"),\n",
    "        # Include all columns in the struct (including ADC_UPDT for each chunk)\n",
    "        F.collect_list(F.struct(*[c for c in META_AND_BLOB_COLS if c != \"EVENT_ID\"])).alias(\"chunks_data\")\n",
    "    )\n",
    "    .withColumn(\"status\", \n",
    "                F.when(F.col(\"compressed_size\") > MAX_BLOB_SIZE, \"oversized\")\n",
    "                .otherwise(\"pending\"))\n",
    "    .withColumn(\"batch_num\", F.lit(0))\n",
    "    .withColumn(\"process_ts\", F.lit(None).cast(\"timestamp\"))\n",
    "    .withColumn(\"error_msg\", F.lit(None).cast(\"string\")))\n",
    "\n",
    "# Write worklist table\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Writing worklist table...\")\n",
    "worklist_with_meta.write.mode(\"overwrite\").saveAsTable(WORKLIST_TABLE)\n",
    "\n",
    "# Create index for efficient querying\n",
    "spark.sql(f\"OPTIMIZE {WORKLIST_TABLE} ZORDER BY (ADC_UPDT, EVENT_ID)\")\n",
    "\n",
    "# Get statistics\n",
    "stats = spark.table(WORKLIST_TABLE).groupBy(\"status\").count().collect()\n",
    "stats_dict = {row[\"status\"]: row[\"count\"] for row in stats}\n",
    "\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Worklist created successfully:\")\n",
    "print(f\"  - Pending: {stats_dict.get('pending', 0):,}\")\n",
    "print(f\"  - Oversized: {stats_dict.get('oversized', 0):,}\")\n",
    "print(f\"  - Total chunks stored: {spark.table(WORKLIST_TABLE).select(F.sum(F.col('chunk_count'))).collect()[0][0]:,}\")\n",
    "\n",
    "# Write oversized events directly to target\n",
    "if stats_dict.get('oversized', 0) > 0:\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Writing {stats_dict.get('oversized', 0)} oversized events to target...\")\n",
    "    \n",
    "    # CORRECTED v2: Simpler approach - just take first chunk for metadata\n",
    "    oversized = (spark.table(WORKLIST_TABLE)\n",
    "                 .filter(F.col(\"status\") == \"oversized\")\n",
    "                 .select(\n",
    "                     \"EVENT_ID\",\n",
    "                     \"ADC_UPDT\",  # Keep ADC_UPDT from worklist level\n",
    "                     \"compressed_size\",\n",
    "                     F.col(\"chunks_data\")[0].alias(\"first_chunk\")  # Just get first chunk for metadata\n",
    "                 ))\n",
    "    \n",
    "    # Now extract fields from first_chunk\n",
    "    oversized_output = oversized.select(\n",
    "        \"EVENT_ID\",\n",
    "        F.col(\"first_chunk.VALID_UNTIL_DT_TM\").alias(\"VALID_UNTIL_DT_TM\"),\n",
    "        F.col(\"first_chunk.VALID_FROM_DT_TM\").alias(\"VALID_FROM_DT_TM\"),\n",
    "        F.col(\"first_chunk.UPDT_DT_TM\").alias(\"UPDT_DT_TM\"),\n",
    "        F.col(\"first_chunk.UPDT_ID\").cast(\"long\").alias(\"UPDT_ID\"),\n",
    "        F.col(\"first_chunk.UPDT_TASK\").cast(\"long\").alias(\"UPDT_TASK\"),\n",
    "        F.col(\"first_chunk.UPDT_CNT\").cast(\"long\").alias(\"UPDT_CNT\"),\n",
    "        F.col(\"first_chunk.UPDT_APPLCTX\").cast(\"long\").alias(\"UPDT_APPLCTX\"),\n",
    "        F.col(\"first_chunk.LAST_UTC_TS\").alias(\"LAST_UTC_TS\"),\n",
    "        \"ADC_UPDT\",  # Use ADC_UPDT from worklist level\n",
    "        F.lit(None).cast(\"binary\").alias(\"BLOB_BINARY\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"CONTENT_TYPE\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"ENCODING\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"BLOB_TEXT\"),\n",
    "        F.col(\"compressed_size\").alias(\"BINARY_SIZE\"),\n",
    "        F.lit(None).cast(\"long\").alias(\"TEXT_LENGTH\"),\n",
    "        F.concat(F.lit(\"Compressed Too Large: \"), F.col(\"compressed_size\"), F.lit(\" bytes\")).alias(\"STATUS\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"anon_text\")\n",
    "    )\n",
    "    \n",
    "    oversized_output.write.mode(\"append\").insertInto(TARGET_TABLE)\n",
    "    print(f\"  Wrote {stats_dict.get('oversized', 0)} oversized events to target\")\n",
    "\n",
    "# Output worklist table name for next notebook\n",
    "print(f\"\\nWorklist table: {WORKLIST_TABLE}\")\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "\n",
    "# Create metadata table for pipeline coordination\n",
    "# Create metadata table for pipeline coordination\n",
    "METADATA_TABLE = f\"{STAGING_DB}.pipeline_metadata\"\n",
    "\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Recreating metadata table...\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {METADATA_TABLE}\")\n",
    "\n",
    "# Create or update metadata table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {METADATA_TABLE} (\n",
    "        run_id STRING,\n",
    "        worklist_table STRING,\n",
    "        total_events INT,\n",
    "        pending_events INT,\n",
    "        oversized_events INT,\n",
    "        created_ts TIMESTAMP,\n",
    "        completed_ts TIMESTAMP,\n",
    "        merged_ts TIMESTAMP,\n",
    "        status STRING,\n",
    "        batch_tables STRING,\n",
    "        processed_events INT\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Insert metadata using SQL\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {METADATA_TABLE}\n",
    "    VALUES (\n",
    "        '{RUN_ID}',\n",
    "        '{WORKLIST_TABLE}',\n",
    "        {new_event_count},\n",
    "        {stats_dict.get('pending', 0)},\n",
    "        {stats_dict.get('oversized', 0)},\n",
    "        current_timestamp(),\n",
    "        NULL,\n",
    "        NULL,\n",
    "        'worklist_created',\n",
    "        NULL,\n",
    "        NULL\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nPipeline metadata saved to: {METADATA_TABLE}\")\n",
    "print(f\"Run ID for next notebooks: {RUN_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06032616-99cf-44aa-a139-214e2fab6e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Blob 1:Worklist",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
