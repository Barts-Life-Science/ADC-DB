{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d362e10f-8e7a-47e4-9f0d-1fe2c542c0c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook 1: Create Worklist with Data - CDF Incremental with Trust Filter\n",
    "# Creates a worklist table with actual blob data for efficient processing\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "SOURCE_TABLE = \"4_prod.raw.mill_ce_blob\"\n",
    "TARGET_TABLE = \"4_prod.bronze.mill_blob_text\"\n",
    "STAGING_DB = \"4_prod.tmp\"\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "WORKLIST_TABLE = f\"{STAGING_DB}.blob_worklist_{RUN_ID}\"\n",
    "MAX_BLOB_SIZE = 16 * 1024 * 1024  # 16 MB\n",
    "USE_CDF = True  # Enable CDF\n",
    "RUN_TS = datetime.now()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"CREATING WORKLIST: {WORKLIST_TABLE}\")\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#=========================\n",
    "# Helper Functions\n",
    "#=========================\n",
    "\n",
    "def get_last_processed_timestamp(target_table: str):\n",
    "    \"\"\"Get the last ADC_UPDT timestamp from target table with 10-minute safety margin\"\"\"\n",
    "    try:\n",
    "        result = spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX(ADC_UPDT), timestamp('1970-01-01T00:00:00Z')) AS max_ts\n",
    "            FROM {target_table}\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        if result and result[0]['max_ts']:\n",
    "            max_ts = result[0]['max_ts']\n",
    "            # Apply 10-minute safety margin to catch any late-arriving updates\n",
    "            safe_ts = spark.sql(f\"\"\"\n",
    "                SELECT timestamp('{max_ts}') - INTERVAL 10 MINUTES AS safe_ts\n",
    "            \"\"\").collect()[0]['safe_ts']\n",
    "            return safe_ts\n",
    "        else:\n",
    "            return datetime(1970, 1, 1)\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not get last timestamp from target: {str(e)}\")\n",
    "        return datetime(1970, 1, 1)\n",
    "\n",
    "def get_changes_via_cdf(source_table: str, start_ts, trust_filter: str = \"Barts\"):\n",
    "    \"\"\"\n",
    "    Try to get changed EVENT_IDs using CDF.\n",
    "    Returns DataFrame with (EVENT_ID, ADC_UPDT) or None if CDF fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  Attempting CDF query from {start_ts}...\")\n",
    "        \n",
    "        # Get latest version\n",
    "        history = spark.sql(f\"DESCRIBE HISTORY {source_table}\")\n",
    "        end_version = history.selectExpr(\"MAX(version) as v\").collect()[0]['v']\n",
    "        \n",
    "        if end_version is None:\n",
    "            print(\"  No history found, falling back to time-window\")\n",
    "            return None\n",
    "        \n",
    "        # Find start version corresponding to timestamp\n",
    "        start_version_row = spark.sql(f\"\"\"\n",
    "            SELECT MIN(version) AS v\n",
    "            FROM (DESCRIBE HISTORY {source_table})\n",
    "            WHERE timestamp >= timestamp('{start_ts}')\n",
    "        \"\"\").collect()[0]\n",
    "        \n",
    "        start_version = start_version_row['v'] if start_version_row and start_version_row['v'] is not None else end_version\n",
    "        \n",
    "        print(f\"  CDF version range: {start_version} to {end_version}\")\n",
    "        \n",
    "        # Detect schema change boundaries\n",
    "        boundary_rows = spark.sql(f\"\"\"\n",
    "            SELECT version\n",
    "            FROM (DESCRIBE HISTORY {source_table})\n",
    "            WHERE version BETWEEN {start_version} AND {end_version}\n",
    "              AND (\n",
    "                    operation IN ('SET TBLPROPERTIES','REPLACE','REPLACE TABLE',\n",
    "                                  'ADD COLUMNS','CHANGE COLUMN','ALTER TABLE ADD COLUMNS',\n",
    "                                  'ALTER TABLE CHANGE COLUMN','CONVERT TO DELTA','RESTORE')\n",
    "                 OR instr(upper(CAST(operationParameters AS STRING)), 'COLUMNMAPPING') > 0\n",
    "                 OR instr(upper(CAST(operationParameters AS STRING)), 'SCHEMA') > 0\n",
    "              )\n",
    "            ORDER BY version\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        boundaries = [r['version'] for r in boundary_rows]\n",
    "        \n",
    "        # Build segments to avoid schema change issues\n",
    "        edges = [start_version] + boundaries + [end_version + 1]\n",
    "        segments = []\n",
    "        \n",
    "        for i in range(len(edges) - 1):\n",
    "            s_ver = edges[i]\n",
    "            e_ver = edges[i+1] - 1\n",
    "            if s_ver <= e_ver:\n",
    "                segments.append(f\"\"\"\n",
    "                    SELECT EVENT_ID, ADC_UPDT, _commit_timestamp\n",
    "                    FROM table_changes('{source_table}', {s_ver}, {e_ver})\n",
    "                    WHERE _change_type IN ('insert', 'update_postimage')\n",
    "                      AND Trust = '{trust_filter}'\n",
    "                \"\"\")\n",
    "        \n",
    "        if not segments:\n",
    "            print(\"  No CDF segments to read, falling back to time-window\")\n",
    "            return None\n",
    "        \n",
    "        # Union all segments\n",
    "        union_sql = \" UNION ALL \".join(segments)\n",
    "        cdf_query = f\"\"\"\n",
    "            SELECT EVENT_ID, ADC_UPDT, MAX(_commit_timestamp) AS _ch_ts\n",
    "            FROM ({union_sql}) ch\n",
    "            GROUP BY EVENT_ID, ADC_UPDT\n",
    "        \"\"\"\n",
    "        \n",
    "        changes_df = spark.sql(cdf_query).select(\"EVENT_ID\", \"ADC_UPDT\").distinct()\n",
    "        change_count = changes_df.count()\n",
    "        print(f\"  CDF found {change_count:,} changed EVENT_IDs\")\n",
    "        \n",
    "        return changes_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  CDF query failed: {str(e)[:500]}\")\n",
    "        print(\"  Falling back to time-window scan\")\n",
    "        return None\n",
    "\n",
    "def get_changes_via_timewindow(source_table: str, start_ts, trust_filter: str = \"Barts\"):\n",
    "    \"\"\"\n",
    "    Fallback: Get changed EVENT_IDs by scanning ADC_UPDT >= start_ts\n",
    "    \"\"\"\n",
    "    print(f\"  Using time-window scan from {start_ts}...\")\n",
    "    \n",
    "    changes_df = spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT EVENT_ID, ADC_UPDT\n",
    "        FROM {source_table}\n",
    "        WHERE ADC_UPDT >= timestamp('{start_ts}')\n",
    "          AND Trust = '{trust_filter}'\n",
    "    \"\"\")\n",
    "    \n",
    "    change_count = changes_df.count()\n",
    "    print(f\"  Time-window scan found {change_count:,} changed EVENT_IDs\")\n",
    "    \n",
    "    return changes_df\n",
    "\n",
    "#=========================\n",
    "# Main Processing Logic\n",
    "#=========================\n",
    "\n",
    "# Step 1: Get last processed timestamp\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Getting last processed timestamp...\")\n",
    "last_processed_ts = get_last_processed_timestamp(TARGET_TABLE)\n",
    "print(f\"Last processed timestamp: {last_processed_ts}\")\n",
    "\n",
    "# Step 2: Get changed EVENT_IDs (CDF with fallback)\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Detecting changes...\")\n",
    "\n",
    "if USE_CDF:\n",
    "    changed_events = get_changes_via_cdf(SOURCE_TABLE, last_processed_ts, trust_filter=\"Barts\")\n",
    "    if changed_events is None:\n",
    "        changed_events = get_changes_via_timewindow(SOURCE_TABLE, last_processed_ts, trust_filter=\"Barts\")\n",
    "else:\n",
    "    print(\"  CDF disabled, using time-window scan\")\n",
    "    changed_events = get_changes_via_timewindow(SOURCE_TABLE, last_processed_ts, trust_filter=\"Barts\")\n",
    "\n",
    "# Step 3: Filter out events already processed with same ADC_UPDT\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Filtering already processed events...\")\n",
    "new_events = (changed_events\n",
    "    .join(\n",
    "        spark.table(TARGET_TABLE).select(\"EVENT_ID\", \"ADC_UPDT\").distinct(),\n",
    "        on=[\"EVENT_ID\", \"ADC_UPDT\"],\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "    .limit(250000))\n",
    "\n",
    "new_event_count = new_events.count()\n",
    "print(f\"Found {new_event_count:,} new Barts events to process\")\n",
    "\n",
    "if new_event_count == 0:\n",
    "    print(\"No new events to process!\")\n",
    "    dbutils.notebook.exit(\"NO_WORK\")\n",
    "\n",
    "# Step 4: Build worklist with blob data\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Building worklist with blob data...\")\n",
    "\n",
    "# Define columns we need\n",
    "META_AND_BLOB_COLS = [\n",
    "    \"EVENT_ID\", \"BLOB_SEQ_NUM\",\n",
    "    \"VALID_UNTIL_DT_TM\", \"VALID_FROM_DT_TM\",\n",
    "    \"UPDT_DT_TM\", \"UPDT_ID\", \"UPDT_TASK\", \"UPDT_CNT\", \"UPDT_APPLCTX\",\n",
    "    \"LAST_UTC_TS\", \"ADC_UPDT\", \"COMPRESSION_CD\", \"BLOB_CONTENTS\", \"BLOB_LENGTH\"\n",
    "]\n",
    "\n",
    "# Load source data for new events (with Trust filter)\n",
    "source_data = (spark.table(SOURCE_TABLE)\n",
    "               .filter(F.col(\"Trust\") == \"Barts\")\n",
    "               .join(F.broadcast(new_events), on=[\"EVENT_ID\", \"ADC_UPDT\"], how=\"inner\")\n",
    "               .select(*META_AND_BLOB_COLS))\n",
    "\n",
    "# Deduplicate using window function (keep most recent version)\n",
    "w_temporal = Window.partitionBy(\"EVENT_ID\", \"BLOB_SEQ_NUM\").orderBy(\n",
    "    F.col(\"VALID_UNTIL_DT_TM\").desc(),\n",
    "    F.col(\"UPDT_DT_TM\").desc(),\n",
    "    F.col(\"LAST_UTC_TS\").desc()\n",
    ")\n",
    "\n",
    "deduped_data = (source_data\n",
    "                .withColumn(\"version_rank\", F.row_number().over(w_temporal))\n",
    "                .filter(F.col(\"version_rank\") == 1)\n",
    "                .drop(\"version_rank\"))\n",
    "\n",
    "# Aggregate chunks by EVENT_ID and ADC_UPDT\n",
    "worklist_with_meta = (deduped_data\n",
    "    .withColumn(\"chunk_size\", F.coalesce(F.col(\"BLOB_LENGTH\").cast(\"long\"), F.lit(0)))\n",
    "    .groupBy(\"EVENT_ID\", \"ADC_UPDT\")\n",
    "    .agg(\n",
    "        F.sum(\"chunk_size\").alias(\"compressed_size\"),\n",
    "        F.count(\"*\").alias(\"chunk_count\"),\n",
    "        F.collect_list(F.struct(*[c for c in META_AND_BLOB_COLS if c != \"EVENT_ID\"])).alias(\"chunks_data\")\n",
    "    )\n",
    "    .withColumn(\"status\", \n",
    "                F.when(F.col(\"compressed_size\") > MAX_BLOB_SIZE, \"oversized\")\n",
    "                .otherwise(\"pending\"))\n",
    "    .withColumn(\"batch_num\", F.lit(0))\n",
    "    .withColumn(\"process_ts\", F.lit(None).cast(\"timestamp\"))\n",
    "    .withColumn(\"error_msg\", F.lit(None).cast(\"string\")))\n",
    "\n",
    "# Write worklist table\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Writing worklist table...\")\n",
    "worklist_with_meta.write.mode(\"overwrite\").saveAsTable(WORKLIST_TABLE)\n",
    "\n",
    "# Optimize for efficient querying\n",
    "spark.sql(f\"OPTIMIZE {WORKLIST_TABLE} ZORDER BY (ADC_UPDT, EVENT_ID)\")\n",
    "\n",
    "# Get statistics\n",
    "stats = spark.table(WORKLIST_TABLE).groupBy(\"status\").count().collect()\n",
    "stats_dict = {row[\"status\"]: row[\"count\"] for row in stats}\n",
    "\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Worklist created successfully:\")\n",
    "print(f\"  - Pending: {stats_dict.get('pending', 0):,}\")\n",
    "print(f\"  - Oversized: {stats_dict.get('oversized', 0):,}\")\n",
    "print(f\"  - Total chunks stored: {spark.table(WORKLIST_TABLE).select(F.sum(F.col('chunk_count'))).collect()[0][0]:,}\")\n",
    "print(f\"  - Trust filter: Barts only\")\n",
    "print(f\"  - Incremental from: {last_processed_ts}\")\n",
    "print(f\"  - CDF enabled: {USE_CDF}\")\n",
    "\n",
    "# Step 5: Handle oversized events\n",
    "if stats_dict.get('oversized', 0) > 0:\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Writing {stats_dict.get('oversized', 0)} oversized events to target...\")\n",
    "    \n",
    "    oversized = (spark.table(WORKLIST_TABLE)\n",
    "                 .filter(F.col(\"status\") == \"oversized\")\n",
    "                 .select(\n",
    "                     \"EVENT_ID\",\n",
    "                     \"ADC_UPDT\",\n",
    "                     \"compressed_size\",\n",
    "                     F.col(\"chunks_data\")[0].alias(\"first_chunk\")\n",
    "                 ))\n",
    "    \n",
    "    oversized_output = oversized.select(\n",
    "        \"EVENT_ID\",\n",
    "        F.col(\"first_chunk.VALID_UNTIL_DT_TM\").alias(\"VALID_UNTIL_DT_TM\"),\n",
    "        F.col(\"first_chunk.VALID_FROM_DT_TM\").alias(\"VALID_FROM_DT_TM\"),\n",
    "        F.col(\"first_chunk.UPDT_DT_TM\").alias(\"UPDT_DT_TM\"),\n",
    "        F.col(\"first_chunk.UPDT_ID\").cast(\"long\").alias(\"UPDT_ID\"),\n",
    "        F.col(\"first_chunk.UPDT_TASK\").cast(\"long\").alias(\"UPDT_TASK\"),\n",
    "        F.col(\"first_chunk.UPDT_CNT\").cast(\"long\").alias(\"UPDT_CNT\"),\n",
    "        F.col(\"first_chunk.UPDT_APPLCTX\").cast(\"long\").alias(\"UPDT_APPLCTX\"),\n",
    "        F.col(\"first_chunk.LAST_UTC_TS\").alias(\"LAST_UTC_TS\"),\n",
    "        \"ADC_UPDT\",\n",
    "        F.lit(None).cast(\"binary\").alias(\"BLOB_BINARY\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"CONTENT_TYPE\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"ENCODING\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"BLOB_TEXT\"),\n",
    "        F.col(\"compressed_size\").alias(\"BINARY_SIZE\"),\n",
    "        F.lit(None).cast(\"long\").alias(\"TEXT_LENGTH\"),\n",
    "        F.concat(F.lit(\"Compressed Too Large: \"), F.col(\"compressed_size\"), F.lit(\" bytes\")).alias(\"STATUS\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"anon_text\")\n",
    "    )\n",
    "    \n",
    "    oversized_output.write.mode(\"append\").insertInto(TARGET_TABLE)\n",
    "    print(f\"  Wrote {stats_dict.get('oversized', 0)} oversized events to target\")\n",
    "\n",
    "# Step 6: Create/update metadata table with schema evolution\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Updating metadata table...\")\n",
    "\n",
    "METADATA_TABLE = f\"{STAGING_DB}.pipeline_metadata\"\n",
    "\n",
    "# Check if table exists and get its columns\n",
    "try:\n",
    "    existing_columns = set([f.name for f in spark.table(METADATA_TABLE).schema.fields])\n",
    "    table_exists = True\n",
    "    print(f\"  Existing metadata table has columns: {existing_columns}\")\n",
    "except:\n",
    "    existing_columns = set()\n",
    "    table_exists = False\n",
    "    print(\"  Creating new metadata table\")\n",
    "\n",
    "# Create table if it doesn't exist\n",
    "if not table_exists:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {METADATA_TABLE} (\n",
    "            run_id STRING,\n",
    "            worklist_table STRING,\n",
    "            total_events INT,\n",
    "            pending_events INT,\n",
    "            oversized_events INT,\n",
    "            created_ts TIMESTAMP,\n",
    "            completed_ts TIMESTAMP,\n",
    "            merged_ts TIMESTAMP,\n",
    "            status STRING,\n",
    "            batch_tables STRING,\n",
    "            processed_events INT,\n",
    "            trust_filter STRING,\n",
    "            cdf_enabled BOOLEAN,\n",
    "            last_processed_ts TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    existing_columns = {'run_id', 'worklist_table', 'total_events', 'pending_events', \n",
    "                       'oversized_events', 'created_ts', 'completed_ts', 'merged_ts', \n",
    "                       'status', 'batch_tables', 'processed_events', 'trust_filter', \n",
    "                       'cdf_enabled', 'last_processed_ts'}\n",
    "\n",
    "# Add missing columns if table exists\n",
    "if table_exists:\n",
    "    if 'trust_filter' not in existing_columns:\n",
    "        spark.sql(f\"ALTER TABLE {METADATA_TABLE} ADD COLUMN trust_filter STRING\")\n",
    "        print(\"  Added column: trust_filter\")\n",
    "    if 'cdf_enabled' not in existing_columns:\n",
    "        spark.sql(f\"ALTER TABLE {METADATA_TABLE} ADD COLUMN cdf_enabled BOOLEAN\")\n",
    "        print(\"  Added column: cdf_enabled\")\n",
    "    if 'last_processed_ts' not in existing_columns:\n",
    "        spark.sql(f\"ALTER TABLE {METADATA_TABLE} ADD COLUMN last_processed_ts TIMESTAMP\")\n",
    "        print(\"  Added column: last_processed_ts\")\n",
    "\n",
    "# Insert metadata\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {METADATA_TABLE}\n",
    "    VALUES (\n",
    "        '{RUN_ID}',\n",
    "        '{WORKLIST_TABLE}',\n",
    "        {new_event_count},\n",
    "        {stats_dict.get('pending', 0)},\n",
    "        {stats_dict.get('oversized', 0)},\n",
    "        current_timestamp(),\n",
    "        NULL,\n",
    "        NULL,\n",
    "        'worklist_created',\n",
    "        NULL,\n",
    "        NULL,\n",
    "        'Barts',\n",
    "        {USE_CDF},\n",
    "        timestamp('{last_processed_ts}')\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nPipeline metadata saved to: {METADATA_TABLE}\")\n",
    "print(f\"Worklist table: {WORKLIST_TABLE}\")\n",
    "print(f\"Run ID for next notebooks: {RUN_ID}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06032616-99cf-44aa-a139-214e2fab6e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Blob 1:Worklist",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
