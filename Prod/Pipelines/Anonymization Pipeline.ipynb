{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d0c2d56-5923-4d95-a1ca-9ab81ab162b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, current_timestamp, row_number,\n",
    "    udf, struct, collect_list, collect_set, create_map, expr, \n",
    "    max as spark_max, array, coalesce\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, StructType, StructField, LongType, ArrayType\n",
    "import os\n",
    "\n",
    "def is_valid_nhs_number(nhs_number):\n",
    "    \"\"\"Validate NHS number using checksum algorithm.\n",
    "    Accepts digits possibly separated by spaces or dashes.\n",
    "    \"\"\"\n",
    "    if not isinstance(nhs_number, str):\n",
    "        return False\n",
    "    # Remove spaces and dashes\n",
    "    nhs_digits = re.sub(r'[\\s-]', '', nhs_number)\n",
    "    if not nhs_digits.isdigit() or len(nhs_digits) != 10:\n",
    "        return False\n",
    "\n",
    "    weights = [10, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "    total = sum(int(digit) * weight for digit, weight in zip(nhs_digits[:9], weights))\n",
    "    remainder = total % 11\n",
    "    check_digit = 11 - remainder\n",
    "\n",
    "    if check_digit == 11:\n",
    "        check_digit = 0\n",
    "    elif check_digit == 10:\n",
    "        return False\n",
    "\n",
    "    return check_digit == int(nhs_digits[9])\n",
    "\n",
    "def _build_dob_patterns(dob_dt):\n",
    "    \"\"\"Build a comprehensive set of regex patterns for a given DOB.\"\"\"\n",
    "    if dob_dt is None:\n",
    "        return []\n",
    "\n",
    "    day = dob_dt.day\n",
    "    month = dob_dt.month\n",
    "    year = dob_dt.year\n",
    "\n",
    "    d = str(day)\n",
    "    dd = f\"{day:02d}\"\n",
    "    m = str(month)\n",
    "    mm = f\"{month:02d}\"\n",
    "    yyyy = str(year)\n",
    "    yy = f\"{year % 100:02d}\"\n",
    "\n",
    "    month_full = calendar.month_name[month]\n",
    "    month_abbr = calendar.month_abbr[month]\n",
    "    months_regex = f\"(?:{re.escape(month_full)}|{re.escape(month_abbr)})\"\n",
    "\n",
    "    sep = r\"[.\\-/\\s]\"\n",
    "    ord_suffix = r\"(?:st|nd|rd|th)?\"\n",
    "\n",
    "    patterns = [\n",
    "        fr\"(?<!\\d){dd}{sep}{mm}{sep}{yyyy}(?!\\d)\",\n",
    "        fr\"(?<!\\d){d}{sep}{m}{sep}{yyyy}(?!\\d)\",\n",
    "        fr\"(?<!\\d){dd}{sep}{mm}{sep}{yy}(?!\\d)\",\n",
    "        fr\"(?<!\\d){d}{sep}{m}{sep}{yy}(?!\\d)\",\n",
    "        fr\"(?<!\\d){yyyy}{sep}{mm}{sep}{dd}(?!\\d)\",\n",
    "        fr\"(?<!\\d){yyyy}{sep}{m}{sep}{d}(?!\\d)\",\n",
    "        fr\"(?<!\\d){dd}{mm}{yyyy}(?!\\d)\",\n",
    "        fr\"(?<!\\d){yyyy}{mm}{dd}(?!\\d)\",\n",
    "        fr\"(?<!\\d){dd}{mm}{yy}(?!\\d)\",\n",
    "    ]\n",
    "\n",
    "    patterns += [\n",
    "        fr\"\\b{d}{ord_suffix}{sep}+{months_regex}{sep}+{yyyy}\\b\",\n",
    "        fr\"\\b{dd}{ord_suffix}{sep}+{months_regex}{sep}+{yyyy}\\b\",\n",
    "        fr\"\\b{d}{ord_suffix}\\s+(?:of\\s+)?{months_regex}{sep}+{yyyy}\\b\",\n",
    "        fr\"\\b{dd}{ord_suffix}\\s+(?:of\\s+)?{months_regex}{sep}+{yyyy}\\b\",\n",
    "        fr\"\\b{months_regex}{sep}+{d}{ord_suffix}{sep}+{yyyy}\\b\",\n",
    "        fr\"\\b{months_regex}{sep}+{dd}{ord_suffix}{sep}+{yyyy}\\b\",\n",
    "        fr\"\\b{d}{ord_suffix}{sep}+{months_regex}{sep}+{yy}\\b\",\n",
    "        fr\"\\b{dd}{ord_suffix}{sep}+{months_regex}{sep}+{yy}\\b\",\n",
    "        fr\"\\b{months_regex}{sep}+{d}{ord_suffix}{sep}+{yy}\\b\",\n",
    "        fr\"\\b{months_regex}{sep}+{dd}{ord_suffix}{sep}+{yy}\\b\",\n",
    "    ]\n",
    "\n",
    "    patterns += [\n",
    "        fr\"\\b{d}{ord_suffix}\\s+(?:of\\s+)?{months_regex}[,\\s\\-]+{yyyy}\\b\",\n",
    "        fr\"\\b{months_regex}[,\\s\\-]+{d}{ord_suffix}[,\\s\\-]+{yyyy}\\b\",\n",
    "        fr\"\\b{d}{ord_suffix}\\s+(?:of\\s+)?{months_regex}[,\\s\\-]+{yy}\\b\",\n",
    "        fr\"\\b{months_regex}[,\\s\\-]+{d}{ord_suffix}[,\\s\\-]+{yy}\\b\",\n",
    "    ]\n",
    "\n",
    "    return patterns\n",
    "\n",
    "def _redact_dob(text, dob_dt):\n",
    "    \"\"\"Redact all common renderings of the given DOB from the text.\"\"\"\n",
    "    if not dob_dt:\n",
    "        return text\n",
    "\n",
    "    patterns = _build_dob_patterns(dob_dt)\n",
    "    for p in patterns:\n",
    "        text = re.sub(p, \"[[DATE OF BIRTH]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r\"(\\[\\[DATE OF BIRTH\\]\\])\\s+\\d{1,2}:\\d{2}(?::\\d{2})?\", r\"\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def simple_phi_redaction(text, first_names=None, middle_names=None, last_names=None, \n",
    "                        dob=None, addresses=None, aliases=None, whitelist=None):\n",
    "    \"\"\"\n",
    "    Simplified PHI redaction without spacy dependencies.\n",
    "    Now accepts lists/arrays for names, addresses, and aliases.\n",
    "    \"\"\"\n",
    "    if text is None or text == '':\n",
    "        return text\n",
    "\n",
    "    if whitelist is None:\n",
    "        whitelist = []\n",
    "\n",
    "    whitelist = [word.lower() for word in whitelist]\n",
    "\n",
    "    def replace_nhs_number(match):\n",
    "        raw = match.group()\n",
    "        if is_valid_nhs_number(raw):\n",
    "            return \"[[NHS Number]]\"\n",
    "        return raw\n",
    "\n",
    "    text = re.sub(r'(?<!\\d)(?:\\d[ -]?){9}\\d(?!\\d)', replace_nhs_number, text)\n",
    "\n",
    "    # Replace aliases\n",
    "    if aliases:\n",
    "        for alias in aliases:\n",
    "            if alias and len(alias) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(alias) + r'\\b',\n",
    "                              \"[[PATIENT IDENTIFIER]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace first names\n",
    "    if first_names:\n",
    "        for name in first_names:\n",
    "            if name and len(name) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(name) + r'\\b',\n",
    "                              \"[[PATIENT FORENAME]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace middle names\n",
    "    if middle_names:\n",
    "        for name in middle_names:\n",
    "            if name and name.strip() and len(name) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(name) + r'\\b',\n",
    "                              \"[[PATIENT MIDDLE NAME]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace surnames\n",
    "    if last_names:\n",
    "        for name in last_names:\n",
    "            if name and len(name) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(name) + r'\\b',\n",
    "                              \"[[PATIENT SURNAME]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Redact DOB\n",
    "    text = _redact_dob(text, dob)\n",
    "\n",
    "    # Replace address components\n",
    "    if addresses:\n",
    "        for addr in addresses:\n",
    "            if not addr:\n",
    "                continue\n",
    "            # Each address is expected to be a dict/Row with fields\n",
    "            for i, field in enumerate(['STREET_ADDR', 'STREET_ADDR2', 'STREET_ADDR3', 'STREET_ADDR4'], 1):\n",
    "                addr_val = addr.get(field) if isinstance(addr, dict) else getattr(addr, field, None)\n",
    "                if addr_val and len(str(addr_val)) > 2:\n",
    "                    addr_text = str(addr_val).strip()\n",
    "                    if len(addr_text) > 2:\n",
    "                        text = re.sub(r'\\b' + re.escape(addr_text) + r'\\b',\n",
    "                                      f\"[[STREET ADDRESS {i}]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "            for field, placeholder in [('CITY', '[[CITY]]'), ('COUNTY', '[[COUNTY]]'), \n",
    "                                      ('STATE', '[[STATE]]'), ('COUNTRY', '[[COUNTRY]]'),\n",
    "                                      ('ZIPCODE', '[[POSTCODE]]'), ('POSTAL_IDENTIFIER', '[[POSTAL IDENTIFIER]]')]:\n",
    "                val = addr.get(field) if isinstance(addr, dict) else getattr(addr, field, None)\n",
    "                if val and len(str(val)) > 2:\n",
    "                    text = re.sub(r'\\b' + re.escape(str(val)) + r'\\b',\n",
    "                                  placeholder, text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_eligible_person_ids(limit=100000, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Get the first N person_ids that have non-anonymized blobs eligible for anonymization.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of person_ids to process (default 100,000)\n",
    "        batch_size: Number of records to process at a time to avoid memory issues\n",
    "    \n",
    "    Returns:\n",
    "        List of person_ids that need anonymization\n",
    "    \"\"\"\n",
    "    print(f\"Finding first {limit:,} person_ids with non-anonymized blobs...\")\n",
    "    \n",
    "    window = Window.partitionBy(\"event_id\").orderBy(\n",
    "        col(\"valid_until_dt_tm\").desc(),\n",
    "        col(\"updt_dt_tm\").desc()\n",
    "    )\n",
    "    \n",
    "    eligible_blobs = spark.table(\"4_prod.bronze.mill_blob_text\") \\\n",
    "        .filter(col(\"STATUS\") == \"Decoded\") \\\n",
    "        .filter((col(\"anon_text\").isNull()) | (col(\"anon_text\") == \"\")) \\\n",
    "        .filter(col(\"BLOB_TEXT\").isNotNull()) \\\n",
    "        .filter(col(\"BLOB_TEXT\") != \"\") \\\n",
    "        .withColumn(\"row\", row_number().over(window)) \\\n",
    "        .filter(col(\"row\") == 1) \\\n",
    "        .drop(\"row\") \\\n",
    "        .select(\"EVENT_ID\")\n",
    "    \n",
    "    person_ids_with_blobs = eligible_blobs \\\n",
    "        .join(\n",
    "            spark.table(\"4_prod.raw.mill_clinical_event\")\n",
    "                .filter(col(\"VALID_UNTIL_DT_TM\") > current_timestamp())\n",
    "                .select(\"EVENT_ID\", \"ENCNTR_ID\"),\n",
    "            on=\"EVENT_ID\",\n",
    "            how=\"inner\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            spark.table(\"4_prod.raw.mill_encounter\")\n",
    "                .select(\"ENCNTR_ID\", \"PERSON_ID\"),\n",
    "            on=\"ENCNTR_ID\",\n",
    "            how=\"inner\"\n",
    "        ) \\\n",
    "        .select(\"PERSON_ID\") \\\n",
    "        .distinct() \\\n",
    "        .limit(limit)\n",
    "    \n",
    "    person_ids = [row.PERSON_ID for row in person_ids_with_blobs.collect()]\n",
    "    \n",
    "    print(f\"Found {len(person_ids):,} eligible person_ids for anonymization\")\n",
    "    \n",
    "    return person_ids\n",
    "\n",
    "def update_blob_text_for_persons(person_ids=None, whitelist=None, limit=100000):\n",
    "    \"\"\"\n",
    "    Main function to update blob text for person IDs.\n",
    "    Spark Connect compatible - uses DataFrame operations instead of broadcast variables.\n",
    "    \n",
    "    Args:\n",
    "        person_ids: List of person_ids to process (optional)\n",
    "        whitelist: List of words to exclude from anonymization\n",
    "        limit: If person_ids is None, number of eligible person_ids to find (default 100,000)\n",
    "    \"\"\"\n",
    "    if whitelist is None:\n",
    "        whitelist = ['Lady', 'Barts', 'Bartshealth', 'Newham', 'Homerton', 'Hospital']\n",
    "    \n",
    "    \n",
    "    if person_ids is None:\n",
    "        person_ids = get_eligible_person_ids(limit=limit)\n",
    "        \n",
    "    if not person_ids:\n",
    "        print(\"No eligible person_ids found for anonymization\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Processing {len(person_ids):,} person IDs...\")\n",
    "\n",
    "    # Step 1: Aggregate patient names - collect all variants per person\n",
    "    print(\"Fetching patient names...\")\n",
    "    patient_names_agg = spark.table(\"4_prod.raw.mill_person_name\") \\\n",
    "        .filter(col(\"PERSON_ID\").isin(person_ids)) \\\n",
    "        .filter(\n",
    "            (col(\"NAME_FIRST\").isNotNull() & (col(\"NAME_FIRST\") != \"\")) |\n",
    "            (col(\"NAME_MIDDLE\").isNotNull() & (col(\"NAME_MIDDLE\") != \"\")) |\n",
    "            (col(\"NAME_LAST\").isNotNull() & (col(\"NAME_LAST\") != \"\"))\n",
    "        ) \\\n",
    "        .groupBy(\"PERSON_ID\") \\\n",
    "        .agg(\n",
    "            collect_set(when(col(\"NAME_FIRST\").isNotNull(), col(\"NAME_FIRST\"))).alias(\"first_names\"),\n",
    "            collect_set(when(col(\"NAME_MIDDLE\").isNotNull(), col(\"NAME_MIDDLE\"))).alias(\"middle_names\"),\n",
    "            collect_set(when(col(\"NAME_LAST\").isNotNull(), col(\"NAME_LAST\"))).alias(\"last_names\")\n",
    "        )\n",
    "\n",
    "    # Step 2: Get patient DOBs\n",
    "    print(\"Fetching patient DOBs...\")\n",
    "    patient_dobs = spark.table(\"4_prod.raw.mill_person\") \\\n",
    "        .filter(col(\"PERSON_ID\").isin(person_ids)) \\\n",
    "        .select(\"PERSON_ID\", col(\"BIRTH_DT_TM\").alias(\"dob\"))\n",
    "\n",
    "    # Step 3: Aggregate addresses per person\n",
    "    print(\"Fetching patient addresses...\")\n",
    "    patient_addresses_agg = spark.table(\"4_prod.raw.mill_address\") \\\n",
    "        .filter(col(\"PARENT_ENTITY_NAME\") == \"PERSON\") \\\n",
    "        .filter(col(\"PARENT_ENTITY_ID\").isin(person_ids)) \\\n",
    "        .filter(col(\"ACTIVE_IND\") == 1) \\\n",
    "        .select(\n",
    "            col(\"PARENT_ENTITY_ID\").alias(\"PERSON_ID\"),\n",
    "            struct(\n",
    "                \"STREET_ADDR\", \"STREET_ADDR2\", \"STREET_ADDR3\", \"STREET_ADDR4\",\n",
    "                \"CITY\", \"COUNTY\", \"STATE\", \"COUNTRY\", \"ZIPCODE\", \"POSTAL_IDENTIFIER\"\n",
    "            ).alias(\"address\")\n",
    "        ) \\\n",
    "        .groupBy(\"PERSON_ID\") \\\n",
    "        .agg(collect_list(\"address\").alias(\"addresses\"))\n",
    "\n",
    "    # Step 4: Aggregate aliases per person\n",
    "    print(\"Fetching patient aliases...\")\n",
    "    patient_aliases_agg = spark.table(\"4_prod.raw.mill_person_alias\") \\\n",
    "        .filter(col(\"PERSON_ID\").isin(person_ids)) \\\n",
    "        .filter(col(\"ACTIVE_IND\") == 1) \\\n",
    "        .filter(col(\"ALIAS\").isNotNull()) \\\n",
    "        .filter(col(\"ALIAS\") != \"\") \\\n",
    "        .groupBy(\"PERSON_ID\") \\\n",
    "        .agg(collect_set(\"ALIAS\").alias(\"aliases\"))\n",
    "\n",
    "    # Step 5: Get encounter information\n",
    "    print(\"Getting encounter mappings...\")\n",
    "    encounter_df = spark.table(\"4_prod.raw.mill_clinical_event\") \\\n",
    "        .filter(col(\"VALID_UNTIL_DT_TM\") > current_timestamp()) \\\n",
    "        .select(\"EVENT_ID\", \"ENCNTR_ID\") \\\n",
    "        .join(\n",
    "            spark.table(\"4_prod.raw.mill_encounter\")\n",
    "                .filter(col(\"PERSON_ID\").isin(person_ids))\n",
    "                .select(\"ENCNTR_ID\", \"PERSON_ID\"),\n",
    "            on=\"ENCNTR_ID\",\n",
    "            how=\"inner\"\n",
    "        ) \\\n",
    "        .select(\"EVENT_ID\", \"PERSON_ID\") \\\n",
    "        .distinct()\n",
    "\n",
    "    print(\"Identifying rows to update...\")\n",
    "    event_ids_df = encounter_df.select(\"EVENT_ID\").distinct()\n",
    "\n",
    "\n",
    "    window = Window.partitionBy(\"EVENT_ID\").orderBy(\n",
    "        col(\"VALID_UNTIL_DT_TM\").desc(),\n",
    "        col(\"UPDT_DT_TM\").desc()\n",
    "    )\n",
    "\n",
    "    latest_meta = (\n",
    "        spark.table(\"4_prod.bronze.mill_blob_text\")\n",
    "            .filter(col(\"STATUS\") == \"Decoded\")\n",
    "            .filter((col(\"anon_text\").isNull()) | (col(\"anon_text\") == \"\"))\n",
    "            .join(event_ids_df, on=\"EVENT_ID\", how=\"inner\")  # Pre-filter to our batch\n",
    "            .select(\"EVENT_ID\", \"VALID_UNTIL_DT_TM\", \"UPDT_DT_TM\") \n",
    "            .withColumn(\"row\", row_number().over(window))\n",
    "            .filter(col(\"row\") == 1)\n",
    "            .drop(\"row\")\n",
    "    )\n",
    "\n",
    "    window_person = Window.partitionBy(\"EVENT_ID\").orderBy(col(\"PERSON_ID\"))\n",
    "    \n",
    "    rows_to_update = (\n",
    "        latest_meta\n",
    "            .join(\n",
    "                spark.table(\"4_prod.bronze.mill_blob_text\")\n",
    "                    .select(\"EVENT_ID\", \"VALID_UNTIL_DT_TM\", \"UPDT_DT_TM\", \"BLOB_TEXT\"),\n",
    "                on=[\"EVENT_ID\", \"VALID_UNTIL_DT_TM\", \"UPDT_DT_TM\"],\n",
    "                how=\"inner\"\n",
    "            )\n",
    "            .join(encounter_df, on=\"EVENT_ID\", how=\"inner\")\n",
    "            .withColumn(\"row_num\", row_number().over(window_person))\n",
    "            .filter(col(\"row_num\") == 1)\n",
    "            .drop(\"row_num\")\n",
    "            .select(\"EVENT_ID\", \"PERSON_ID\", \"BLOB_TEXT\")\n",
    "    )\n",
    "\n",
    "    # Join all patient info to rows_to_update\n",
    "    print(\"Joining patient information...\")\n",
    "    rows_with_info = rows_to_update \\\n",
    "        .join(patient_names_agg, on=\"PERSON_ID\", how=\"left\") \\\n",
    "        .join(patient_dobs, on=\"PERSON_ID\", how=\"left\") \\\n",
    "        .join(patient_addresses_agg, on=\"PERSON_ID\", how=\"left\") \\\n",
    "        .join(patient_aliases_agg, on=\"PERSON_ID\", how=\"left\") \\\n",
    "        .repartition(2000, col(\"EVENT_ID\"))  # Keep partitions small\n",
    "\n",
    "    row_count = rows_with_info.count()\n",
    "    print(f\"Found {row_count:,} rows to anonymize\")\n",
    "\n",
    "    if row_count == 0:\n",
    "        print(\"No rows to update\")\n",
    "        return 0\n",
    "\n",
    "    # Step 6: Create UDF that accepts arrays\n",
    "    def anonymize_udf_impl(blob_text, first_names, middle_names, last_names, \n",
    "                          dob, addresses, aliases):\n",
    "        if blob_text is None or blob_text == '':\n",
    "            return blob_text\n",
    "        return simple_phi_redaction(\n",
    "            blob_text, \n",
    "            first_names or [], \n",
    "            middle_names or [], \n",
    "            last_names or [], \n",
    "            dob,\n",
    "            addresses or [], \n",
    "            aliases or [], \n",
    "            whitelist\n",
    "        )\n",
    "\n",
    "    anonymize_text_udf = udf(anonymize_udf_impl, StringType())\n",
    "\n",
    "    # Step 7: Apply anonymization\n",
    "    print(\"Applying anonymization...\")\n",
    "    anonymized_df = rows_with_info \\\n",
    "        .withColumn(\n",
    "            \"anon_text\", \n",
    "            anonymize_text_udf(\n",
    "                col(\"BLOB_TEXT\"), \n",
    "                col(\"first_names\"), \n",
    "                col(\"middle_names\"), \n",
    "                col(\"last_names\"),\n",
    "                col(\"dob\"),\n",
    "                col(\"addresses\"), \n",
    "                col(\"aliases\")\n",
    "            )\n",
    "        ) \\\n",
    "        .select(\"EVENT_ID\", \"anon_text\") \\\n",
    "        .dropDuplicates([\"EVENT_ID\"])  # Ensure one row per EVENT_ID for MERGE\n",
    "\n",
    "    # Step 8: Update the original table\n",
    "    print(\"Updating the table...\")\n",
    "    temp_table_name = f\"temp_updates_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    anonymized_df.write.mode(\"overwrite\").saveAsTable(f\"4_prod.bronze.{temp_table_name}\")\n",
    "\n",
    "    try:\n",
    "        merge_query = f\"\"\"\n",
    "        MERGE INTO 4_prod.bronze.mill_blob_text AS target\n",
    "        USING 4_prod.bronze.{temp_table_name} AS source\n",
    "        ON target.EVENT_ID = source.EVENT_ID\n",
    "           AND target.STATUS = 'Decoded'\n",
    "           AND (target.anon_text IS NULL OR target.anon_text = '')\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET anon_text = source.anon_text\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query)\n",
    "        print(\"Update completed successfully!\")\n",
    "    finally:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS 4_prod.bronze.{temp_table_name}\")\n",
    "\n",
    "    # Return summary statistics\n",
    "    updated_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(DISTINCT EVENT_ID) as count\n",
    "        FROM 4_prod.bronze.mill_blob_text\n",
    "        WHERE anon_text IS NOT NULL\n",
    "          AND EVENT_ID IN (\n",
    "            SELECT EVENT_ID FROM 4_prod.bronze.mill_blob_text WHERE STATUS = 'Decoded'\n",
    "          )\n",
    "    \"\"\").collect()[0]['count']\n",
    "\n",
    "    print(f\"Successfully updated {updated_count:,} rows\")\n",
    "\n",
    "    # Print summary - collect aggregated stats\n",
    "    print(\"\\nAnonymization summary:\")\n",
    "    stats = rows_with_info.select(\n",
    "        col(\"PERSON_ID\"),\n",
    "        expr(\"size(coalesce(first_names, array()))\").alias(\"first_count\"),\n",
    "        expr(\"size(coalesce(middle_names, array()))\").alias(\"middle_count\"),\n",
    "        expr(\"size(coalesce(last_names, array()))\").alias(\"last_count\"),\n",
    "        expr(\"size(coalesce(addresses, array()))\").alias(\"address_count\"),\n",
    "        expr(\"size(coalesce(aliases, array()))\").alias(\"alias_count\")\n",
    "    ).agg(\n",
    "        expr(\"count(distinct PERSON_ID)\").alias(\"patient_count\"),\n",
    "        expr(\"sum(first_count)\").alias(\"total_first\"),\n",
    "        expr(\"sum(middle_count)\").alias(\"total_middle\"),\n",
    "        expr(\"sum(last_count)\").alias(\"total_last\"),\n",
    "        expr(\"sum(address_count)\").alias(\"total_addresses\"),\n",
    "        expr(\"sum(alias_count)\").alias(\"total_aliases\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"- Patients processed: {stats['patient_count']:,}\")\n",
    "    print(f\"- Name variants found:\")\n",
    "    print(f\"  - First names: {stats['total_first']:,}\")\n",
    "    print(f\"  - Middle names: {stats['total_middle']:,}\")\n",
    "    print(f\"  - Last names: {stats['total_last']:,}\")\n",
    "    print(f\"- Addresses processed: {stats['total_addresses']:,}\")\n",
    "    print(f\"- Aliases processed: {stats['total_aliases']:,}\")\n",
    "\n",
    "    return updated_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Process batches of eligible person_ids automatically\n",
    "    try:\n",
    "        update_blob_text_for_persons(limit=100000)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing first 100,000: {e}\")\n",
    "    \n",
    "    try:\n",
    "        update_blob_text_for_persons(limit=100000)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing second 100,000: {e}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Anonymization Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
