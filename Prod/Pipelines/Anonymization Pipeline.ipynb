{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d0c2d56-5923-4d95-a1ca-9ab81ab162b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, current_timestamp, row_number,\n",
    "    udf, struct, collect_list, create_map, expr\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, StructType, StructField, LongType\n",
    "import os\n",
    "\n",
    "def is_valid_nhs_number(nhs_number):\n",
    "    \"\"\"Validate NHS number using checksum algorithm.\n",
    "    Accepts digits possibly separated by spaces or dashes.\n",
    "    \"\"\"\n",
    "    if not isinstance(nhs_number, str):\n",
    "        return False\n",
    "    # Remove spaces and dashes\n",
    "    nhs_digits = re.sub(r'[\\s-]', '', nhs_number)\n",
    "    if not nhs_digits.isdigit() or len(nhs_digits) != 10:\n",
    "        return False\n",
    "\n",
    "    weights = [10, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "    total = sum(int(digit) * weight for digit, weight in zip(nhs_digits[:9], weights))\n",
    "    remainder = total % 11\n",
    "    check_digit = 11 - remainder\n",
    "\n",
    "    if check_digit == 11:\n",
    "        check_digit = 0\n",
    "    elif check_digit == 10:\n",
    "        return False\n",
    "\n",
    "    return check_digit == int(nhs_digits[9])\n",
    "\n",
    "def _build_dob_patterns(dob_dt):\n",
    "    \"\"\"Build a comprehensive set of regex patterns for a given DOB.\"\"\"\n",
    "    if dob_dt is None:\n",
    "        return []\n",
    "\n",
    "    day = dob_dt.day\n",
    "    month = dob_dt.month\n",
    "    year = dob_dt.year\n",
    "\n",
    "    d = str(day)\n",
    "    dd = f\"{day:02d}\"\n",
    "    m = str(month)\n",
    "    mm = f\"{month:02d}\"\n",
    "    yyyy = str(year)\n",
    "    yy = f\"{year % 100:02d}\"\n",
    "\n",
    "    month_full = calendar.month_name[month]   # e.g., \"January\"\n",
    "    month_abbr = calendar.month_abbr[month]   # e.g., \"Jan\"\n",
    "    months_regex = f\"(?:{re.escape(month_full)}|{re.escape(month_abbr)})\"\n",
    "\n",
    "    # Common separators (space, slash, dash, dot)\n",
    "    sep = r\"[.\\-/\\s]\"\n",
    "    # Ordinal suffix for day (st, nd, rd, th)\n",
    "    ord_suffix = r\"(?:st|nd|rd|th)?\"\n",
    "\n",
    "    # Numeric date formats\n",
    "    patterns = [\n",
    "        # dd/mm/yyyy, d/m/yyyy (with various separators)\n",
    "        fr\"(?<!\\d){dd}{sep}{mm}{sep}{yyyy}(?!\\d)\",\n",
    "        fr\"(?<!\\d){d}{sep}{m}{sep}{yyyy}(?!\\d)\",\n",
    "        # dd/mm/yy, d/m/yy\n",
    "        fr\"(?<!\\d){dd}{sep}{mm}{sep}{yy}(?!\\d)\",\n",
    "        fr\"(?<!\\d){d}{sep}{m}{sep}{yy}(?!\\d)\",\n",
    "        # yyyy/mm/dd\n",
    "        fr\"(?<!\\d){yyyy}{sep}{mm}{sep}{dd}(?!\\d)\",\n",
    "        fr\"(?<!\\d){yyyy}{sep}{m}{sep}{d}(?!\\d)\",\n",
    "        # Compact numeric forms (ddmmyyyy, yyyymmdd, ddmmyy)\n",
    "        fr\"(?<!\\d){dd}{mm}{yyyy}(?!\\d)\",\n",
    "        fr\"(?<!\\d){yyyy}{mm}{dd}(?!\\d)\",\n",
    "        fr\"(?<!\\d){dd}{mm}{yy}(?!\\d)\",\n",
    "    ]\n",
    "\n",
    "    # Month name formats:\n",
    "    # d[st|nd|rd|th]? <sep|' of '> Month <sep> yyyy\n",
    "    patterns += [\n",
    "        # Day Month Year (allow hyphen/space/slash/dot between day-month and month-year)\n",
    "        fr\"\\b{d}{ord_suffix}{sep}+{months_regex}{sep}+{yyyy}\\b\",\n",
    "        fr\"\\b{dd}{ord_suffix}{sep}+{months_regex}{sep}+{yyyy}\\b\",\n",
    "        # Day of Month Year\n",
    "        fr\"\\b{d}{ord_suffix}\\s+(?:of\\s+)?{months_regex}{sep}+{yyyy}\\b\",\n",
    "        fr\"\\b{dd}{ord_suffix}\\s+(?:of\\s+)?{months_regex}{sep}+{yyyy}\\b\",\n",
    "\n",
    "        # Month Day Year\n",
    "        fr\"\\b{months_regex}{sep}+{d}{ord_suffix}{sep}+{yyyy}\\b\",\n",
    "        fr\"\\b{months_regex}{sep}+{dd}{ord_suffix}{sep}+{yyyy}\\b\",\n",
    "\n",
    "        # 2-digit year variants\n",
    "        fr\"\\b{d}{ord_suffix}{sep}+{months_regex}{sep}+{yy}\\b\",\n",
    "        fr\"\\b{dd}{ord_suffix}{sep}+{months_regex}{sep}+{yy}\\b\",\n",
    "        fr\"\\b{months_regex}{sep}+{d}{ord_suffix}{sep}+{yy}\\b\",\n",
    "        fr\"\\b{months_regex}{sep}+{dd}{ord_suffix}{sep}+{yy}\\b\",\n",
    "    ]\n",
    "\n",
    "    # Also allow comma between month and year/day\n",
    "    patterns += [\n",
    "        fr\"\\b{d}{ord_suffix}\\s+(?:of\\s+)?{months_regex}[,\\s\\-]+{yyyy}\\b\",\n",
    "        fr\"\\b{months_regex}[,\\s\\-]+{d}{ord_suffix}[,\\s\\-]+{yyyy}\\b\",\n",
    "        fr\"\\b{d}{ord_suffix}\\s+(?:of\\s+)?{months_regex}[,\\s\\-]+{yy}\\b\",\n",
    "        fr\"\\b{months_regex}[,\\s\\-]+{d}{ord_suffix}[,\\s\\-]+{yy}\\b\",\n",
    "    ]\n",
    "\n",
    "    return patterns\n",
    "\n",
    "def _redact_dob(text, dob_dt):\n",
    "    \"\"\"Redact all common renderings of the given DOB from the text.\"\"\"\n",
    "    if not dob_dt:\n",
    "        return text\n",
    "\n",
    "    patterns = _build_dob_patterns(dob_dt)\n",
    "    for p in patterns:\n",
    "        text = re.sub(p, \"[[DATE OF BIRTH]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Optionally remove trailing time-of-day immediately after a DOB that just got redacted.\n",
    "    text = re.sub(r\"(\\[\\[DATE OF BIRTH\\]\\])\\s+\\d{1,2}:\\d{2}(?::\\d{2})?\", r\"\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def simple_phi_redaction(text, patient_info=None, address_info=None, alias_info=None, whitelist=None):\n",
    "    \"\"\"\n",
    "    Simplified PHI redaction without spacy dependencies.\n",
    "    Now includes: NHS (with spaces/dashes), patient aliases, address anonymization, and DOB redaction.\n",
    "    Handles multiple name variants for the same patient.\n",
    "    \"\"\"\n",
    "    if text is None or text == '':\n",
    "        return text\n",
    "\n",
    "    if whitelist is None:\n",
    "        whitelist = []\n",
    "\n",
    "    # Convert whitelist to lowercase for case-insensitive comparison\n",
    "    whitelist = [word.lower() for word in whitelist]\n",
    "\n",
    "    # First, find and replace all valid NHS numbers (allowing spaces or dashes)\n",
    "    def replace_nhs_number(match):\n",
    "        raw = match.group()\n",
    "        if is_valid_nhs_number(raw):\n",
    "            return \"[[NHS Number]]\"\n",
    "        return raw\n",
    "\n",
    "    # Match 10 digits possibly separated by spaces or dashes (exactly 10 digits total)\n",
    "    text = re.sub(r'(?<!\\d)(?:\\d[ -]?){9}\\d(?!\\d)', replace_nhs_number, text)\n",
    "\n",
    "    # Replace patient aliases if info is provided\n",
    "    if alias_info:\n",
    "        for alias in alias_info:\n",
    "            if alias and len(alias) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(alias) + r'\\b',\n",
    "                              \"[[PATIENT IDENTIFIER]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace patient names if info is provided - NOW HANDLES MULTIPLE VARIANTS\n",
    "    if patient_info:\n",
    "        # Replace forename(s) - handle both single strings and sets\n",
    "        forenames = patient_info.get('NAME_FIRST', set())\n",
    "        if isinstance(forenames, str):\n",
    "            forenames = {forenames}  # Convert single string to set\n",
    "        for name in forenames:\n",
    "            if name and len(name) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(name) + r'\\b',\n",
    "                              \"[[PATIENT FORENAME]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Replace middle name(s)\n",
    "        middle_names = patient_info.get('NAME_MIDDLE', set())\n",
    "        if isinstance(middle_names, str):\n",
    "            middle_names = {middle_names}\n",
    "        for name in middle_names:\n",
    "            if name and name.strip() and len(name) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(name) + r'\\b',\n",
    "                              \"[[PATIENT MIDDLE NAME]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Replace surname(s)\n",
    "        surnames = patient_info.get('NAME_LAST', set())\n",
    "        if isinstance(surnames, str):\n",
    "            surnames = {surnames}\n",
    "        for name in surnames:\n",
    "            if name and len(name) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(name) + r'\\b',\n",
    "                              \"[[PATIENT SURNAME]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Redact DOB\n",
    "        dob = patient_info.get('BIRTH_DT_TM')\n",
    "        text = _redact_dob(text, dob)\n",
    "\n",
    "    # Replace address components if info is provided\n",
    "    if address_info:\n",
    "        for addr in address_info:\n",
    "            # Replace street addresses\n",
    "            for i, field in enumerate(['STREET_ADDR', 'STREET_ADDR2', 'STREET_ADDR3', 'STREET_ADDR4'], 1):\n",
    "                if addr.get(field) and len(addr[field]) > 2:\n",
    "                    addr_text = addr[field].strip()\n",
    "                    if len(addr_text) > 2:\n",
    "                        text = re.sub(r'\\b' + re.escape(addr_text) + r'\\b',\n",
    "                                      f\"[[STREET ADDRESS {i}]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replace city\n",
    "            if addr.get('CITY') and len(addr['CITY']) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(addr['CITY']) + r'\\b',\n",
    "                              \"[[CITY]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replace county\n",
    "            if addr.get('COUNTY') and len(addr['COUNTY']) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(addr['COUNTY']) + r'\\b',\n",
    "                              \"[[COUNTY]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replace state\n",
    "            if addr.get('STATE') and len(addr['STATE']) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(addr['STATE']) + r'\\b',\n",
    "                              \"[[STATE]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replace country\n",
    "            if addr.get('COUNTRY') and len(addr['COUNTRY']) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(addr['COUNTRY']) + r'\\b',\n",
    "                              \"[[COUNTRY]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replace postal code\n",
    "            if addr.get('ZIPCODE') and len(addr['ZIPCODE']) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(addr['ZIPCODE']) + r'\\b',\n",
    "                              \"[[POSTCODE]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replace postal identifier\n",
    "            if addr.get('POSTAL_IDENTIFIER') and len(addr['POSTAL_IDENTIFIER']) > 2:\n",
    "                text = re.sub(r'\\b' + re.escape(addr['POSTAL_IDENTIFIER']) + r'\\b',\n",
    "                              \"[[POSTAL IDENTIFIER]]\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_eligible_person_ids(limit=100000, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Get the first N person_ids that have non-anonymized blobs eligible for anonymization.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of person_ids to process (default 100,000)\n",
    "        batch_size: Number of records to process at a time to avoid memory issues\n",
    "    \n",
    "    Returns:\n",
    "        List of person_ids that need anonymization\n",
    "    \"\"\"\n",
    "    print(f\"Finding first {limit:,} person_ids with non-anonymized blobs...\")\n",
    "    \n",
    "    # Window function to get the most recent blob per event\n",
    "    window = Window.partitionBy(\"event_id\").orderBy(\n",
    "        col(\"valid_until_dt_tm\").desc(),\n",
    "        col(\"updt_dt_tm\").desc()\n",
    "    )\n",
    "    \n",
    "    # Find eligible blobs that need anonymization\n",
    "    eligible_blobs = spark.table(\"4_prod.bronze.mill_blob_text\") \\\n",
    "        .filter(col(\"STATUS\") == \"Decoded\") \\\n",
    "        .filter((col(\"anon_text\").isNull()) | (col(\"anon_text\") == \"\")) \\\n",
    "        .filter(col(\"BLOB_TEXT\").isNotNull()) \\\n",
    "        .filter(col(\"BLOB_TEXT\") != \"\") \\\n",
    "        .withColumn(\"row\", row_number().over(window)) \\\n",
    "        .filter(col(\"row\") == 1) \\\n",
    "        .drop(\"row\") \\\n",
    "        .select(\"EVENT_ID\")\n",
    "    \n",
    "    # Join with clinical events and encounters to get person_ids\n",
    "    person_ids_with_blobs = eligible_blobs \\\n",
    "        .join(\n",
    "            spark.table(\"4_prod.raw.mill_clinical_event\")\n",
    "                .filter(col(\"VALID_UNTIL_DT_TM\") > current_timestamp())\n",
    "                .select(\"EVENT_ID\", \"ENCNTR_ID\"),\n",
    "            on=\"EVENT_ID\",\n",
    "            how=\"inner\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            spark.table(\"4_prod.raw.mill_encounter\")\n",
    "                .select(\"ENCNTR_ID\", \"PERSON_ID\"),\n",
    "            on=\"ENCNTR_ID\",\n",
    "            how=\"inner\"\n",
    "        ) \\\n",
    "        .select(\"PERSON_ID\") \\\n",
    "        .distinct() \\\n",
    "        .limit(limit)\n",
    "    \n",
    "    # Collect the person_ids\n",
    "    person_ids = [row.PERSON_ID for row in person_ids_with_blobs.collect()]\n",
    "    \n",
    "    print(f\"Found {len(person_ids):,} eligible person_ids for anonymization\")\n",
    "    \n",
    "    return person_ids\n",
    "\n",
    "def update_blob_text_for_persons(person_ids=None, whitelist=None, limit=100000):\n",
    "    \"\"\"\n",
    "    Main function to update blob text for person IDs.\n",
    "    If person_ids is None, it will find the first N eligible person_ids automatically.\n",
    "    \n",
    "    Args:\n",
    "        person_ids: List of person_ids to process (optional)\n",
    "        whitelist: List of words to exclude from anonymization\n",
    "        limit: If person_ids is None, number of eligible person_ids to find (default 100,000)\n",
    "    \"\"\"\n",
    "    if whitelist is None:\n",
    "        whitelist = ['Lady', 'Barts', 'Bartshealth', 'Newham', 'Homerton', 'Hospital']\n",
    "    \n",
    "    # If no person_ids provided, find eligible ones\n",
    "    if person_ids is None:\n",
    "        person_ids = get_eligible_person_ids(limit=limit)\n",
    "        \n",
    "    if not person_ids:\n",
    "        print(\"No eligible person_ids found for anonymization\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Processing {len(person_ids):,} person IDs...\")\n",
    "\n",
    "    # Step 1: Get ALL patient name variants for all person_ids\n",
    "    print(\"Fetching patient names...\")\n",
    "    patient_names_df = spark.table(\"4_prod.raw.mill_person_name\") \\\n",
    "        .filter(col(\"PERSON_ID\").isin(person_ids)) \\\n",
    "        .select(\"PERSON_ID\", \"NAME_FIRST\", \"NAME_MIDDLE\", \"NAME_LAST\") \\\n",
    "        .collect()\n",
    "\n",
    "    # Create a dictionary that stores ALL name variants for each person\n",
    "    patient_names_dict = {}\n",
    "    for row in patient_names_df:\n",
    "        person_id = row['PERSON_ID']\n",
    "        if person_id not in patient_names_dict:\n",
    "            patient_names_dict[person_id] = {\n",
    "                'PERSON_ID': person_id,\n",
    "                'NAME_FIRST': set(),\n",
    "                'NAME_MIDDLE': set(),\n",
    "                'NAME_LAST': set()\n",
    "            }\n",
    "        \n",
    "        # Add each name variant to a set (avoiding duplicates and handling nulls)\n",
    "        if row['NAME_FIRST'] and row['NAME_FIRST'].strip():\n",
    "            patient_names_dict[person_id]['NAME_FIRST'].add(row['NAME_FIRST'].strip())\n",
    "        if row['NAME_MIDDLE'] and row['NAME_MIDDLE'].strip():\n",
    "            patient_names_dict[person_id]['NAME_MIDDLE'].add(row['NAME_MIDDLE'].strip())\n",
    "        if row['NAME_LAST'] and row['NAME_LAST'].strip():\n",
    "            patient_names_dict[person_id]['NAME_LAST'].add(row['NAME_LAST'].strip())\n",
    "\n",
    "    # Step 1b: Get patient DOBs from mill_person and add to patient_names_dict\n",
    "    print(\"Fetching patient DOBs...\")\n",
    "    dob_rows = spark.table(\"4_prod.raw.mill_person\") \\\n",
    "        .filter(col(\"PERSON_ID\").isin(person_ids)) \\\n",
    "        .select(\"PERSON_ID\", \"BIRTH_DT_TM\") \\\n",
    "        .collect()\n",
    "    \n",
    "    # Add DOB to patient info dict\n",
    "    for row in dob_rows:\n",
    "        pid = row['PERSON_ID']\n",
    "        if pid not in patient_names_dict:\n",
    "            patient_names_dict[pid] = {\n",
    "                'PERSON_ID': pid,\n",
    "                'NAME_FIRST': set(),\n",
    "                'NAME_MIDDLE': set(),\n",
    "                'NAME_LAST': set()\n",
    "            }\n",
    "        patient_names_dict[pid]['BIRTH_DT_TM'] = row['BIRTH_DT_TM']\n",
    "\n",
    "    # Step 2: Get addresses for all person_ids\n",
    "    print(\"Fetching patient addresses...\")\n",
    "    addresses_df = spark.table(\"4_prod.raw.mill_address\") \\\n",
    "        .filter(col(\"PARENT_ENTITY_NAME\") == \"PERSON\") \\\n",
    "        .filter(col(\"PARENT_ENTITY_ID\").isin(person_ids)) \\\n",
    "        .filter(col(\"ACTIVE_IND\") == 1) \\\n",
    "        .select(\"PARENT_ENTITY_ID\", \"STREET_ADDR\", \"STREET_ADDR2\", \"STREET_ADDR3\",\n",
    "                \"STREET_ADDR4\", \"CITY\", \"COUNTY\", \"STATE\", \"COUNTRY\",\n",
    "                \"ZIPCODE\", \"POSTAL_IDENTIFIER\") \\\n",
    "        .collect()\n",
    "\n",
    "    # Create a dictionary of addresses by person_id (can have multiple addresses per person)\n",
    "    addresses_dict = {}\n",
    "    for row in addresses_df:\n",
    "        person_id = row['PARENT_ENTITY_ID']\n",
    "        if person_id not in addresses_dict:\n",
    "            addresses_dict[person_id] = []\n",
    "        addresses_dict[person_id].append(row.asDict())\n",
    "\n",
    "    # Step 3: Get patient aliases for all person_ids\n",
    "    print(\"Fetching patient aliases...\")\n",
    "    aliases_df = spark.table(\"4_prod.raw.mill_person_alias\") \\\n",
    "        .filter(col(\"PERSON_ID\").isin(person_ids)) \\\n",
    "        .filter(col(\"ACTIVE_IND\") == 1) \\\n",
    "        .filter(col(\"ALIAS\").isNotNull()) \\\n",
    "        .select(\"PERSON_ID\", \"ALIAS\") \\\n",
    "        .collect()\n",
    "\n",
    "    # Create a dictionary of aliases by person_id\n",
    "    aliases_dict = {}\n",
    "    for row in aliases_df:\n",
    "        person_id = row['PERSON_ID']\n",
    "        alias = row['ALIAS']\n",
    "        if person_id not in aliases_dict:\n",
    "            aliases_dict[person_id] = []\n",
    "        if alias and alias.strip():  # Only add non-empty aliases\n",
    "            aliases_dict[person_id].append(alias.strip())\n",
    "\n",
    "    # Step 4: Get the encounter information to link person_id with event_id\n",
    "    print(\"Getting encounter mappings...\")\n",
    "    encounter_df = spark.table(\"4_prod.raw.mill_clinical_event\") \\\n",
    "        .filter(col(\"VALID_UNTIL_DT_TM\") > current_timestamp()) \\\n",
    "        .select(\"EVENT_ID\", \"ENCNTR_ID\") \\\n",
    "        .join(\n",
    "            spark.table(\"4_prod.raw.mill_encounter\")\n",
    "                .filter(col(\"PERSON_ID\").isin(person_ids))\n",
    "                .select(\"ENCNTR_ID\", \"PERSON_ID\"),\n",
    "            on=\"ENCNTR_ID\",\n",
    "            how=\"inner\"\n",
    "        ) \\\n",
    "        .select(\"EVENT_ID\", \"PERSON_ID\") \\\n",
    "        .distinct()\n",
    "\n",
    "    # Step 5: Identify rows to update based on lookup_blob_content logic\n",
    "    print(\"Identifying rows to update...\")\n",
    "    window = Window.partitionBy(\"event_id\").orderBy(\n",
    "        col(\"valid_until_dt_tm\").desc(),\n",
    "        col(\"updt_dt_tm\").desc()\n",
    "    )\n",
    "\n",
    "    # Get the most recent decoded blob for each event_id that needs updating\n",
    "    rows_to_update = spark.table(\"4_prod.bronze.mill_blob_text\") \\\n",
    "        .filter(col(\"STATUS\") == \"Decoded\") \\\n",
    "        .filter((col(\"anon_text\").isNull()) | (col(\"anon_text\") == \"\")) \\\n",
    "        .withColumn(\"row\", row_number().over(window)) \\\n",
    "        .filter(col(\"row\") == 1) \\\n",
    "        .drop(\"row\") \\\n",
    "        .join(encounter_df, on=\"EVENT_ID\", how=\"inner\") \\\n",
    "        .select(\"EVENT_ID\", \"PERSON_ID\", \"BLOB_TEXT\")\n",
    "\n",
    "    # Count rows to process\n",
    "    row_count = rows_to_update.count()\n",
    "    print(f\"Found {row_count:,} rows to anonymize\")\n",
    "\n",
    "    if row_count == 0:\n",
    "        print(\"No rows to update\")\n",
    "        return 0\n",
    "\n",
    "    # Step 6: Create UDF for anonymization with address, alias, and DOB support\n",
    "    def anonymize_udf(blob_text, person_id):\n",
    "        if blob_text is None or blob_text == '':\n",
    "            return blob_text\n",
    "        patient_info = patient_names_dict.get(person_id, {})\n",
    "        address_info = addresses_dict.get(person_id, [])\n",
    "        alias_info = aliases_dict.get(person_id, [])\n",
    "        return simple_phi_redaction(blob_text, patient_info, address_info, alias_info, whitelist)\n",
    "\n",
    "    # Register UDF\n",
    "    anonymize_text_udf = udf(anonymize_udf, StringType())\n",
    "\n",
    "    # Step 7: Apply anonymization\n",
    "    print(\"Applying anonymization...\")\n",
    "    anonymized_df = rows_to_update \\\n",
    "        .withColumn(\"anon_text\", anonymize_text_udf(col(\"BLOB_TEXT\"), col(\"PERSON_ID\"))) \\\n",
    "        .select(\"EVENT_ID\", \"anon_text\")\n",
    "\n",
    "    # Step 8: Update the original table using a temporary table\n",
    "    print(\"Updating the table...\")\n",
    "\n",
    "    temp_table_name = f\"temp_updates_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    anonymized_df.write.mode(\"overwrite\").saveAsTable(f\"4_prod.bronze.{temp_table_name}\")\n",
    "\n",
    "    try:\n",
    "        merge_query = f\"\"\"\n",
    "        MERGE INTO 4_prod.bronze.mill_blob_text AS target\n",
    "        USING 4_prod.bronze.{temp_table_name} AS source\n",
    "        ON target.EVENT_ID = source.EVENT_ID\n",
    "           AND target.STATUS = 'Decoded'\n",
    "           AND (target.anon_text IS NULL OR target.anon_text = '')\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                anon_text = source.anon_text\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query)\n",
    "        print(\"Update completed successfully!\")\n",
    "    finally:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS 4_prod.bronze.{temp_table_name}\")\n",
    "\n",
    "    # Return summary statistics\n",
    "    updated_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(DISTINCT EVENT_ID) as count\n",
    "        FROM 4_prod.bronze.mill_blob_text\n",
    "        WHERE anon_text IS NOT NULL\n",
    "          AND EVENT_ID IN (\n",
    "            SELECT EVENT_ID FROM 4_prod.bronze.mill_blob_text WHERE STATUS = 'Decoded'\n",
    "          )\n",
    "    \"\"\").collect()[0]['count']\n",
    "\n",
    "    print(f\"Successfully updated {updated_count:,} rows\")\n",
    "\n",
    "    # Print summary of anonymization including name variants\n",
    "    print(\"\\nAnonymization summary:\")\n",
    "    print(f\"- Patients processed: {len(patient_names_dict):,}\")\n",
    "    \n",
    "    # Count total unique name variants\n",
    "    total_first_names = sum(len(p.get('NAME_FIRST', set())) for p in patient_names_dict.values())\n",
    "    total_middle_names = sum(len(p.get('NAME_MIDDLE', set())) for p in patient_names_dict.values())\n",
    "    total_last_names = sum(len(p.get('NAME_LAST', set())) for p in patient_names_dict.values())\n",
    "    \n",
    "    print(f\"- Name variants found:\")\n",
    "    print(f\"  - First names: {total_first_names:,}\")\n",
    "    print(f\"  - Middle names: {total_middle_names:,}\")\n",
    "    print(f\"  - Last names: {total_last_names:,}\")\n",
    "    print(f\"- Addresses processed: {sum(len(addrs) for addrs in addresses_dict.values()):,}\")\n",
    "    print(f\"- Aliases processed: {sum(len(aliases) for aliases in aliases_dict.values()):,}\")\n",
    "\n",
    "    return updated_count\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Process first 100,000 eligible person_ids automatically\n",
    "    update_blob_text_for_persons(limit=100000)\n",
    "    update_blob_text_for_persons(limit=100000)\n",
    "    update_blob_text_for_persons(limit=25000)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Anonymization Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
