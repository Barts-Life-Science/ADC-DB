{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f52c145-9a2a-41fc-b9d9-64f2a0d0a7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "project_identifier = 'dar050'\n",
    "\n",
    "rde_tables = ['rde_aliases', 'rde_all_procedures', 'rde_all_problems','rde_all_diagnosis', 'rde_emergencyd', 'rde_emergency_findings', 'rde_encounter', 'rde_patient_demographics', 'rde_pc_diagnosis', 'rde_pc_problems', 'rde_pc_procedures', 'rde_powerforms', 'rde_radiology', 'rde_measurements']\n",
    "\n",
    "# Don't know about ig risk and severity yet\n",
    "max_ig_risk = 3\n",
    "max_ig_severity = 2\n",
    "columns_to_exclude = ['ADC_UPDT']\n",
    "\n",
    "# Select the cohort: meet both alcohol intoxication & minor head injury \n",
    "alcohol_diag = spark.sql (\"select distinct(Person_ID) from 4_prod.rde.rde_all_diagnosis where CKI in ('18653004', '1149333003', '25702006' )\")\n",
    "alcohol_prob = spark.sql (\"select distinct(Person_ID) from 4_prod.rde.rde_all_problems where CKI in ('18653004', '1149333003', '25702006')\")\n",
    "\n",
    "alcohol = alcohol_diag.union(alcohol_prob)\n",
    "    \n",
    "head_diag = spark.sql (\"select distinct(Person_ID) from 4_prod.rde.rde_all_diagnosis where CKI = '274164006'\")\n",
    "head_prob = spark.sql (\"select distinct(Person_ID) from 4_prod.rde.rde_all_problems where CKI = '274164006'\")\n",
    "\n",
    "head = head_diag.union(head_prob)\n",
    "cohort_person_id = head.join(alcohol, ['Person_ID']).select('Person_ID').distinct()\n",
    "\n",
    "# Create cohort view\n",
    "cohort_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW 6_mgmt.cohorts.{project_identifier} AS\n",
    "SELECT \n",
    "    DISTINCT pd.person_id AS PERSON_ID,\n",
    "    CAST(NULL AS STRING) AS SUBCOHORT\n",
    "FROM 4_prod.rde.rde_patient_demographics pd\n",
    "WHERE pd.person_id IN (SELECT person_id FROM 4_prod.rde.rde_encounter)\n",
    "\"\"\"\n",
    "spark.sql(cohort_sql)\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"USE CATALOG 5_projects\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS 5_projects.{project_identifier}\")\n",
    "\n",
    "# Get list of existing views in the target schema\n",
    "existing_views_df = spark.sql(f\"\"\"\n",
    "    SHOW VIEWS IN 5_projects.{project_identifier}\n",
    "\"\"\")\n",
    "\n",
    "# Drop all existing views in the schema\n",
    "if existing_views_df.count() > 0:\n",
    "    for row in existing_views_df.collect():\n",
    "        view_name = row.viewName\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {project_identifier}.{view_name}\")\n",
    "        print(f\"Dropped view: {project_identifier}.{view_name}\")\n",
    "\n",
    "def get_columns_with_high_tags(table_name):\n",
    "    # Get columns with high ig_risk\n",
    "    high_risk_columns = spark.sql(f\"\"\"\n",
    "        SELECT column_name\n",
    "        FROM 4_prod.information_schema.column_tags\n",
    "        WHERE schema_name = 'rde'\n",
    "        AND table_name = '{table_name}'\n",
    "        AND tag_name = 'ig_risk'\n",
    "        AND tag_value > {max_ig_risk}\n",
    "    \"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "    # Get columns with high ig_severity\n",
    "    high_severity_columns = spark.sql(f\"\"\"\n",
    "        SELECT column_name\n",
    "        FROM 4_prod.information_schema.column_tags\n",
    "        WHERE schema_name = 'rde'\n",
    "        AND table_name = '{table_name}'\n",
    "        AND tag_name = 'ig_severity'\n",
    "        AND tag_value > {max_ig_severity}\n",
    "    \"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "    # Convert the combined list to a set before returning\n",
    "    return high_risk_columns + high_severity_columns\n",
    "\n",
    "# Function to get column names excluding specified columns and columns with high tags\n",
    "def get_columns_except_excluded(table_name):\n",
    "    # Get all columns from the table\n",
    "    all_columns = spark.table(f\"4_prod.rde.{table_name}\").columns\n",
    "    \n",
    "    # Get columns with high risk or severity tags\n",
    "    high_tag_columns = get_columns_with_high_tags(table_name)\n",
    "\n",
    "    all_exluded_columns = high_tag_columns + columns_to_exclude\n",
    "    \n",
    "    # Filter out excluded columns using set difference\n",
    "    filtered_columns = list(set(all_columns) - set(all_exluded_columns))\n",
    "    \n",
    "    # Convert back to sorted list and join\n",
    "    return \", \".join(sorted(filtered_columns))\n",
    "\n",
    "# Function to determine the person ID column name\n",
    "def get_person_id_column(table_name):\n",
    "    columns = spark.table(f\"4_prod.rde.{table_name}\").columns\n",
    "    if 'PERSON_ID' in columns:\n",
    "        return 'PERSON_ID'\n",
    "    elif 'PERSONID' in columns:\n",
    "        return 'PERSONID'\n",
    "    elif 'Person_ID' in columns:\n",
    "        return 'Person_ID'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "for table in rde_tables:\n",
    "    # Get columns string\n",
    "    columns = get_columns_except_excluded(table)\n",
    "    \n",
    "    # Get the appropriate person ID column name\n",
    "    person_id_col = get_person_id_column(table)\n",
    "    \n",
    "    if person_id_col:\n",
    "        # Create view SQL with cohort filtering\n",
    "        view_sql = f\"\"\"\n",
    "        CREATE OR REPLACE VIEW 5_projects.{project_identifier}.{table}\n",
    "        AS\n",
    "        WITH source_data AS (\n",
    "            SELECT {columns}\n",
    "            FROM 4_prod.rde.{table}\n",
    "        )\n",
    "        SELECT s.*\n",
    "        FROM source_data s\n",
    "        INNER JOIN 6_mgmt.cohorts.{project_identifier} c\n",
    "        ON s.{person_id_col} = c.PERSON_ID\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # If no person ID column exists, create view without filtering\n",
    "        view_sql = f\"\"\"\n",
    "        CREATE OR REPLACE VIEW 5_projects.{project_identifier}.{table}\n",
    "        AS\n",
    "        SELECT {columns}\n",
    "        FROM 4_prod.rde.{table}\n",
    "        \"\"\"\n",
    "        print(f\"Warning: No person ID column found in {table}. Creating view without cohort filtering.\")\n",
    "    \n",
    "    # Execute the SQL\n",
    "    spark.sql(view_sql)\n",
    "    \n",
    "    print(f\"Created view: 5_projects.{project_identifier}.{table}\")\n",
    "\n",
    "# Create schema view\n",
    "schema_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW 5_projects.{project_identifier}.schema AS\n",
    "SELECT \n",
    "    table_name,\n",
    "    column_name,\n",
    "    COALESCE(comment, '') as column_comment\n",
    "FROM 5_projects.information_schema.columns\n",
    "WHERE table_catalog = '5_projects'\n",
    "AND table_schema = '{project_identifier}'\n",
    "AND table_name != 'schema'\n",
    "ORDER BY table_name, column_name\n",
    "\"\"\"\n",
    "spark.sql(schema_sql)\n",
    "print(f\"Created schema view: 5_projects.{project_identifier}.schema\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DAR050",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
